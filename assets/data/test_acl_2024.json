{"hash_id": 286720000462099611, "entities": ["memory footprint reduction", "training time efficiency", "large language models"], "background": "1. The need to reduce the memory footprint and training time in finetuning large language models, as existing methods still require considerable memory and do not simultaneously address all three contributors to the memory demand: model weights, optimizer states, and intermediate activations. 2. The desire to improve the applicability and efficacy of memory-efficient finetuning methods for very large models (100 billion parameters or more), which are increasingly common but also increasingly challenging to finetune due to memory constraints."}
{"hash_id": 1009711757213687892, "entities": ["limiting nonverbal information", "complex semantics discernment", "unsupervised scenarios", "nonverbal modalities cues", "clustering performance"], "background": "1. The limitations of existing methods in leveraging nonverbal information for discerning complex semantics in unsupervised scenarios. 2. The recognition that non-verbal modalities (video and audio) can provide critical cues for semantics discovery and improve clustering performance."}
{"hash_id": 9207589798655166985, "entities": ["fake news risk", "large language models", "detection method limitations"], "background": "1. The increasing risk of fake news and plagiarism due to the advancement of large language models that can generate human-like text. 2. The limitations of current detection methods that perform well only in specific domains or with known sources of machine-generated text."}
{"hash_id": 8141659205394742517, "entities": ["privacypreserving language models", "dp parameters", "privacy risks", "model inference"], "background": "1. The need for a fair and comprehensive evaluation of privacy-preserving language models to ensure that privacy leaks are adequately addressed and compared across different models. 2. The concern that existing evaluations of PPLMs often focus on DP parameters and do not account for privacy risks during model inference, which could lead to underestimated privacy breaches in real-world applications."}
{"hash_id": 58030756258770471, "entities": ["beam search decoding", "nbest hypotheses", "large language models"], "background": "1. The limitations of current translation methods that rely on beam search decoding and top-1 hypothesis selection, which do not fully exploit the rich information in N-best hypotheses. 2. The potential to improve translation quality by leveraging the rich linguistic knowledge and strong reasoning abilities of large language models."}
{"hash_id": 5362855012868998890, "entities": ["memory footprint reduction", "computational efficiency", "subbit quantization", "model performance preservation"], "background": "1. The need to reduce the memory footprint and computational requirements of Large Language Models (LLMs) for deployment on resource-constrained devices. 2. The challenge of preserving model performance at sub-4-bit quantization levels, where existing methods significantly degrade accuracy."}
{"hash_id": 1012593116373699955, "entities": ["large language models", "retrieved information", "text generation", "lowcost training", "rag optimization"], "background": "1. The need to improve the effectiveness of large language models in utilizing retrieved information for enhanced text generation in RAG. 2. The requirement for a low-cost, general training method that can optimize LLMs for RAG across various tasks without relying on supervised data."}
{"hash_id": 462646107607107960, "entities": ["chinese spelling check dataset", "native speaker error patterns", "pseudo data generation method"], "background": "1. The need for a Chinese spelling check dataset that represents the error patterns of native speakers, as existing datasets are tailored to non-native speakers and do not reflect the actual errors made by natives. 2. The requirement for a method that can generate large-scale and high-quality pseudo data that aligns with the real-world input scenarios of native speakers, improving model performance."}
{"hash_id": 1132468410553669341, "entities": ["enhance interplay abilities", "supervised finetuning", "mitigate performance conflicts"], "background": "1. The need to understand and enhance the interplay of multiple abilities in large language models during supervised fine-tuning to achieve versatility similar to proprietary models. 2. The requirement to mitigate performance conflicts and catastrophic forgetting that arise when training models on composite data for multiple tasks."}
{"hash_id": 7879279795955528156, "entities": ["large language models", "interpretability", "automated evaluation method"], "background": "1. The lack of understanding of the underlying explanatory process in Large Language Models (LLMs) hinders their interpretability and reliability. 2. The need for an automated evaluation method for natural language explanations that does not rely on resource-intensive annotation or domain-specific knowledge."}
{"hash_id": 7305789756823201983, "entities": ["nli models", "lowresource languages", "automatically labeled data"], "background": "1. The need to develop NLI models for low-resource languages like Romanian to improve natural language understanding in under-studied languages. 2. The requirement to enhance the performance of NLI models on automatically labeled data, which often contains noise and affects model accuracy."}
{"hash_id": 2075658437620581890, "entities": ["numerical reasoning", "information fusion", "large language models", "complex analytics"], "background": "1. The need to evaluate and improve the numerical reasoning and information fusion abilities of large language models when dealing with a mix of text and numerical data. 2. The potential of large language models to process vast amounts of text and data, which could be harnessed for complex analytics in various domains, such as sports data."}
{"hash_id": 1576612397044653988, "entities": ["binary link prediction", "literaturebased hypothesis", "large language models", "nuanced scientific contexts"], "background": "1. The need to go beyond the traditional binary link prediction in literature-based hypothesis generation to enhance the expressivity and novelty of scientific ideas. 2. The desire to leverage the strong progress in large language models to capture nuanced scientific contexts and generate novel directions grounded in existing literature."}
{"hash_id": 793359736268958667, "entities": ["computational burden reduction", "visually conditioned language generation", "optimization process simplification"], "background": "1. The need to reduce the computational burden and resource requirements of pre-training visually conditioned language generation models, particularly in research environments with limited computational resources. 2. The desire to simplify the optimization process and remove the complexities associated with multi-stage training and hyperparameter tuning in existing models."}
{"hash_id": 418998089294336695, "entities": ["large language models", "critical evaluation", "trustworthiness", "prompting techniques", "confidence misalignments"], "background": "1. The increasing integration of Large Language Models (LLMs) into high-stakes areas requires a critical evaluation of their trustworthiness and behavior. 2. Existing prompting techniques rely on models' self-evaluation of confidence, but misalignments between expressed confidence and internal probabilities could lead to misleading results and impact real-world decision-making."}
{"hash_id": 4987634211664958819, "entities": ["crosslingual knowledge editing", "knowledge updating", "multilingual settings", "computational expense"], "background": "1. The need to update knowledge in LLMs without the computational expense and unreliability of fine-tuning, especially in multilingual settings where knowledge editing in one language must reflect across all languages. 2. The absence of effective methods for cross-lingual knowledge editing that are scalable and can handle large volumes of edits while maintaining accuracy."}
{"hash_id": 4658296944293411800, "entities": ["pronoun disambiguation", "multimodal reasoning", "texttoimage models", "complex textvisual matching"], "background": "1. The need to extend common-sense reasoning into multimodal domains, particularly for pronoun disambiguation tasks, which are essential for improving the interpretability and effectiveness of text-to-image models. 2. The gap in existing tools and models that can accurately match complex texts with visuals, which is crucial for applications in education and digital media."}
{"hash_id": 4810635051382267494, "entities": ["documentlevel relation extraction", "largescale noisy training data", "architectural limitations"], "background": "1. The need to improve performance on document-level relation extraction by effectively utilizing large-scale, noisy training data. 2. The recognition that existing methods have architectural limitations that prevent them from fully leveraging the potential of such data."}
{"hash_id": 142299639767080153, "entities": ["robustness challenges", "spurious correlations", "imbalanced label distributions", "conceptlevel features", "lack of concept labels"], "background": "1. The need to address robustness challenges in language models caused by spurious correlations at the concept level, which arise from imbalanced label distributions in training data or in-context learning exemplars. 2. The absence of focus on concept-level features in previous research due to the lack of concept labels and the difficulty in identifying conceptual content in input texts."}
{"hash_id": 1786763545244245489, "entities": ["probabilistic nature", "commonsense", "nuanced evaluation", "machine commonsense"], "background": "1. The need to capture the probabilistic nature of commonsense, where multiple answers can be correct, and evaluate models accordingly. 2. The desire to create a more nuanced and accurate evaluation of machine commonsense that reflects diverse human perspectives and real-world scenarios."}
{"hash_id": 5856731545823541513, "entities": ["jailbreak prompts", "resourceintensive detection", "efficient approach", "llm safety"], "background": "1. Existing methods for detecting jailbreak prompts are resource-intensive and require extensive data collection and training processes. 2. There is a need for a more efficient and accurate approach to ensure the safety of LLMs against jailbreak prompts without additional finetuning."}
{"hash_id": 2682718699623758473, "entities": ["specialized empathy understanding", "domainspecific context", "figurative language role", "blackbox approaches gap"], "background": "1. The need for more nuanced and specialized understanding of empathy in online environments, particularly in domain-specific contexts such as mental health issues related to acne. 2. The gap in current research that has predominantly relied on black-box approaches and failed to consider the role of figurative language in expressing empathy online."}
{"hash_id": 6663037442859266247, "entities": ["semantic understanding gaps", "negation and implicature", "intent embedding models"], "background": "1. The current evaluation benchmarks for conversational systems do not adequately measure the semantic understanding gaps, specifically in relation to negation and implicature. 2. The rise of Large Language Models (LLMs) and prompt-based encoding has created an opportunity to improve the semantic capabilities of intent embedding models in real-world conversational systems."}
{"hash_id": 1580919173683960651, "entities": ["unified understanding", "eif task performance", "embodied learning", "llmcentric designs", "multiagent communication"], "background": "1. The lack of a unified understanding of how different components, such as visual perception and action execution, affect EIF task performance in embodied learning. 2. The need to improve the performance of EIF tasks by leveraging LLM-centric designs and multi-agent communication strategies."}
{"hash_id": 8698971195841864407, "entities": ["evaluation methodology", "medical text generation", "automatic evaluation methods"], "background": "1. The need for a reliable evaluation methodology to help medical experts decide whether to adopt text generation systems and which system to use. 2. The high cost and poor scalability of human evaluation and the lack of fine-grained evaluation aspects in existing automatic evaluation methods for medical text generation."}
{"hash_id": 4127896050405576094, "entities": ["llm proficiency", "formatfollowing capabilities", "ai agent applications"], "background": "1. The lack of comprehensive benchmarks to evaluate LLMs' proficiency in following formats, which is crucial for their application as AI agents in various domains. 2. The need to understand and improve LLMs' format-following capabilities to enhance their practicality and effectiveness in tasks that require adherence to specific formats."}
{"hash_id": 1187437380503171755, "entities": ["finegrained semantic capture", "sentence representations", "computational efficiency gains", "hypernetworks"], "background": "1. The need to improve the fine-grained semantic capture of sentence representations, especially when conditioning on specific perspectives, which is not fully achieved by current state-of-the-art sentence embeddings. 2. The potential computational efficiency gains from using hypernetworks to dynamically adapt sentence representations without the need for large-scale retraining."}
{"hash_id": 5301941241777131251, "entities": ["crosslingual transfer", "multilingual language models", "lowresource languages", "multisource language training", "optimal language combinations"], "background": "1. The need to improve the effectiveness of cross-lingual transfer in multilingual language models, especially when dealing with low-resource languages. 2. The desire to understand the mechanisms behind the success of multi-source language training and to provide practical guidance for selecting optimal language combinations for transfer."}
{"hash_id": 856635414188972204, "entities": ["multilingual nlp systems", "evaluation benchmarks", "lowresource languages"], "background": "1. The lack of high-quality, parallel evaluation benchmarks for assessing the multilingual capabilities of NLP systems, particularly in low-resource languages. 2. The need to understand the true extent of multilingual abilities in large language models (LLMs) and to bridge the technological gap between high-and low-resource languages."}
{"hash_id": 5303338049861815961, "entities": ["training proof paths", "inference tactics", "failed attempts", "trialanderror data"], "background": "1. The discrepancy between training on successful proof paths and the need to sample various tactics during inference, which suggests that learning from failed attempts could improve the model's performance. 2. The lack of trial-and-error data in existing datasets, which could provide valuable feedback for the model to avoid previously failed tactics and thus enhance the efficiency of proof search."}
{"hash_id": 5876736062437657324, "entities": ["training data influence", "model debugging", "scalability challenges", "influence estimation methods", "llms training datasets"], "background": "1. The need to trace back the influence of training data on specific generations from large language models, which is crucial for model debugging, improving explainability, and ensuring data quality and security. 2. The scalability challenges of existing influence estimation methods, which do not efficiently handle the massive scale of LLMs and their trillion-token training datasets."}
{"hash_id": 1792034781030370710, "entities": ["benchmark gap", "visually grounded tasks", "autonomous agents assessment"], "background": "1. The need to bridge the gap between the capabilities of current benchmarks and the real-world visually grounded tasks that require agents to process both visual and textual information. 2. The desire to advance the development of autonomous agents by providing a rigorous assessment that simulates human interaction with modern computing interfaces."}
{"hash_id": 6442060765756506843, "entities": ["automated evaluation methods", "human judgment correlation", "text summarization models", "rouge limitations", "detailed feedback on summary quality"], "background": "1. The need for automated evaluation methods that better correlate with human judgment and provide fine-grained analysis for improving text summarization models. 2. The limitation of existing evaluation metrics, such as ROUGE, in terms of their weak correlation with human evaluations and the lack of detailed feedback on summary quality."}
{"hash_id": 5534979838884701829, "entities": ["multimodal instructiontune data", "videotext alignment", "supervised finetuning", "grounding challenge"], "background": "1. The lack of sufficient quality and quantity of multimodal instruction-tune data hinders the alignment of video and text modalities in large multimodal models. 2. Existing methods like Supervised Fine-Tuning (SFT) do not adequately address the challenge of poor grounding of video content in the alignment process."}
{"hash_id": 7457972349435338344, "entities": ["standardized problem setting", "shared benchmark dataset", "ad text generation techniques"], "background": "1. The need for a standardized problem setting to enable comparability and generalization of ad text generation techniques across different studies and applications. 2. The absence of a shared benchmark dataset that hinders reproducibility and the ability to evaluate and compare ad text generation models objectively."}
{"hash_id": 780198790785756730, "entities": ["llm abstraction ability", "human intelligence", "nlp tasks", "pretraining alignment"], "background": "1. The current state-of-the-art LLMs are deficient in abstraction ability, which is crucial for human intelligence and various NLP tasks, yet improving this ability in LLMs has been unexplored. 2. Enhancing abstraction in LLMs is challenging due to the lack of alignment between the knowledge acquired during pre-training and the instructions given for abstraction tasks."}
{"hash_id": 3436351897276507973, "entities": ["limited offline datasets", "reinforcement learning", "interactive decisionmaking"], "background": "1. Language models do not perform well when fine-tuned using supervised learning on limited offline datasets for complex, multi-round interaction tasks. 2. There is a need for a more effective approach to train language models within interactive decision-making environments using reinforcement learning."}
{"hash_id": 2708886120893774101, "entities": ["unsupervised verb metaphors detection", "chatgpt performance", "tacit knowledge", "metaphors detection systems"], "background": "1. The need to improve ChatGPT's performance on unsupervised verb metaphors detection, which is crucial for various NLP tasks, without relying on large-scale labeled data. 2. The desire to leverage the tacit knowledge embedded in ChatGPT to enhance the generalization and accuracy of metaphors detection systems."}
{"hash_id": 5319321343658471128, "entities": ["balance performance", "instructionfollowing abilities", "catastrophic forgetting"], "background": "1. The challenge of maintaining a balance between high performance on specific tasks and preserving the model's original general instruction-following abilities during fine-tuning. 2. The need to mitigate catastrophic forgetting where the model loses previously learned knowledge when adapted to new tasks."}
{"hash_id": 8341300909104985629, "entities": ["filter modules", "noise compression", "retrievalaugmented generation", "answer generation", "compressed information"], "background": "1. The suboptimal performance of existing filter modules in noise compression within retrieval-augmented generation. 2. The need to improve both the correctness of answer generation and the conciseness of the compressed information in noisy data."}
{"hash_id": 675142816771932797, "entities": ["label leakage", "evaluation metrics", "human judgment"], "background": "1. Existing evaluation metrics for free-text rationales are not robust against label leakage, which can lead to incorrect assessments of rationale quality. 2. There is a need for a more reliable evaluation method that aligns with human judgment and accurately measures the informativeness of rationales without being influenced by spurious label information."}
{"hash_id": 316101641225847242, "entities": ["human language", "protein language", "large language models", "balanced dataset", "bidirectional proteintext generation"], "background": "1. The need to bridge the gap between human language and protein language to improve the understanding and generation of protein sequences by Large Language Models. 2. The requirement for a more balanced and instructionally rich dataset to fine-tune LLMs for bidirectional protein-text generation tasks."}
{"hash_id": 4466174791062644480, "entities": ["semantic textual similarity", "conditional sts", "linguistic foundation"], "background": "1. To reduce ambiguity and improve the evaluation of semantic textual similarity by addressing issues in the Conditional STS (C-STS) dataset. 2. To provide a linguistic foundation for constructing C-STS data with clear and semantically grounded conditions."}
{"hash_id": 8531397074775551675, "entities": ["improve reasoning abilities", "large language models", "chainofthought prompting"], "background": "1. The need to improve the reasoning abilities of large language models, which are crucial for a wide range of complex tasks. 2. The potential to leverage chain-of-thought prompting as a means to significantly enhance these models' capabilities, as evidenced by recent research."}
{"hash_id": 9106513307858201202, "entities": ["temporal reasoning benchmark", "performance gaps", "llms", "human cognition"], "background": "1. The lack of a comprehensive temporal reasoning benchmark that evaluates the full range of temporal reasoning phenomena in large language models. 2. The need to understand and quantify the performance gaps and discrepancies in temporal reasoning capabilities across different types of reasoning and between state-of-the-art LLMs and human cognition."}
{"hash_id": 3050461725189478268, "entities": ["language models", "analogical reasoning", "largerscale data sources"], "background": "1. The current language models struggle with analogical reasoning due to the limited scale of available resources for training. 2. Larger-scale data sources are needed to train specialized analogy-making models and improve the analogical reasoning capabilities of both smaller LMs and LLMs."}
{"hash_id": 5790914791347836282, "entities": ["continual learning", "dialogue systems", "knowledge transfer", "dialogue state tracking", "realworld applications"], "background": "1. The need for dialogue systems to continually learn new tasks without forgetting previously acquired knowledge, which is crucial for their adaptability and effectiveness in real-world applications. 2. The potential for improving knowledge transfer between tasks by leveraging the inherent correlations in dialogue state tracking domains, which current methods overlook."}
{"hash_id": 7798546647451385610, "entities": ["limitation of rag systems", "lengthy document processing", "suboptimal chunking", "semantic coherence"], "background": "1. The need to overcome the limitations of traditional RAG systems, which struggle with processing lengthy documents and filtering out irrelevant content, leading to inaccurate response generation. 2. The desire to eliminate the suboptimal chunking process, which disrupts semantic coherence and results in incomplete and incoherent retrieved information."}
{"hash_id": 5008013519584024902, "entities": ["effective prompting", "chainofthought reasoning", "exemplars annotation"], "background": "1. The need to improve the performance of large language models on complex reasoning tasks through effective prompting with chain-of-thought reasoning. 2. The requirement to reduce human effort in selecting the most effective exemplars for annotation, given the variability in reasoning tasks and the emergent abilities of large language models."}
{"hash_id": 1230152829127275884, "entities": ["improve retrieval accuracy", "large language models", "stylistic deviations", "existing metrics inadequacy"], "background": "1. The need to improve retrieval accuracy in code search when using Large Language Models, which is currently constrained by stylistic deviations in generated code. 2. The recognition that existing metrics are inadequate in capturing the stylistic nuances that affect the relevance and quality of code search results."}
{"hash_id": 8648836358930523152, "entities": ["unifying framework", "lexical semantic change", "computational social science", "comprehensive tool", "social cultural change"], "background": "1. The need for a unifying framework to integrate the multiple dimensions of lexical semantic change detected by historical linguists and computational linguists. 2. The desire to provide computational social scientists with a comprehensive tool to understand and model social and cultural change through language."}
{"hash_id": 5260148236163066939, "entities": ["catastrophic forgetting", "continual learning", "rehearsalbased methods"], "background": "1. Large language models (LLMs) often experience catastrophic forgetting when updated in a continual learning manner, which hinders their performance in real-world applications. 2. Traditional rehearsal-based methods to mitigate forgetting require access to the original training data, which may not be available, especially when working with publicly-released LLM checkpoints."}
{"hash_id": 4153514282355829200, "entities": ["large language models", "code generation", "traditional verification properties"], "background": "1. The current challenge of large language models (LLMs) in code generation, where the correct solution is not always generated in a single attempt. 2. The need to move beyond traditional verification properties from software engineering that are assumed to be superior to generated code solutions, but are often produced by the same model."}
{"hash_id": 7952254047926829937, "entities": ["llmbased chatbots", "mitigate hallucinations", "sensitive scenarios"], "background": "1. The need to mitigate hallucinations in LLM-based chatbots without requiring additional training or data annotation. 2. The desire to enhance the reliability of chatbots in sensitive scenarios such as healthcare and education by providing citable evidence for generated responses."}
{"hash_id": 3791005376390483818, "entities": ["standardized evaluation", "watermarking algorithms", "text generation quality"], "background": "1. The need for a standardized and unbiased evaluation method to ensure the effectiveness and fairness of watermarking algorithms for large language models. 2. The requirement to maintain the quality of text generation while inserting watermarks to avoid compromising the utility of the language models."}
{"hash_id": 1959872426822278924, "entities": ["inductive biases", "syntactic structures", "dependency structures", "selfattention mechanism"], "background": "1. The lack of inductive biases of syntactic structures in the Transformer architecture, which could improve generalization in language modeling tasks. 2. The potential synergy between dependency structures, which focus on token relationships, and the self-attention mechanism in Transformers."}
{"hash_id": 2083318077591631026, "entities": ["accumulated delays", "error propagation", "synchronization", "realtime translation"], "background": "1. The need to overcome the accumulated delays and error propagation in existing pipeline methods for simultaneous speech-to-speech translation. 2. The desire to improve synchronization between the speaker and listener in real-time translation scenarios."}
{"hash_id": 8973622617703596271, "entities": ["privacy concerns", "benchmark datasets", "pretraining data contamination", "detection method"], "background": "1. The need to address privacy concerns and the potential leakage of benchmark datasets in the pre-training phase of LLMs. 2. The requirement for a more reliable method to detect pre-training data contamination, as existing approaches based on superficial features like generated texts or loss metrics are not always trustworthy."}
{"hash_id": 8223586003329078378, "entities": ["swift analysis", "complex events", "temporal complex events"], "background": "1. The need for swift and precise analysis of complex events in the ever-increasing volume of online news. 2. The limitations of existing natural language processing techniques in handling the temporal dynamics and extensive text of Temporal Complex Events (TCE)."}
{"hash_id": 593187633180410588, "entities": ["controllable drama scripts", "interactive storytelling", "character consistency", "plot coherence"], "background": "1. The need for more controllable and interactive drama scripts generated by language models, as current models may struggle with constraints at the storyline level. 2. The desire to leverage the advanced reasoning abilities of large language models for creative storytelling, while ensuring character consistency and plot coherence."}
{"hash_id": 1469572366826210611, "entities": ["improve sample efficiency", "reinforcement learning", "large language action spaces", "adapt pretrained models", "rl environment characteristics"], "background": "1. The need to improve sample efficiency in reinforcement learning, especially in scenarios with large natural language action spaces, which can lead to a curse of dimensionality. 2. The requirement to adapt pretrained language models to the specific characteristics of an RL environment without the need for additional domain-specific datasets or assumptions about action admissibility."}
{"hash_id": 1920738508337183841, "entities": ["computational cost reduction", "long context scenarios", "llm performance improvement", "noise introduction", "position bias"], "background": "1. The need to reduce the computational cost and latency associated with using large language models in long context scenarios, which can also lead to significant financial savings. 2. The requirement to improve the performance of LLMs by overcoming the challenges of noise introduction due to longer prompts and position bias affecting the capture of relevant information."}
{"hash_id": 8326227273387373036, "entities": ["missing modality conditions", "model performance degradation", "computational infeasibility"], "background": "1. The need to handle real-world scenarios where missing modality conditions occur frequently, leading to a degradation in model performance. 2. The computational infeasibility and instability of finetuning large-scale multimodal models on small datasets or in low-resource domains."}
{"hash_id": 6847107200060713400, "entities": ["integrated gec processes", "large language models", "overcorrection phenomenon"], "background": "1. The need to improve the efficiency and accuracy of GEC by integrating the detection and correction processes, which have traditionally been separate, to better harness the potential of large language models. 2. The desire to address the over-correction phenomenon that can occur with the use of large language models in GEC, which leads to suboptimal performance compared to smaller models."}
{"hash_id": 2037421031543090131, "entities": ["explainable ai", "llm preferences", "ai reliability", "nuanced alignment"], "background": "1. The need for more explainable and quantitative understanding of human and LLM preferences to avoid issues like over-optimization and reward hacking in language models. 2. The requirement to improve the reliability of AI systems by aligning them with more nuanced and strategically adaptable preferences."}
{"hash_id": 4427464463108389883, "entities": ["cosine saturation zones", "text embeddings", "sts benchmarks"], "background": "1. The cosine function, commonly used in STS tasks, has saturation zones that lead to vanishing gradients, preventing the learning of subtle semantic differences in text embeddings. 2. Existing text embedding models neglect the effects of cosine saturation zones, resulting in poor performance on STS benchmarks and downstream tasks, especially with long texts."}
{"hash_id": 4321336378429896364, "entities": ["character fidelity", "roleplaying agents", "selfreport assessments"], "background": "1. The need for a more nuanced understanding of character fidelity in role-playing agents beyond knowledge and linguistic patterns. 2. The limitations of self-report assessments in capturing the true personalities of RPAs, including contradictions in role-playing instructions and the potential for biased responses."}
{"hash_id": 3139444260885347453, "entities": ["zeroshot text detection", "random perturbation", "contrastive learning approach", "language pattern differences"], "background": "1. To enhance the performance and robustness of machine-generated text detection, particularly in zero-shot settings, by overcoming the limitations of random perturbation and logit regression threshold dependency in DetectGPT. 2. To bridge the gap between fine-tuned and metric-based detection methods by integrating a novel contrastive learning approach that better utilizes perturbations to capture language pattern differences between human-written and machine-generated texts."}
{"hash_id": 1453330330057634167, "entities": ["standardized definition", "factual claim detection", "scalable annotation approach"], "background": "1. The need for a standardized definition of factual claims to improve consistency and generalizability in the task of factual claim detection. 2. The requirement for a more cost-effective and scalable approach to annotating factual claims, given the volume of online content and the limitations of manual annotation."}
{"hash_id": 599689601139485822, "entities": ["hallucinated answers", "evidencebased qa", "data creation", "finetuning methods"], "background": "1. The current LLMs exhibit a high rate of hallucinated answers and false citations in Evidence-Based QA, which hinders their reliability and traceability in practical applications. 2. There is a need for efficient and scalable data creation and fine-tuning methods to enhance the performance of LLMs in Evidence-Based QA without compromising their generalisability."}
{"hash_id": 6137991942073222278, "entities": ["preservation of world knowledge", "supervised finetuning", "scalable adaptation method"], "background": "1. The need to preserve the world knowledge within large language models while enhancing their performance on downstream tasks through supervised fine-tuning with increased instruction data. 2. The requirement for a scalable and efficient method that can adapt LLMs to a wide range of tasks without overwriting the existing knowledge in the models."}
{"hash_id": 7039675822224381472, "entities": ["factual inaccuracies mitigation", "llm selfevaluation", "internal knowledge utilization"], "background": "1. The need to mitigate factual inaccuracies (hallucinations) in LLMs without relying on high-quality human annotations, which are scarce and expensive. 2. The recognition that LLMs have the potential to self-evaluate their own responses and utilize their internal knowledge to improve factuality."}
{"hash_id": 5873405389497615086, "entities": ["audiocentric interaction", "lalms", "standardized evaluation"], "background": "1. The lack of benchmarks capable of evaluating the audio-centric interaction capabilities of LALMs, which hinders advancements in the field and makes it difficult to track progress. 2. The need to provide guidance for future research and improvements in LALMs by revealing their limitations through standardized evaluation."}
{"hash_id": 4332150340575147218, "entities": ["humanai systems", "llms integration", "evaluation methods", "realworld scenarios"], "background": "1. The increasing integration of LLMs into diverse human-AI systems, including critical domains, which raises concerns about their potential misalignment with human values. 2. The need for reliable evaluation methods that correlate with authentic human-AI interactions, as current controlled settings do not adequately reflect real-world scenarios."}
{"hash_id": 2318447569878920214, "entities": ["bilingual lexicon induction", "linear mapping", "diverse structures", "static subspace assignments", "poor local optima"], "background": "1. Existing bilingual lexicon induction methods struggle with accuracy, especially for distant and low-resource language pairs, due to the reliance on a single linear mapping for language alignment, which does not account for the diverse structures within embedding spaces. 2. The need to overcome the limitations of static subspace assignments and the tendency to get stuck in poor local optima when initial mappings are suboptimal."}
{"hash_id": 7447675526003694000, "entities": ["interpretable sentence representations", "disentangling semantic features", "deep neural models integration"], "background": "1. The need for more interpretable and controllable sentence representations in NLP, particularly focusing on disentangling general sentence semantic features beyond task-specific factors like sentiment. 2. The limitation of current methods in localizing and controlling different semantic factors in the latent space, which hinders the integration of deep neural models with formal linguistic representations."}
{"hash_id": 1195514579994125301, "entities": ["automated story premise generation", "automatic story generation asg", "current method limitations"], "background": "1. The need for an automated method to generate diverse and high-quality story premises to improve the effectiveness of Automatic Story Generation (ASG) frameworks. 2. The desire to overcome the limitations of current methods, such as inconsistent quality, limited customization, and the labor-intensive nature of human-curated premises."}
{"hash_id": 6402295576707492680, "entities": ["outlier detection", "openset semisupervised", "indistribution texts", "measurement disagreements"], "background": "1. The need to improve outlier detection in open-set semi-supervised text classification, as out-of-distribution texts are often incorrectly recognized as in-distribution, leading to suboptimal performance. 2. The potential to enhance the\u533a\u5206 between in-distribution and out-of-distribution examples by maximizing measurement disagreements, which could lead to better overall classification outcomes."}
{"hash_id": 5434147087765418541, "entities": ["tool utilization", "safety concerns", "llms", "safety challenges"], "background": "1. The current research primarily focuses on enhancing the capabilities of LLMs through tool utilization, neglecting the safety concerns that arise from their integration with tools. 2. There is an urgent need for a comprehensive analysis of the safety challenges faced by LLMs in tool learning to ensure their safe and reliable application in real-world scenarios."}
{"hash_id": 9186963887329940446, "entities": ["improve nli generalization", "distributiongeneral abilities", "small models fast inference"], "background": "1. The need to improve the generalization of NLI models to new and unseen text domains, which is critical for practical applications such as fact-checking and source attribution of LLM outputs. 2. The lack of exploration of distribution-general abilities of models for downstream semantic tasks beyond NLI itself, especially when using small models and fast inference is key."}
{"hash_id": 3860428788572627409, "entities": ["posttraining quantization", "large language models", "catastrophic forgetting"], "background": "1. The need to improve the suboptimal performance of existing post-training quantization methods for large language models, particularly when faced with more challenging configurations and unseen datasets. 2. The requirement to enhance the robustness of quantized models without the need for retraining, while also avoiding catastrophic forgetting when adapting to new data."}
{"hash_id": 6088959014502887784, "entities": ["data quality", "nlp benchmarks", "annotation errors"], "background": "1. The need to improve data quality and consistency in NLP benchmarks, which directly impacts machine learning performance and user trust. 2. The absence of prior work focusing on differentiating between annotation errors and legitimate human label variation, especially in complex scenarios where the signal is not binary."}
{"hash_id": 8976390267211014229, "entities": ["large language models", "computational argumentation", "systematic evaluation"], "background": "1. The need to leverage the capabilities of large language models in the field of computational argumentation, which is crucial for various domains like law and public policy. 2. The absence of a systematic evaluation and standardization of computational argumentation tasks that include both argument mining and generation, which hinders progress in the field."}
{"hash_id": 2804463813320855708, "entities": ["resourcelimited finetuning", "large language models", "parameterefficient finetuning"], "background": "1. The need to fine-tune Large Language Models (LLMs) under limited resources, especially when dealing with complex, knowledge-intensive tasks that require a larger model capacity. 2. The desire to improve upon the limited fine-tuning performance of existing Parameter-Efficient Fine-tuning (PEFT) methods due to constrained model capacity and the fixed memory capacity of GPUs."}
{"hash_id": 2220265119850674119, "entities": ["error accumulation prevention", "stepbystep adjustments", "llm reasoning accuracy"], "background": "1. The increasing complexity of reasoning tasks leads to a higher likelihood of errors in LLMs, which motivated the need for a method that can dynamically intervene during the reasoning process to prevent error accumulation. 2. Existing methods lack fine-grained, step-by-step adjustments to guide LLM reasoning, which inspired the development of a more adaptive and responsive approach to improve reasoning accuracy."}
{"hash_id": 3139224557043404570, "entities": ["hallucinations in language models", "multimodal reasoning", "knowledge source integration"], "background": "1. To overcome the challenge of hallucinations generated by large language models when used as an implicit knowledge source in visual question answering. 2. To improve the alignment and integration of multiple knowledge sources, such as images, knowledge graphs, and large language models, for more effective multimodal reasoning."}
{"hash_id": 3783661116731376134, "entities": ["kv caching", "memory overhead", "inference latency", "datafree compression", "quantization techniques"], "background": "1. The need to reduce memory overhead and inference latency in large language models using key-value (KV) caching without compromising generation quality. 2. The requirement for a data-free approach to KV cache compression to overcome the limitations of current quantization techniques that rely on calibration or training data."}
{"hash_id": 1491212004821257581, "entities": ["improve longcontext language models", "multihop reasoning tasks", "enhance robustness", "noisy contexts", "smallerscale models"], "background": "1. The need to improve the performance of long-context language models in multi-hop reasoning tasks, where they currently underperform compared to leading multi-hop reasoning systems. 2. The desire to enhance the robustness of these models in the presence of noisy contexts and their ability to incorporate knowledge effectively, especially for smaller-scale models."}
{"hash_id": 6087738499965118655, "entities": ["extreme miscalibration", "adversarial robustness", "nlp model evaluation"], "background": "1. The existence of extreme miscalibration in NLP models, which can create an illusion of adversarial robustness that is not genuine, leading to potential security vulnerabilities and incorrect assessments of model performance. 2. The need for more reliable and accurate methods to evaluate the robustness of NLP models against adversarial attacks, ensuring that improvements in robustness are not just superficial."}
{"hash_id": 8619914002473728438, "entities": ["matthew effect mitigation", "conversational recommendation", "preference learning complexity"], "background": "1. The need to alleviate the Matthew effect in conversational recommendation systems, where popular items are overexposed and less popular ones are underrepresented, especially as user-system interactions evolve over time. 2. The requirement to extend beyond traditional pairwise interactions in preference learning to capture the intricate complexity of user preferences, which involves multiple factors simultaneously."}
{"hash_id": 7649268554569239623, "entities": ["distant supervision", "scientific nli tasks", "semisupervised learning", "suboptimal performance"], "background": "1. The noise in the labels obtained through distant supervision hampers the performance of classifiers in Scientific NLI tasks, and traditional semi-supervised learning methods are not effective in mitigating this issue. 2. Existing semi-supervised learning approaches either discard a large number of examples or do not fully utilize the historical behavior of the classifiers, leading to suboptimal performance."}
{"hash_id": 8280960006634592663, "entities": ["temporal variations", "language models", "adaptation techniques"], "background": "1. The need to adapt language models to temporal variations in language use to prevent performance degradation when applied to data from different time periods. 2. The challenge of designing adaptation techniques due to the multitude of time scales and potential unavailability of data from the target time period."}
{"hash_id": 4124164875957164556, "entities": ["longer contexts processing", "language models", "positional encodings", "computational costs", "transformers"], "background": "1. The need to process longer contexts for complex tasks such as summarizing books or answering questions with extensive retrieved information, which current language models and positional encodings struggle to handle. 2. The desire to reduce the high computational and memory costs associated with extending the context window of large language models, especially when using transformers."}
{"hash_id": 1563339948478847441, "entities": ["machine learning models", "outofdistribution data", "singlesource domain generalization"], "background": "1. The need to improve the robustness and generalization of machine learning models, particularly pre-trained language models, when faced with out-of-distribution (OOD) data during testing. 2. The absence of a satisfactory solution for single-source domain generalization in text classification, where models must perform well on unseen domains without access to target domain information or the need for model tuning."}
{"hash_id": 7587014628532310944, "entities": ["multilingual benchmark", "linguistic acceptability", "crosslingual transfer", "syntaxrelated tasks"], "background": "1. The lack of a large-scale multilingual benchmark to systematically test the linguistic acceptability of sentences in various languages. 2. The need to explore and improve the cross-lingual transfer capabilities and syntax-related task performance of multilingual language models."}
{"hash_id": 767849141787944534, "entities": ["improve accuracy", "contextual named entities", "downstream tasks", "multitoken entities"], "background": "1. The need to improve the accuracy of transcribing contextual named entities, which are crucial for semantic understanding in downstream tasks but are often inaccurately transcribed due to their infrequent occurrence in training data. 2. The recognition that previous approaches treated entities as individual tokens, leading to incomplete and error-prone transcriptions, and the desire to preserve the integrity of multi-token entities during decoding."}
{"hash_id": 5689696568957369101, "entities": ["join relationships", "realworld opendomain settings", "data lakes", "question decomposition"], "background": "1. The limitations of previous methods that assume the answer to a question can be found in a single table or through question decomposition, which may not capture the necessary join relationships between tables. 2. The need to infer join relationships among tables in real-world open-domain settings, especially when dealing with data lakes or large dumps of tables without provided key-foreign-key constraints."}
{"hash_id": 6845062059034853057, "entities": ["improve inductive ability", "large language models", "deductive capability"], "background": "1. The need to improve the limited inductive ability of Large Language Models (LLMs) which is crucial for tasks like discovering consistent transformations from input-output pairs. 2. The recognition that while LLMs struggle with induction, they have a strong deductive capability which can be leveraged to enhance their inductive performance."}
{"hash_id": 1753927298940917401, "entities": ["performance gap", "mathematical reasoning", "scalable method", "diverse math problems"], "background": "1. The need to bridge the performance gap between open-source and closed-source large language models in mathematical reasoning tasks. 2. The requirement for a scalable and cost-effective method to generate high-quality and diverse math problems with reliable solutions for training large language models."}
{"hash_id": 4926875809844292674, "entities": ["traditional modular dialogue systems", "error accumulation", "taskoriented dialogue systems"], "background": "1. To overcome the limitations of traditional modular dialogue systems, which suffer from error accumulation, poor generalization, high customization costs, and low fault tolerance. 2. To reduce the dependency on large amounts of fully-annotated data and to enhance the flexibility and robustness of task-oriented dialogue systems in new dialogue scenarios."}
{"hash_id": 6027804513059441034, "entities": ["limited llms applicability", "irspecific concepts", "aligning llm capabilities"], "background": "1. The limited applicability of large language models (LLMs) in information retrieval (IR) tasks due to the infrequent occurrence of IR-specific concepts in natural language. 2. The need to align LLMs' capabilities with human tasks and preferences, especially in understanding and executing complex IR tasks."}
{"hash_id": 2377295924459370297, "entities": ["incontext learning", "pretrained language models", "performance variance"], "background": "1. The need to improve the average and worst-case accuracy of in-context learning across diverse pre-trained language models and tasks. 2. The requirement to reduce performance variance that arises from varying demonstrations, permutations, and templates."}
{"hash_id": 2472457025867762809, "entities": ["high costs resource consumption", "parameterefficient method", "model scaling"], "background": "1. The need to reduce the high costs and unaffordable resource consumption associated with full finetuning of large language models, especially when deploying multiple LoRAs concurrently. 2. The requirement for a more parameter-efficient method that can improve performance with the same or fewer trainable parameters, particularly as model sizes continue to scale up."}
{"hash_id": 3597862421078041971, "entities": ["zeroshot event detection", "limited event diversity", "model performance enhancement"], "background": "1. Existing zero-shot event detection approaches yield sporadic successes and generally do not meet expectations, often due to the limited diversity of event types and definitions in training datasets. 2. The need to improve model performance by enhancing their ability to follow event definitions, especially when faced with unseen event types."}
{"hash_id": 7846085397936308513, "entities": ["conversational dense retrieval", "interpretability", "session embeddings", "search intent", "trustworthiness"], "background": "1. The lack of interpretability in conversational dense retrieval models hinders the understanding of model behaviors and targeted improvements, leading to obstacles in enhancing search effectiveness and addressing potential biases or errors. 2. The opacity of session embeddings in conversational dense retrieval makes it difficult to decipher the models' comprehension of search intent, which is crucial for trustworthiness and user satisfaction in conversational search."}
{"hash_id": 188948212678255952, "entities": ["large language models", "text detectors", "realistic attacks"], "background": "1. The increasing misuse of large language models generating persuasive and human-like text, which raises concerns about deception, academic misconduct, and disinformation. 2. The need to evaluate and improve the robustness of machine-generated text detectors against a wide range of realistic attacks to ensure their effectiveness in preventing such misuse."}
{"hash_id": 7809254163348371254, "entities": ["improve event coreference resolution", "avoid pseudofeatures", "large language models", "taskspecific adaptation"], "background": "1. The need to improve the understanding of complex and diverse contexts in event coreference resolution to avoid learning pseudofeatures. 2. The desire to harness the powerful contextual understanding of large language models while overcoming their limitations in task-specific adaptation for information extraction tasks."}
{"hash_id": 3592483307229206676, "entities": ["dependency on costly data", "bounded rationality", "division of labor"], "background": "1. The need to reduce dependency on costly and non-reproducible data reliance, especially from closed-source models, for agent learning in QA tasks. 2. The aim to improve the efficiency and effectiveness of language agents by adhering to the principle of bounded rationality through division of labor."}
{"hash_id": 310658951165660907, "entities": ["inefficient assembly code search", "manual heuristic approaches", "highquality dataset scarcity", "compilation process complexities"], "background": "1. The current methods for assembly code search are inefficient, relying on manual and heuristic approaches that are time-consuming and require extensive experience. 2. The scarcity of high-quality datasets for assembly code search due to the complexities of the compilation process hinders the development of more effective search algorithms."}
{"hash_id": 2372166849991071782, "entities": ["computational overhead reduction", "memory demands reduction", "generalization ability", "lowrank adaptation methods"], "background": "1. The need to reduce the computational overhead and memory demands associated with fine-tuning large language models on diverse tasks. 2. The desire to improve generalization ability and performance while maintaining the computational efficiency of low-rank adaptation methods."}
{"hash_id": 7057790530378224893, "entities": ["enhance llms reasoning", "learning from past mistakes", "costeffective strategies", "leveraging errors"], "background": "1. The need to enhance LLMs' reasoning abilities by incorporating a crucial aspect of human cognition: learning from past mistakes. 2. The potential for cost-effective strategies to improve LLMs by leveraging errors, which requires less effort than creating hand-crafted golden references."}
{"hash_id": 5491339632838726144, "entities": ["associated words", "dialogue understanding", "emotional states", "cognitive states"], "background": "1. Existing approaches overlook the associated words between dialogue utterances, leading to a lack of nuanced understanding of emotional and cognitive states. 2. The human process of understanding empathy involves an iterative association of pivotal associated words, which current models do not effectively simulate."}
{"hash_id": 3659523331824658221, "entities": ["vulnerability of llms", "attack prompts", "knowledge editing methods", "toxic regions", "llm performance"], "background": "1. The need to address the vulnerability of LLMs to carefully crafted attack prompts that can elicit harmful content and disrupt social order. 2. The potential of knowledge editing methods to make precise, permanent adjustments to the toxic regions of LLMs without significantly compromising overall performance."}
{"hash_id": 6928942790094214453, "entities": ["multilingual pretrained models", "lowresource languages", "crosslingual transfer learning"], "background": "1. The limitations of current multilingual pre-trained language models in handling low-resource languages and languages with unseen scripts, which results in a performance gap. 2. The need for an automated and efficient method to determine the optimal vocabulary size and initialize embeddings for improved cross-lingual transfer learning."}
{"hash_id": 2342052986253992416, "entities": ["unified hallucination detection", "multimodal tasks", "task singularity", "finegrained hallucinations"], "background": "1. The lack of a unified approach to detect hallucinations across various multimodal tasks and hallucination categories, which hinders the progress of reliable MLLM evaluation and practical deployment. 2. The need to address the limitations of previous research, such as task singularity, limited hallucination categories, and incomplete granularity, to ensure the detection of fine-grained hallucinations."}
{"hash_id": 5169286804666399742, "entities": ["tokenlevel training", "characterlevel infilling", "subtoken prediction", "model perplexity"], "background": "1. Traditional methods focused on token-level training, leading to sub-optimal performance in character-level infilling tasks due to the presence of sub-tokens. 2. Existing character-level infilling approaches that predict sub-tokens in inference have diminished performance due to the large perplexity of the model on sub-tokens."}
{"hash_id": 5986251002892289282, "entities": ["semantic representation", "information retrieval", "longcontext language modeling", "llama", "chatgpt performance"], "background": "1. The need to improve semantic representation and information retrieval in long-context language modeling without the limitations of chunked context. 2. The desire to enhance the performance of large language models like LLaMA-2 and ChatGPT in tasks involving long sequences."}
{"hash_id": 7259117691819519473, "entities": ["fixedlength encoding methods", "sign language", "information density", "expertannotated glosses"], "background": "1. The need to overcome the limitations of fixed-length encoding methods that overlook the uneven information density in sign language, leading to under-encoding of important regions and over-encoding of unimportant regions. 2. The desire to improve scalability and eliminate the dependency on expert-annotated intermediate representations like glosses, which restrict the model's potential."}
{"hash_id": 8473045751342263368, "entities": ["specialized large language models", "ocean science domain", "domain experts"], "background": "1. The lack of specialized Large Language Models (LLMs) that cater to the unique needs and complexities of the ocean science domain. 2. The untapped potential of LLMs to assist domain experts like oceanographers in various tasks due to the immense and intricate nature of ocean data."}
{"hash_id": 2316500931419507408, "entities": ["sophisticated cits", "esl education", "large language models"], "background": "1. The need for more sophisticated and pedagogically informed CITS that can teach complex concepts in ESL education, as current systems are limited in scope and employ simplistic instructional strategies. 2. The potential of Large Language Models (LLMs) to serve as effective tutors for ESL learners by replicating native-speaking contexts and adapting to diverse learning needs."}
{"hash_id": 7758731263832766425, "entities": ["industry collaboration", "nlp research", "academic independence", "corporate capture"], "background": "1. To investigate the growing concern that collaboration with industry might be leading to a situation where NLP research is unduly influenced by corporate interests, potentially compromising academic independence and diversity of research directions. 2. To provide empirical evidence that can help the research community understand the extent of reliance on industry artifacts and contributions, which is crucial for assessing the risks of corporate capture in NLP."}
{"hash_id": 8026426214060300632, "entities": ["large language models", "dense retrieval tasks", "global semantic embeddings"], "background": "1. The need to leverage the strong semantic understanding of large language models (LLMs) for dense retrieval tasks, which require discriminative embeddings to represent the relationship between queries and documents. 2. The requirement to adapt LLMs, which are trained via auto-regression, to produce global semantic embeddings suitable for dense retrieval, as their default training mechanism focuses on local semantics."}
{"hash_id": 3464285510534057263, "entities": ["large language models", "lowresource languages", "data imbalance", "generative capabilities"], "background": "1. Large language models (LLMs) have shown impressive performance in high-resource languages but struggle with low-resource languages due to data imbalance in pre-training. 2. There is a need to elicit LLMs' generative capabilities in low-resource languages without relying on supervised data, to democratize access to these models across diverse linguistic communities."}
{"hash_id": 7027230980663775901, "entities": ["social media bot detection", "adversarial bot strategies", "large language models"], "background": "1. The need to advance social media bot detection to counteract the increasing sophistication of adversarial bot strategies that can evade current detection methods. 2. The exploration of the potential of large language models (LLMs) to significantly improve bot detection performance while acknowledging the risks they pose in terms of being used to create more evasive bots."}
{"hash_id": 4424804622898720252, "entities": ["intrinsic reflection capability", "overconfidence inconsistency", "llms selfevaluation"], "background": "1. The need to improve the intrinsic reflection capability of LLMs without relying on external feedback, as external feedback is often unavailable in practical scenarios. 2. The desire to address the overconfidence and inconsistency issues that undermine the effectiveness of LLMs' self-evaluation and self-correction processes."}
{"hash_id": 4696923636896094704, "entities": ["humanai interactions", "language models", "communicate uncertainties", "user overreliance", "false confidence"], "background": "1. The need to improve the reliability of human-AI interactions by ensuring that language models effectively communicate their uncertainties to users. 2. The concern over user overreliance on AI-generated information, especially when that information is expressed with false confidence."}
{"hash_id": 7839882170873913169, "entities": ["code correctness", "erroneous outcomes", "nlp research validity"], "background": "1. The risk of erroneous outcomes and misleading findings due to the presumption of code correctness based on the quality of results. 2. The need to complement the current focus on reproducibility with an emphasis on software quality to ensure the soundness and validity of research findings in NLP."}
{"hash_id": 6872962776837494238, "entities": ["limitation of intermediate transcripts", "error propagation", "timestamp estimation", "speechtotext translation"], "background": "1. The limitations of current approaches that rely on intermediate transcripts, such as error propagation, loss of prosody information, inapplicability to unwritten languages, and increased computational cost. 2. The lack of attention given to timestamp estimation in direct speech-to-text translation models, and the absence of a reliable metric for evaluating the quality of predicted timestamps."}
{"hash_id": 5433993951034200897, "entities": ["realtime speechtotext translation", "unbounded audio streams", "simultaneous translation methods", "suitable evaluation metric"], "background": "1. The need for real-time speech-to-text translation in scenarios with continuous and unbounded audio streams, such as interpreting and live lectures, which existing simultaneous translation methods are not designed to handle effectively. 2. The absence of a suitable metric to evaluate the performance of streaming speech-to-text translation systems, which is essential for research and development in this area."}
{"hash_id": 6702097640533445421, "entities": ["alignment retrievers llms", "retrievalaugmented generation", "annotation cost reduction"], "background": "1. The need to improve the alignment between retrievers and large language models to enhance the performance of retrieval-augmented generation in specific domains and tasks. 2. The requirement to reduce the annotation cost and improve the quality and diversity of relevance data for training retrievers in conjunction with black-box LLMs."}
{"hash_id": 2252292361127942643, "entities": ["ondevice llms", "privacy concerns", "computational burden", "llm customization"], "background": "1. To overcome the limitations of on-device LLMs, which are constrained by the smaller scale of models and to avoid the privacy concerns and computational burden associated with maintaining customized LLMs on cloud servers. 2. To provide a practical method for on-device LLM customization that does not require additional training on the device, ensuring performance on user-defined tasks without the need for extensive datasets or computational resources."}
{"hash_id": 5178031650360967757, "entities": ["standardized tooling", "large language models", "open science", "synthetic data generation"], "background": "1. The lack of standardized tooling and the technical complexities of using large language models have hindered open science and reproducibility in NLP research. 2. The need to simplify and automate workflows involving LLMs, such as synthetic data generation and model fine-tuning, to increase research productivity and facilitate comparison between different models and configurations."}
{"hash_id": 2561145875344602661, "entities": ["undertranslation phenomenon", "nmt systems", "translation quality"], "background": "1. The need to understand and mitigate the under-translation phenomenon that persists in state-of-the-art NMT systems, which negatively impacts translation quality. 2. The lack of comprehensive explanations and effective solutions for the underlying causes of under-translation, despite its recognized challenges in the field."}
{"hash_id": 7589436291860558953, "entities": ["advanced llms lmms capabilities", "scientific reasoning benchmark", "multimodal reasoning assessment"], "background": "1. The need for a more rigorous benchmark to challenge and accurately evaluate the advanced capabilities of LLMs and LMMs, particularly in scientific reasoning, as current benchmarks are no longer sufficient for top-tier models. 2. The requirement to assess and improve multimodal reasoning abilities in AI, as existing benchmarks predominantly focus on text and do not fully capture the complexity of scientific problems that require visual and mathematical understanding."}
{"hash_id": 1468886706621298205, "entities": ["asymmetric isa relationships", "pseudoleaves", "taxonomy completion", "finegrained relationships"], "background": "1. The limitations of previous approaches in modeling asymmetric \"is-a\" relationships in taxonomies and the inefficient use of pseudo-leaves, which biases network learning and reduces the ability to handle multiple parents in insertion cases. 2. The need to capture more complex and fine-grained relationships within taxonomies to improve the accuracy and effectiveness of taxonomy completion."}
{"hash_id": 7310782313675345051, "entities": ["bridging planning gap", "llmbased agents", "realworld constraints", "simulation environment", "multitasking efficiency"], "background": "1. The need to bridge the gap between the planning capabilities of LLM-based agents and the temporal and resource constraints present in real-world scenarios. 2. The absence of a simulation environment that evaluates language agents' multitasking efficiency in the context of time-awareness and parallel processing."}
{"hash_id": 4297407935472240057, "entities": ["privacy concerns", "memorization behaviors", "domainspecific data"], "background": "1. The need to understand and mitigate privacy and copyright concerns arising from the memorization of sensitive data by language models during fine-tuning. 2. The gap in knowledge regarding memorization behaviors during fine-tuning compared to pre-training, especially considering the use of domain-specific data and diverse training objectives."}
{"hash_id": 2093387354144361889, "entities": ["machinegenerated text", "large language models", "binary classification"], "background": "1. The surge in machine-generated text (MGT) due to the advent of Large Language Models (LLMs) has raised concerns about its potential misuse and societal implications, such as the spread of fake news and the erosion of trust in communication and academic integrity. 2. Existing MGT detection methods are primarily binary classification focused on English and do not account for mixed human-machine text, which is significantly different from real-world scenarios."}
{"hash_id": 2580936667346654593, "entities": ["improve chat consistency", "multiround dialogues", "dialogue coherence"], "background": "1. The need to improve chat consistency in multi-round dialogues, which is currently hindered by traditional tuning methods that ignore the role disparities between speakers and the interactive nature of dialogues. 2. The urgency to solve consistency issues in scenarios where maintaining dialogue coherence is crucial, such as those involving real-world human communication disparities."}
{"hash_id": 8841744776422967120, "entities": ["risk of losing information", "varying degrees of duplication", "training efficiency llms", "performance compromise"], "background": "1. The risk of losing valuable information when duplicates are simply detected and removed without considering the varying degrees of duplication. 2. The need to improve training efficiency for large language models (LLMs) without compromising their performance."}
{"hash_id": 4898506112056192505, "entities": ["reporting bias", "exposure bias", "commonsense knowledge", "large language models", "implicit storytelling"], "background": "1. The reporting bias of commonsense rules and the exposure bias of rule-based commonsense reasoning hinder the effective incorporation of commonsense knowledge into large language models. 2. Humans convey and pass down commonsense implicitly through stories, which suggests a potentially more effective approach for expressing and retrieving commonsense in LLMs."}
{"hash_id": 4155957893745751006, "entities": ["training costs", "storage redundancy", "finetuning limitations", "taskspecific features"], "background": "1. The high training costs and storage redundancy associated with full training and maintaining separate replicas of large language models for each task. 2. The limitations of existing fine-tuning methods, including local optimization, the inability to train with full rank, and a lack of focus on task-specific relevant features."}
{"hash_id": 3381896475234528790, "entities": ["multimodal models", "contextual examples", "analogical reasoning", "human cognitive processes"], "background": "1. The limitations of current large multimodal models in generalizing from contextual examples and their inflexible design, which hinders their ability to process multiple images and adapt to new situations. 2. The inspiration from human cognitive processes, where analogical reasoning from past experiences is a fundamental element in solving complex problems."}
{"hash_id": 2256885209615979145, "entities": ["unified largescale dataset", "event understanding methods", "eventbased applications", "comprehensive annotation", "specific aspects focus"], "background": "1. The need for a unified, large-scale dataset that covers the full process of event understanding, including event detection, argument extraction, and relation extraction, to facilitate the development of end-to-end event understanding methods. 2. The desire to bridge the gap between existing datasets that either focus on specific aspects of event understanding or lack comprehensive annotation, hindering the advancement of event-based applications and the potential of large language models."}
{"hash_id": 5166204040813926811, "entities": ["rigorous quantitative framework", "llms reasoning assessment", "benchmark overfitting mitigation"], "background": "1. The need for a more rigorous and quantitative framework to assess the reasoning capabilities of LLMs, as current benchmarks fail to accurately characterize or classify these abilities. 2. The requirement to mitigate the risk of LLMs overfitting or memorizing benchmarks, which can lead to an overestimation of their capabilities."}
{"hash_id": 2450566443593080965, "entities": ["crosslingual translation", "text watermarks", "watermarking consistency"], "background": "1. The need to maintain the effectiveness of text watermarks across different languages to prevent the misuse of content generated by large language models. 2. The requirement for a better understanding of how cross-lingual translation affects watermarking consistency to improve the robustness of watermarking technology."}
{"hash_id": 4509929626284740006, "entities": ["novice peer counselors", "detailed feedback", "highstakes scenarios"], "background": "1. The need to empower novice peer counselors with detailed and contextualized feedback to improve their clinical skills without the dependency on time-consuming and costly human supervision. 2. The desire to minimize the risk of harmful or low-quality feedback in high-stakes scenarios, such as emotional support conversations."}
{"hash_id": 1449511672435082744, "entities": ["multilingual large language models", "lowresource languages", "incontext learning", "crosslingual transfer"], "background": "1. The need to enhance the performance of multilingual large language models, especially in low-resource languages, where the models are underrepresented during pretraining and finetuning. 2. The desire to improve the effectiveness of in-context learning (ICL) prompts for multilingual LLMs by leveraging English as a bridge language to facilitate cross-lingual transfer."}
{"hash_id": 174504064608113400, "entities": ["llms", "english response bias", "multilingual environments", "instructiontuning data"], "background": "1. Large Language Models (LLMs) are biased towards generating English responses even when given non-English instructions, which hinders their effectiveness in multilingual environments. 2. Existing approaches to address this issue require collecting multilingual instruction-tuning data and retraining models, which can be resource-intensive and costly."}
{"hash_id": 2162237243199763553, "entities": ["privacy risks", "text embeddings", "realistic threat model"], "background": "1. The need to address privacy risks in text embeddings, especially in scenarios where attackers do not have direct access to the original embedding model, which is a more realistic threat model. 2. The requirement to develop a method that can infer sensitive information from text embeddings without extensive querying of the model, as current approaches assume such privileged access."}
{"hash_id": 6406298358009098314, "entities": ["improving llm decision mechanism", "realworld applications", "optimizing llms", "robustness adaptability"], "background": "1. To improve the understanding of LLMs' decision mechanism, which is crucial for enhancing their performance in real-world applications. 2. To provide insights into how to optimize LLMs or prompts to improve their robustness and adaptability in conflicting situations."}
{"hash_id": 1402865792361948105, "entities": ["user privacy", "language models", "personal devices", "large language models llms", "limited resources"], "background": "1. The need to safeguard user privacy while enabling language models to execute commands efficiently, especially as they are increasingly deployed on personal devices. 2. The challenge of deploying more powerful large language models (LLMs) on local devices with limited resources while maintaining their performance."}
{"hash_id": 2413706329197393807, "entities": ["neural retrieval methods", "long document context", "passage retrieval", "user effort reduction"], "background": "1. The limitations of current neural retrieval methods that focus on short texts and lack the ability to understand the context of long documents, leading to inefficient retrieval of relevant passages. 2. The need to reduce user effort in searching for relevant information within long documents, such as Wikipedia articles and scientific papers, by improving the accuracy of passage retrieval."}
{"hash_id": 3774372974766091149, "entities": ["selection bias mitigation", "sft phase", "mcsb capability", "answeroption association"], "background": "1. The need to mitigate selection bias in LLMs during the SFT phase, as it significantly affects the reliability of MCQs. 2. The recognition that improving MCSB capability is crucial for LLMs to associate answer options with their corresponding symbols effectively."}
{"hash_id": 6176261811239836610, "entities": ["domain knowledge graphs", "human intervention", "large language models", "contextual noise"], "background": "1. The heavy reliance on human intervention in the construction of domain knowledge graphs limits their scalability and practical applicability in real-world scenarios. 2. The emergence of large language models (LLMs) offers a potential solution to automate the KG construction process but is hindered by issues of contextual noise and knowledge hallucination."}
{"hash_id": 3732877947863536856, "entities": ["modality uncertainty", "hate sentiment detection", "modality imbalance", "discriminative capabilities"], "background": "1. The need to address the ignored issue of modality uncertainty, where the contribution degree of each modality (image and text) to the detection of hate sentiment is not considered, leading to potential misclassifications. 2. The necessity to overcome modality imbalance, where the dominant modality (text) can suppress the optimization of the other modality (image), limiting the model's ability to fully utilize the discriminative capabilities of each modality."}
{"hash_id": 5963805306955663306, "entities": ["visual grounding methods", "vqa", "outofdistribution performance"], "background": "1. To improve the effectiveness of Visual Grounding methods in VQA by addressing the issue of inaccurate evaluation caused by flawed assumptions about the presence of relevant visual information. 2. To enhance the Out-of-Distribution (OOD) performance of VQA models and reduce dataset biases by ensuring that models rely on question-relevant visual features."}
{"hash_id": 3136698705938930965, "entities": ["automated preference metrics", "generative ai systems", "favoritism in metrics"], "background": "1. The need to accurately evaluate the quality of automated preference metrics for generative AI systems, as current measures only assess agreement with human ratings and not the distribution of errors. 2. The recognition that favoritism in these metrics can lead to incorrect system rankings, affecting the reliability of evaluations."}
{"hash_id": 1153253629095724729, "entities": ["online discussions", "posthoc moderation", "inappropriate language", "computational mitigation"], "background": "1. The need to create civil and productive online discussions without relying on expensive and time-consuming post-hoc moderation. 2. The desire to prevent the negative impact of inappropriate language in arguments by computationally mitigating it during content creation."}
{"hash_id": 927038892653115059, "entities": ["semantic meaning", "structural interactions", "graph linearization", "pretrained lms"], "background": "1. The need to capture both the semantic meaning of text in knowledge graphs and the complex structural interactions within the graph. 2. The limitation of current methods that either linearize graphs, underutilizing structure, or use GNNs that don't represent text features as well as pretrained LMs."}
{"hash_id": 7454406063119813459, "entities": ["semantic change", "taxonomies", "sense representation", "lexical semantic change detection"], "background": "1. The computational community has largely disregarded taxonomies of semantic change proposed in the past, focusing instead on quantifying the degree of change without considering the qualitative type of change. 2. There is a need for a more precise and abstract level of sense representation that can summarize multiple usages of a word, reducing complexity and uncertainty in lexical semantic change detection."}
{"hash_id": 2138850107156421881, "entities": ["misinformation spread", "user trust breaches", "llmgenerated information", "experimental framework", "factual confidence estimation"], "background": "1. The spread of misinformation and breaches of user trust due to the unreliable factuality of LLM-generated information. 2. The lack of a unified experimental framework to compare and assess the reliability and robustness of different factual confidence estimation methods for LLMs."}
{"hash_id": 7592733688348384591, "entities": ["alignment of language models", "instruction datasets", "tuning process", "standard criteria"], "background": "1. The need to enhance the alignment of large language models with human instructions by improving the quality of the instruction datasets used for tuning, without solely relying on increasing dataset size. 2. The absence of standard criteria for selecting high-quality instruction data, leading to inefficiencies and potential noise in the tuning process."}
{"hash_id": 2711466404042592370, "entities": ["reasoning chains", "language models", "steplevel annotated datasets"], "background": "1. The need to thoroughly evaluate and improve the correctness of reasoning chains in language models, as they are crucial for enhancing performance in complex reasoning tasks. 2. The absence of high-quality, step-level annotated datasets that can effectively assess the verification methods for reasoning chains, despite their potential to improve reasoning in language models."}
{"hash_id": 5769711731432730447, "entities": ["collaborative document revision", "nlp assistance", "textbased collaboration"], "background": "1. The need for a deeper understanding of the collaborative document revision process to facilitate effective communication and enhance NLP assistance in knowledge work. 2. The potential for NLP applications to automate and improve the efficiency of edit analysis and text-based collaboration by jointly modeling reviews, revisions, and responses."}
{"hash_id": 5066665343266975855, "entities": ["long sequence processing", "attention mechanisms", "hierarchical language processing", "higherlevel semantic representations"], "background": "1. The need to process long sequences such as books without the computational overhead caused by the quadratic scaling of attention mechanisms in traditional language models. 2. The recognition that human language processing is hierarchical, which suggests that modeling language at higher-level semantic representations could improve performance on long-document tasks."}
{"hash_id": 3970388852883602462, "entities": ["knowledge editing techniques", "large language models", "dynamic updating"], "background": "1. Existing knowledge editing techniques impede LLMs from effectively combining new knowledge with their inherent knowledge, leading to suboptimal performance. 2. The dynamic nature of the world requires LLMs to be frequently updated with new information to remain relevant and accurate."}
{"hash_id": 7102096097780695924, "entities": ["diversity code generation", "llms reasoning paths", "code solution evaluation"], "background": "1. The need to enhance the diversity of code generation responses to improve the capability of LLMs in handling various reasoning paths. 2. The requirement to improve the ability of LLMs to evaluate the correctness of code solutions, which in turn would enhance their code generation accuracy."}
{"hash_id": 7460796828352409378, "entities": ["transformers", "internal processes", "glass box interpretability", "restartincremental models"], "background": "1. The need to understand the internal processes of Transformers that allow them to revise interpretations in the face of local ambiguities, which is crucial for improving the efficiency and accuracy of natural language processing models. 2. The desire to develop glass box interpretability methods that can provide insights into the dynamics of updates in internal states, thus enhancing the transparency and explainability of restart-incremental (RI) models."}
{"hash_id": 402158665884087488, "entities": ["spatial reasoning abilities", "reallife applications", "dedicated dataset", "textual spatial reasoning paths"], "background": "1. The need to enhance the spatial reasoning abilities of large language models for real-life applications such as dialogues, navigation, and robotics. 2. The absence of a dedicated dataset with textual spatial reasoning paths for training and evaluating the performance of large language models in spatial reasoning tasks."}
{"hash_id": 5458976150798160629, "entities": ["enhance dialogue planning", "llms reactive nature", "traditional approaches inefficiency"], "background": "1. The need to enhance dialogue planning in LLMs, which currently struggle with proactively steering conversations towards specific goals due to their reactive nature. 2. The inefficiency and suboptimal performance of traditional approaches to dialogue planning, such as elaborate prompt engineering and policy network integration."}
{"hash_id": 1656285335502821918, "entities": ["commonsense inference", "knowledge models", "contextually irrelevant facts", "autoregressive training", "nondiverse inferences"], "background": "1. Existing knowledge models for commonsense inference often generate contextually irrelevant facts due to being trained on general commonsense knowledge graphs. 2. These models also tend to produce non-diverse inferences due to their autoregressive training objectives, limiting the variety of relevant commonsense inferences."}
{"hash_id": 2464281943913240947, "entities": ["creative nlg tasks", "llm paradigm", "task definitions", "evaluation metrics"], "background": "1. The need to systematically investigate and define creative NLG tasks within the LLM paradigm, given the flexibility and new degrees of freedom they introduce. 2. The requirement to establish consensus on task definitions and evaluation metrics for creative NLG tasks, such as citation text generation, which currently lack standardization."}
{"hash_id": 9027969945021306974, "entities": ["theory of mind", "collaborative plan acquisition", "computational collaborative agents"], "background": "1. To investigate the true impact of Theory of Mind (ToM) modelling on dialogue-based collaborative plan acquisition (CPA) since its effectiveness in this context remains under-explored. 2. To challenge the assumption that ToM features are crucial for improving performance in collaborative tasks and to understand their actual relevance in computational collaborative agents."}
{"hash_id": 4636784983344492384, "entities": ["temporal knowledge reasoning", "large language models", "evolving factual knowledge"], "background": "1. Large Language Models (LLMs) struggle with temporal knowledge reasoning, leading to inaccurate or misleading responses due to limited handling of evolving factual knowledge and complex temporal logic. 2. The need to improve LLMs' ability to construct knowledge proactively and self-direct their learning from both correct and incorrect historical reasoning samples."}
{"hash_id": 3120478265029887360, "entities": ["code generation", "legal ethical security", "watermarking methods"], "background": "1. The need to address legal, ethical, and security concerns arising from the use of large language models in code generation, such as code licensing issues, plagiarism, and malware generation. 2. The lack of effective watermarking methods that can reliably detect machine-generated code, especially in low-entropy environments typical of code generation tasks."}
{"hash_id": 7389658807795976299, "entities": ["large language models", "code generation", "competitive programming", "human programming cycle", "problemsolving capabilities"], "background": "1. To improve the performance of large language models (LLMs) in code generation tasks, particularly for complex competitive programming problems where existing models fall short in deep understanding and successful execution. 2. To emulate the human programming cycle by incorporating stages such as recalling relevant examples, planning, and debugging, which are crucial for enhancing problem-solving capabilities in code generation."}
{"hash_id": 4561092382578726524, "entities": ["finegrained visual alignment", "object level understanding", "domainspecific tasks", "visionlanguage models", "generalization without training"], "background": "1. The need to improve fine-grained visual alignment and grounding in vision-language models, which currently struggle with object level understanding and are sensitive to spurious background features. 2. The requirement for vision-language models to generalize to unseen visual concepts without the need for additional training, particularly for domain-specific tasks."}
{"hash_id": 176518459847414138, "entities": ["evaluation of large language models", "extended contexts understanding", "f metrics limitations"], "background": "1. The need for a more precise and equitable evaluation of large language models' capabilities in understanding and reasoning over extended contexts, as current benchmarks fall short in this regard. 2. The requirement to overcome the limitations of existing benchmarks that rely on F1 metrics, which can inaccurately score responses and undervalue correct answers that differ from the reference responses."}
{"hash_id": 26956030017262065, "entities": ["domainspecific tasks", "large language models", "costeffective methods", "patent data characteristics", "efficiency in applications"], "background": "1. The limitations of large language models (LLMs) in achieving satisfactory performance for domain-specific tasks like patent approval prediction, despite their success in general language tasks. 2. The need for cost-effective methods that do not rely on computationally expensive model scaling, especially in light of the distinct characteristics of patent data and the importance of efficiency for real-world applications."}
{"hash_id": 5144301729521339862, "entities": ["negative pair sampling", "slu performance", "asr errors", "textual data robustness"], "background": "1. Existing methods sample negative pairs incorrectly in pre-training, neglect explicit erroneous predictions, and treat manual and ASR transcripts indistinguishably, leading to suboptimal SLU performance. 2. The need to improve the robustness of SLU against ASR errors without relying on extra speech-related information, focusing solely on textual data."}
{"hash_id": 7503425532581962882, "entities": ["alignment degree", "attentional capture", "multimodal sequential learning"], "background": "1. The under-explored alignment degree between different calculated attentional results of the same query in existing Transformer-based multimodal sequential learning methods. 2. The need to constrain and improve the efficiency of attentional capture in multimodal interactions to address the inevitable mistakes in query-key associations during training."}
{"hash_id": 2000607433187012284, "entities": ["large multimodal models", "kbvqa", "flmr retrievers"], "background": "1. The need to improve the performance of Large Multimodal Models (LMMs) in challenging tasks like KB-VQA, where retrieval of relevant information from document collections is crucial. 2. The potential to enhance existing retrievers, such as FLMR, by scaling up their capabilities through pre-training and studying the effects of model and data size on performance."}
{"hash_id": 1994986547930740164, "entities": ["searchbased dialog models", "computational expense", "contextualization of utterances"], "background": "1. To reduce the computational expense of reencoding the dialog history at every turn in search-based dialog models. 2. To improve the contextualization of utterances in dialog sequences without the need for additional weights or complex computations."}
{"hash_id": 8352437471533564047, "entities": ["improve clipbased systems", "subtle meme differences", "hatefulness classification", "adaptable detection system"], "background": "1. The need to improve the sensitivity of existing CLIP-based systems to subtle differences in memes that are crucial for accurate hatefulness classification. 2. The requirement for a detection system that can adapt to new examples of hateful memes without the need for retraining, considering the rapidly evolving nature of online content."}
{"hash_id": 5958223442718810019, "entities": ["computational efficiency", "transformer models", "linear characteristics", "expressiveness embeddings", "model performance"], "background": "1. The need to improve the computational efficiency of transformer models by understanding and optimizing their linear characteristics. 2. The desire to enhance the expressiveness of embeddings and model performance on various benchmarks without sacrificing the effectiveness of transformer architectures."}
{"hash_id": 7400236543016226672, "entities": ["subjective datasets", "annotator perspectives", "label noise", "temporal distribution shifts"], "background": "1. The need to incorporate diverse perspectives of annotators in subjective datasets without compromising the accuracy due to individual annotator errors. 2. The requirement for a robust method that can handle additional label noise and temporal distribution shifts of opinions in subjective annotation tasks."}
{"hash_id": 7113667926961326804, "entities": ["language models", "grounded knowledge", "auditory domain", "sound recognition"], "background": "1. To investigate whether language models can learn grounded knowledge of sounds without explicit audio supervision during training, which has implications for their ability to understand and acquire meaning from text-only data. 2. To extend the understanding of how language models represent perceptual concepts beyond vision to the auditory domain, which could improve their performance in tasks involving sound recognition or understanding."}
{"hash_id": 8654622517321528215, "entities": ["discerning humanwritten texts", "advanced language models", "detection methods", "paraphrasing attacks"], "background": "1. The increasing difficulty in discerning between texts generated by humans and advanced language models due to the latter's ability to produce content that mimics human writing. 2. The need for improved detection methods that can perform well across different domains and can resist \"paraphrasing attacks\" that can fool existing classifiers."}
{"hash_id": 5925239192475044807, "entities": ["alignment methods", "opensource llms", "mitigation strategies"], "background": "1. The need to expose the vulnerabilities of current alignment methods used in open-sourced large language models, which are not sufficient to prevent the generation of undesired content. 2. The importance of raising awareness within the community about the potential risks of misuse and the need for more advanced mitigation strategies to ensure the safety of open-source LLMs."}
{"hash_id": 6184855141097370490, "entities": [], "background": "1. To challenge the consensus that optimizing in-context examples (ICE) is always crucial for improving large language model performance, particularly when task-specific instructions are provided. 2. To provide a practical method for determining whether to focus on optimizing ICE or improving instructions for a given task."}
{"hash_id": 258301499036630014, "entities": ["comprehensive benchmark", "executionbased evaluation", "programming language tasks"], "background": "1. The need for a comprehensive benchmark that can evaluate LLMs on a wide range of programming languages and tasks, simulating real-world software development scenarios. 2. The requirement for an execution-based evaluation metric to ensure the practical applicability and correctness of the code generated by LLMs."}
{"hash_id": 6647030797278574712, "entities": ["explanation behavior", "large language models", "automated evaluation tool"], "background": "1. The need to understand and improve the explanation behavior of large language models, as their reasoning capabilities and the quality of their explanations are poorly understood. 2. The requirement for an automated, cost-effective tool that can evaluate the intrinsic quality of reasoning chains without relying on human annotations or expensive API calls."}
{"hash_id": 3782574866382716639, "entities": ["multiinstruction handling", "efficiency improvement", "multitask llm capabilities"], "background": "1. The need to improve the efficiency of large language models by enabling them to handle multiple instructions at once, reducing the need for multiple inference calls and thus saving time. 2. The desire to explore and evaluate the true capabilities of LLMs in multi-task scenarios, which are not adequately captured by existing benchmarks that focus on single-task or domain-specific multi-step instructions."}
{"hash_id": 816236986368330045, "entities": ["multiagent collaboration", "software development", "past experiences", "large language modeldriven agents", "historical communications"], "background": "1. To improve the efficiency of multi-agent collaboration in software development by reducing repetitive errors and unnecessary trial-and-error processes through the incorporation of past experiences. 2. To enhance the autonomy of large language model-driven agents by guiding them to learn from historical communications and apply this knowledge to solve unseen tasks with less human involvement."}
{"hash_id": 4886766830574154746, "entities": ["distinguishing new intents", "supervision lack", "hypersphere distribution", "representation dilemma"], "background": "1. Existing NID algorithms struggle with distinguishing new intents due to a lack of supervision, leading to intertwined centers that cannot be effectively separated. 2. The need to obtain optimal hypersphere distribution for identifying new intents to overcome the representation dilemma caused by biased feature representations due to the dominance of labeled samples."}
{"hash_id": 8882866500211390530, "entities": ["evaluation methodology", "llmbased chatbots", "speaker verification accuracy"], "background": "1. The lack of a rigorous evaluation methodology to assess the personalization abilities of LLM-based role-playing chatbots in maintaining the linguistic style and personal characteristics of the target speaker. 2. The need to isolate the impact of conversation topic on speaker verification accuracy to better understand and improve the performance of conversational agents."}
{"hash_id": 251047974303112916, "entities": ["large language models", "practical efficacy", "data science lifecycle", "evaluation benchmarks"], "background": "1. The need to assess the practical efficacy of Large Language Models as data science agents in real-world applications due to the complexity of data analysis and the varied demands of these applications. 2. The absence of comprehensive evaluation benchmarks that cover the entire data science lifecycle, which hinders the understanding of these agents' performance and the identification of areas for improvement."}
{"hash_id": 5424430820400884371, "entities": ["language models", "multilingual processing", "multilingual capabilities"], "background": "1. The need to explain the mechanisms by which large language models process multiple languages without explicit multilingual training. 2. The desire to improve the understanding and control of the multilingual capabilities of these models for better steering and optimization of their outputs."}
{"hash_id": 7032960034866849352, "entities": ["incorrect knowledge retention", "knowledge updating", "direct finetuning conflicts"], "background": "1. Large Language Models are prone to retaining incorrect or outdated knowledge from their training corpus, which can lead to inaccurate outputs. 2. Existing methods of knowledge updating through direct fine-tuning can be ineffective due to conflicts between old and new knowledge."}
{"hash_id": 317302375778589519, "entities": ["parameterefficient alignment", "computational cost", "alignment dataset quality"], "background": "1. The high computational cost of pre-training and fine-tuning large language models makes it inaccessible for most researchers, necessitating the need for more parameter-efficient alignment methods. 2. There is a lack of comprehensive studies on the effect of different alignment dataset qualities, methods, and model variations on the downstream performance of these models."}
{"hash_id": 3759633658243545134, "entities": ["zeroshot dialogue state tracking", "prompt integration methods", "transformer models computational overhead"], "background": "1. The need to improve the efficiency and effectiveness of zero-shot dialogue state tracking by overcoming the limitations of current prompt integration methods. 2. The requirement to enhance the utilization of prompts across all layers of transformer models without increasing computational overhead during inference."}
{"hash_id": 8209896898552931946, "entities": ["llms generation probability scores", "zeroshot document reranking", "comparison selection strategies"], "background": "1. The need to fully capitalize on the potential of LLMs by utilizing generation probability scores to capture the uncertainty of pairwise comparison results. 2. The requirement to improve the robustness and effectiveness of zero-shot document re-ranking by addressing the sensitivity of LLMs to the initial ranking order and the lack of flexibility in existing comparison selection strategies."}
{"hash_id": 3538154608009484570, "entities": ["improve speech tokenization", "semantic information retention", "robust tokenization method"], "background": "1. The need to improve the performance of discrete speech tokens by retaining more semantic information, which is currently lost during the tokenization process for large language models. 2. The requirement for a more robust and efficient speech tokenization method that can be applied across various speech encoders and languages, without the need for large amounts of parallel data."}
{"hash_id": 5806497031893382902, "entities": ["eventlevel inconsistencies", "multimodal fake news", "detection framework", "poorquality news samples"], "background": "1. The need to effectively address event-level inconsistencies in multimodal fake news, which existing methods have not fully tackled. 2. The requirement to develop a robust detection framework that can handle poor-quality news samples, which are common in real-world scenarios."}
{"hash_id": 2033803508574472205, "entities": ["model coherence dependencies", "narrative contexts", "large language models", "practical framework"], "background": "1. The need to explicitly model coherence dependencies in narrative contexts for improved comprehension, as individual passages are cohesively related and the end-to-end paradigm may not suffice. 2. The goal to leverage the strengths of Large Language Models (LLMs) to create a practical framework that does not rely on human annotations and can be applied across various narrative comprehension tasks."}
{"hash_id": 3333628602206488559, "entities": ["uncovering security vulnerabilities", "recommendation systems", "stealthy attacks", "item exposure manipulation"], "background": "1. The need to uncover and address new security vulnerabilities introduced by the use of large language models in recommendation systems. 2. The requirement for research that can protect these systems from stealthy attacks that could manipulate item exposure without being easily detected."}
{"hash_id": 6252988378941018514, "entities": ["existing finetuning approaches", "error cascade effect", "reasoning tasks"], "background": "1. The high cost and limitations of existing fine-tuning approaches that rely on external resources such as human annotation or larger models. 2. The need to reduce the error cascade effect in reasoning tasks where minor errors can lead to significant inaccuracies."}
{"hash_id": 7578181683083435486, "entities": ["improved qa interpretability", "structured explanations", "reinforcement learning", "structured reasoning"], "background": "1. The need to improve the interpretability, traceability, and trustworthiness of question-answering (QA) systems by providing structured explanations. 2. The gap in existing methods that fail to capture the complex, hierarchical, and branching logical structures inherent in structured reasoning, and the underutilization of reinforcement learning for this purpose."}
{"hash_id": 7160386888422561322, "entities": ["parameterefficient finetuning", "noisy labels", "robustness", "generalization capabilities"], "background": "1. The need to improve the robustness and generalization capabilities of parameter-efficient fine-tuning (PEFT) methods in the presence of noisy labels, which are common in real-world datasets. 2. The recognition that while PEFT methods have inherent robustness due to their limited capacity, this also makes them more susceptible to interference from noisy labels, hindering learning from clean samples."}
{"hash_id": 6126512115487551510, "entities": ["computational burden", "transformers", "token redundancy", "pretrained language models"], "background": "1. The high computational burden of transformers, scaling quadratically with input sequence length, which limits their application in real-time and resource-constrained devices. 2. The redundancy in token representations due to dense information flows, which leads to unnecessary and expensive computations in pre-trained language models."}
{"hash_id": 4625524536912535607, "entities": ["language models understanding", "raw protein data", "proteintotext generation", "proteintext modeling"], "background": "1. The need to improve language models' capability to understand raw protein data, such as amino acid sequences, which they struggle with due to a lack of pretraining on such data. 2. The absence of comprehensive exploration in protein-to-text generation and the lack of quantitative evaluation benchmarks for protein-text modeling tasks."}
{"hash_id": 8955567572453299732, "entities": ["performance evaluation", "language models", "data contamination", "evaluation protocol", "llms"], "background": "1. The need to accurately gauge the performance of large language models without the inflation caused by data contamination. 2. The requirement for a generalized evaluation protocol that can assess genuine performance amidst data contamination across diverse tasks and domains for both open and closed-source LLMs."}
{"hash_id": 5852549868296192078, "entities": ["evaluation of ei", "llms", "emotion recognition"], "background": "1. The need for more robust and comprehensive benchmarks to evaluate the EI of LLMs, as current benchmarks focus on emotion recognition and neglect other essential EI capabilities. 2. The requirement to move beyond existing datasets that contain frequent patterns and explicit information, leading to unreliable evaluations and not fully capturing the complexity of EI."}
{"hash_id": 6770321587326248362, "entities": ["improve robustness", "aigenerated text detectors", "adversarial attacks", "high accuracy detection"], "background": "1. The need to improve the robustness of AI-generated text detectors against adversarial attacks, which can lead to misclassification and potential misuse of AI-generated content. 2. The requirement for a detector that can maintain high accuracy even with minor changes in the input text, ensuring reliable detection across various domains and genres."}
{"hash_id": 4526327837287417494, "entities": ["financial questionanswering systems", "diverse question types", "improved methodologies", "complex financial questions"], "background": "1. The need for a comprehensive dataset that encompasses diverse question types and contexts in the financial domain to accurately evaluate financial question-answering systems. 2. The requirement for improved methodologies that can handle complex financial questions and provide detailed, accurate, and informative responses."}
{"hash_id": 7613097427213404702, "entities": ["inner reasoning processes", "natural language explanations", "evaluating faithfulness", "selfconsistency"], "background": "1. The need to ensure that natural language explanations from LLMs accurately reflect their inner reasoning processes for improved trustworthiness and understanding of their decisions. 2. The recognition that existing tests for evaluating faithfulness are actually measuring self-consistency at the output level and do not provide insight into the model's internal workings."}
{"hash_id": 1167220256916096746, "entities": ["external knowledge", "vqa tasks", "llm predictions", "information extraction"], "background": "1. The need to improve the utilization of external knowledge in VQA tasks to enhance the accuracy and reliability of LLM predictions, especially given the presence of noise and limitations in context length. 2. The requirement to actively extract and incorporate valuable information from external knowledge to avoid distracting the LLM with irrelevant or partially irrelevant content."}
{"hash_id": 164867607752876600, "entities": ["texttoimage diffusion models", "intricate details", "scalable approach", "visionlanguage inputs"], "background": "1. The limitations of existing text-to-image diffusion models in synthesizing images with intricate details from concise textual descriptions, especially involving multiple entities or nuanced scenes. 2. The need for a more scalable and efficient approach that can process generalized vision-language inputs, which include a mix of textual and visual information in free forms."}
{"hash_id": 6065022666582979997, "entities": ["llms qualitative advantages", "steerability", "documentlevel translation", "catastrophic forgetting", "nmt models"], "background": "1. The need to preserve the unique qualitative advantages of LLMs, such as steerability, document-level translation abilities, and the production of less literal translations, which are lost during traditional fine-tuning on parallel data. 2. The desire to enhance overall translation quality while avoiding catastrophic forgetting of LLM-specific behaviors that surpass those of Neural Machine Translation (NMT) models."}
{"hash_id": 1782739359686584159, "entities": ["bias in llms", "generated contexts", "conflicting information integration"], "background": "1. The need to understand and mitigate the bias in LLMs towards generated contexts, which can lead to the propagation of incorrect information. 2. The desire to enhance the effectiveness of LLMs in integrating diverse types of contexts, particularly when they contain conflicting information, to improve their performance in knowledge-intensive tasks."}
{"hash_id": 5815871856950514668, "entities": ["integrated song synthesis", "singing voice generation", "accompaniment generation", "natural language prompts"], "background": "1. The need for a more integrated approach to song synthesis that combines both singing voice and accompaniment generation, which existing works do not adequately address. 2. The desire to create a controllable song generation process that can be guided by natural language prompts to ensure style consistency and diversity in output."}
{"hash_id": 8821054587319018338, "entities": ["open domain question answering", "inference efficiency", "computational overhead", "answer accuracy"], "background": "1. The need to improve the inference efficiency of open domain question answering models, which can be slow due to the processing of numerous long passages. 2. The desire to maintain or enhance answer accuracy while reducing the computational overhead associated with the retrieval and encoding of a large number of passages."}
{"hash_id": 642330128311688211, "entities": ["evaluation methods", "discourse relation", "counterfactuals comprehension"], "background": "1. The need to move beyond traditional accuracy-based evaluation methods for discourse relation prediction, which do not fully capture a model's reasoning or consistent comprehension of discourse semantics. 2. The desire to develop a method that can assess a language model's true faithfulness and reliability in understanding complex discourse relations, particularly when they involve counterfactuals and logical consistency."}
{"hash_id": 8043251233258811845, "entities": ["readability assessment tools", "wikipedia articles", "nonenglish languages", "systematic overview"], "background": "1. The lack of readability assessment tools for Wikipedia articles in languages other than English, which limits access to knowledge for non-English speakers. 2. The need to provide a systematic overview of the readability of Wikipedia across multiple languages to support readers, editors, and researchers."}
{"hash_id": 8147245881046175704, "entities": ["minimize harmful content", "language models performance", "computational infeasibility", "existing methods limitations"], "background": "1. The need to minimize the generation of harmful content by language models and understand the sources of their performance abilities. 2. The computational infeasibility of retraining models multiple times to measure the influence of each training dataset, and the limitations of existing methods in terms of memory and efficiency."}
{"hash_id": 8358830159204183603, "entities": ["multilingual embeddings", "linguistic concepts alignment", "zeroshot capabilities", "crosslingual alignment"], "background": "1. The need to quantitatively measure the alignment and overlap of linguistic concepts across languages within multilingual embeddings to understand their effectiveness. 2. The desire to explain the emergence of zero-shot capabilities in multilingual models and how fine-tuning affects their cross-lingual alignment."}
{"hash_id": 5930303865725108256, "entities": ["respect global diversity", "cultural sensitivity", "avoiding stereotypes"], "background": "1. The need to respect and value the diversity of global cultures, and to avoid promoting stereotypes and entrenching cultural biases through language models. 2. The requirement to enhance user experience by fostering cultural sensitivity, particularly for \"normal users\" who may not have professional knowledge of prompt engineering."}
{"hash_id": 3440787384633986545, "entities": ["language model compression", "structured sparsity pattern", "oneshot pruning efficiency"], "background": "1. The need to compress large language models (LLMs) to reduce computational resource demands, especially as model sizes increase. 2. The observed significant decline in model performance when adopting the 2:4 structured sparsity pattern due to group constraints, which hampers the efficiency of one-shot pruning."}
{"hash_id": 39936865269398090, "entities": ["nested ner tasks", "boundary detection", "computational cost reduction"], "background": "1. The need to improve the boundary detection of nested entities, which is crucial for accurate classification in Nested NER tasks. 2. The desire to reduce the computational cost and noise introduced by low-quality candidate spans, especially in cases of high token overlap between nested entities."}
{"hash_id": 4052873079583995066, "entities": ["compositional generalization", "mctg methods", "performance drop", "attribute combinations"], "background": "1. The lack of a comprehensive benchmark to evaluate the compositional generalization of MCTG methods, which is crucial for real-world applications where data with all possible attribute combinations is scarce. 2. The significant performance drop observed in existing MCTG approaches when tested on compositional data, indicating a need for improved methods that can generalize to unseen attribute combinations."}
{"hash_id": 2594930023466276708, "entities": ["pretrained llms", "domainspecific capabilities", "computational democratization"], "background": "1. The need to improve the multifaceted capabilities of pre-trained LLMs in specific domains like programming and mathematics without compromising their general abilities. 2. The desire to reduce the computational resources and data requirements for enhancing LLMs, thus democratizing LLM research."}
{"hash_id": 2794872655616882107, "entities": ["contrastive learning", "narrative coherence", "hard negative samples", "automated generation"], "background": "1. Existing methods for creating negative samples in contrastive learning for narrative coherence are coarse-grained and superficial, leading to low-quality negatives that are easily distinguishable from the real narratives. 2. There is a need for an automated method to generate contrastive narratives that can provide hard negative samples without the costly and non-scalable process of manual annotation."}
{"hash_id": 2470913198095962212, "entities": ["seqseq nlp tasks", "inductive biases", "transformers architecture"], "background": "1. The lack of strong structural inductive biases in popular neural architectures like Transformers hinders their ability to generalize systematically beyond the training distribution, especially in seq2seq NLP tasks. 2. Existing methods to inject inductive biases into seq2seq models are either architecture-specific, making them inflexible, or they require computationally expensive processes like meta-learning with MAML."}
{"hash_id": 1147284254880247540, "entities": ["ai authorship attribution", "ship of theseus paradox", "digital content authenticity"], "background": "1. The need to establish criteria for attributing authorship in texts that have been modified by AI, particularly as LLMs become more proficient in generating and altering content. 2. The philosophical inquiry into the nature of originality and change in texts, akin to the Ship of Theseus paradox, and its implications for copyright, plagiarism, and authenticity in digital content."}
{"hash_id": 5531762578663113153, "entities": ["spoken dialogue", "large language models", "paralinguistics modeling"], "background": "1. The need to improve the capabilities of Large Language Models (LLMs) to understand and respond to speaking styles in spoken dialogue, which is crucial for natural and engaging human-like interactions. 2. The absence of suitable datasets and advanced methods that focus on modeling paralinguistics and speaking styles in speech-to-speech conversational systems."}
{"hash_id": 323334615477211467, "entities": ["unanswerable questions", "kbqa models", "performance maintenance"], "background": "1. The need to improve the robustness of KBQA models in handling unanswerable questions, as current models assume all questions are answerable and struggle with different categories of unanswerability. 2. The requirement to maintain high performance for answerable questions while introducing the capability to detect and handle unanswerable ones, as superficial adaptations to existing models have been shown to hurt performance on answerable questions."}
{"hash_id": 1208690452330615430, "entities": ["multimodal large language models", "finegrained local information", "multimodal grounding"], "background": "1. The need to improve multi-modal large language models' capability to perceive and understand fine-grained local information, which is crucial for tasks that require detailed grounding. 2. The absence of a model that can perform fine-grained multi-modal grounding across image, video, and audio modalities, thus limiting the applicability of existing models in diverse real-world scenarios."}
{"hash_id": 8159746210912686653, "entities": ["lowresource languages", "machine translation", "dataeconomical methods"], "background": "1. The need to extend machine translation to the vast majority of the world's languages, many of which are low-resource and unseen by translation models. 2. The requirement for more efficient and data-economical methods that can adapt to new languages without the need for extensive retraining or additional parallel data."}
{"hash_id": 4673309139852369471, "entities": ["limitation of linearization", "transformer models", "tabletotext generation", "structural information preservation"], "background": "1. The need to overcome the limitations of current linearization methods that result in verbose representations, lack of space efficiency, and limited context windows in Transformer models. 2. The desire to improve generalization capabilities in table-to-text generation models by preserving the structural information of the input data."}
{"hash_id": 9025283882691956880, "entities": ["answer granularity", "lexical matching", "human evaluation discrepancy", "language models overconfidence"], "background": "1. The need to account for different levels of answer granularity in question answering evaluation to better reflect the knowledge encapsulated in language models. 2. The desire to reduce the discrepancy between lexical matching and human evaluation, particularly in cases where language models generate incorrect but specific answers due to overconfidence."}
{"hash_id": 4017306086214787024, "entities": ["linguistic analysis", "interlinear glossed text", "morphological segmentation models", "resourceconstrained settings"], "background": "1. To expedite the process of linguistic analysis, including the creation of Interlinear Glossed Text (IGT), which is crucial for endangered language documentation but is costly and time-consuming to generate. 2. To improve the performance of morphological segmentation models in severely resource-constrained settings by utilizing the more abundant translation data that is often available in language documentation pipelines."}
{"hash_id": 1095349082977656719, "entities": ["evaluation benchmark", "coderelated tasks", "large language models", "code understanding", "challenging dataset"], "background": "1. The need for a unified and comprehensive evaluation benchmark for code-related tasks that covers multiple programming languages and tasks, and uses execution-based metrics rather than just lexical similarity. 2. The requirement to advance the development of more general-purpose large language models capable of code understanding, generation, translation, and retrieval by providing a challenging and diverse dataset for training and evaluation."}
{"hash_id": 222477167629768930, "entities": ["evaluation method llms", "longform text generation", "llm proficiency"], "background": "1. The need for a more robust and precise evaluation method for LLMs in generating long-form text, as current methods relying on automated metrics and crowdsourcing are labor-intensive, inefficient, and discordant with human judgment criteria. 2. The gap in understanding the proficiency of LLMs in generating long-form texts, which is an essential aspect for sophisticated applications but has been relatively unexplored."}
{"hash_id": 5545430622294352543, "entities": ["large language models", "contextual information", "computational patterns", "internal parametric knowledge"], "background": "1. The need to understand how large language models (LLMs) balance contextual information with stored factual knowledge, particularly when the two conflict. 2. The importance of identifying distinct computational patterns that indicate when an LLM is grounded in context versus relying on its internal parametric knowledge."}
{"hash_id": 5881439715705431340, "entities": ["multimodal large language models", "multipanel images", "existing benchmarks gap"], "background": "1. The need to evaluate and improve the capabilities of Multimodal Large Language Models (MLLMs) in understanding complex multipanel images, which are commonly encountered in real-world scenarios. 2. The gap in existing benchmarks that adequately challenge MLLMs in comprehending the content and layout of multipanel images, as evidenced by their struggles with interpreting such images despite their success with single-panel images."}
{"hash_id": 4760268776894703649, "entities": ["web agents", "realworld applicability", "multimodal models", "browsing experience"], "background": "1. The need to bridge the gap between the capabilities of existing web agents and their real-world applicability, as current agents are restricted to single modalities and simplified environments. 2. The desire to leverage the advancements in large multimodal models to create a more effective and human-like web browsing experience."}
{"hash_id": 1191980839108022690, "entities": ["contextspecific linguistic elements", "generalizable framework", "social meaning detection"], "background": "1. Traditional classification models often over-fit to context-specific linguistic elements, making it difficult to transfer social meaning detection to unseen domains. 2. There is a need for a generalizable framework that can break through the opaque surface form of conversation text to make social cues more transparent and actionable."}
{"hash_id": 6080173590122479584, "entities": ["target identification", "multiword units", "frame identification", "negative samples"], "background": "1. The need to improve the coverage and accuracy of target identification, especially for discontinuous multi-word lexical units, which previous models have failed to address effectively. 2. The requirement to enhance frame identification by effectively utilizing negative samples, particularly for targets with limited candidate frames, to improve model performance on rare and under-utilized frames."}
{"hash_id": 5799698017629921546, "entities": ["reduce llm output variability", "reranking method", "openended generation tasks"], "background": "1. The need to reduce the variability in the quality of sampled outputs from Large Language Models (LLMs) without increasing computational overhead significantly. 2. The requirement for a reranking method that can be applied to open-ended generation tasks, such as code generation, summarization, and translation, where there is no unique correct answer or chain of thought to marginalize over."}
{"hash_id": 2901845585026589430, "entities": ["interpreting complex feedback", "scientific writing", "revising scientific papers"], "background": "1. The need to improve the capabilities of NLP systems in interpreting complex writing feedback and making contentful edits in technical domains like scientific writing. 2. The lack of datasets and research on tasks related to revising scientific papers based on peer feedback, which hinders the development of advanced writing assistants."}
{"hash_id": 7539998741370727037, "entities": ["labeling hard training data", "scalable oversight problem", "domainspecific tasks"], "background": "1. The difficulty and cost associated with labeling hard training data, especially in specialized domains, which leads to the scalable oversight problem. 2. The need to improve model performance on challenging domain-specific tasks without access to large human-labeled training corpora."}
{"hash_id": 5685790478357608065, "entities": ["instruction tuning", "lowerresource languages", "pivot language resources"], "background": "1. The need to improve instruction tuning for large language models in lower-resource languages, where models exhibit\u8f83\u5dee\u7684\u6027\u80fd due to the imbalanced distribution of languages in pre-training data. 2. The desire to bridge the gap between high-resource and lower-resource languages by leveraging the pivot language's rich resources and the model's stronger performance in English."}
{"hash_id": 1600657132926266826, "entities": ["error propagation", "structured reasoning", "style discrepancy", "reasoning graphs"], "background": "1. To overcome the issue of error propagation in single-pass decoding and the lack of error correction in existing structured reasoning approaches using large language models. 2. To address the challenge of style discrepancy that arises when representing reasoning graphs as flattened strings, which leads to output style mismatch and subpar performance."}
{"hash_id": 5120324780351200614, "entities": ["overcoming llm limitations", "selfreflection degeneration", "diverse insights feedback"], "background": "1. The need to overcome the limitations of self-reflection in LLMs, which can lead to a degeneration of thought when models become overly confident in their answers. 2. The recognition that diverse insights and external feedback from different model families are crucial for enhancing the reasoning capabilities of LLMs."}
{"hash_id": 7726803763652040405, "entities": ["selfassessment capabilities", "feedback generation", "reasoning loop", "knowledgerich tasks"], "background": "1. The need to improve the self-assessment and feedback generation capabilities of large language models in the absence of ground truth or external resources. 2. The requirement to prevent large language models from getting stuck in a particular reasoning loop and to enhance their performance in knowledge-rich reasoning tasks."}
{"hash_id": 1224336758603455828, "entities": ["lack of datasets", "story detection models", "communitycentric platforms"], "background": "1. The lack of appropriate datasets and tools for training and evaluating story detection models across diverse online communities. 2. The need to understand the distributional characteristics and social functions of storytelling in large community-centric social media platforms."}
{"hash_id": 461809873929080532, "entities": ["deep syntactic parsing", "large language models", "constituency parsing", "chunking steps"], "background": "1. The need to improve the performance of large language models on deep syntactic parsing tasks, as they struggle with generating correct full parse trees despite being effective in shallow parsing. 2. The desire to leverage the strengths of LLMs in chunking and to decompose the complex task of constituency parsing into simpler steps to enhance overall parsing performance."}
{"hash_id": 1163716711457264275, "entities": ["anticipation bias", "dialogue summarization", "multiple language models"], "background": "1. The need to overcome the limitations of using one large language model at a time, which can lead to anticipation bias and loss of important information in dialogue summarization. 2. The desire to leverage the capabilities of multiple large language models without treating them as black boxes, in order to discriminatively learn essential content from different aspects of a dialogue."}
{"hash_id": 7261575056315730590, "entities": ["chinese medical domain", "natural language processing", "large language models"], "background": "1. The need for more effective natural language processing solutions that are specifically tailored to the Chinese medical domain to bridge the knowledge gap and align with human preferences. 2. The requirement to improve the generalization ability and safety of large language models in medical applications, particularly in handling lengthy and contextually coherent medical texts."}
{"hash_id": 5823168036589270407, "entities": ["large language models", "chainofthought prompts", "inner mechanism", "llm reasoning"], "background": "1. The limited understanding of how large language models process Chain-of-Thought prompts internally, which hinders our ability to improve their reasoning capabilities. 2. The need to provide a fundamental understanding of the inner mechanism of LLM reasoning, especially regarding the role of specific components like equations in CoT prompts."}
{"hash_id": 8783715718685731264, "entities": ["ratingbased human evaluation", "referential expressions", "comprehensive evaluation", "referential success"], "background": "1. The current method of rating-based human evaluation for REG models may not capture the nuanced differences in referential expressions, leading to indistinguishable outcomes between advanced neural models and simple rule-based systems. 2. The need for a more comprehensive evaluation that assesses not only the quality of REs but also their referential success and potential for optimization in discourse."}
{"hash_id": 9053570188791307773, "entities": ["gap between event summarization", "topic summarization", "dynamic timeline summarization", "incremental approach"], "background": "1. The need to bridge the gap between event and topic timeline summarization, which have been treated as separate entities, to create a more comprehensive and coherent narrative from text streams. 2. The recognition that existing timeline summarization methods are not well-suited to the dynamic and evolving nature of real-world events, which require an incremental approach to keep timelines up-to-date."}
{"hash_id": 1185104649605905259, "entities": ["docre performance improvement", "inferred potential facts", "error propagation mitigation"], "background": "1. The need to improve the performance of DocRE models by inferring potential facts that are not explicitly mentioned in the text. 2. The desire to mitigate error propagation issues that arise from pipeline approaches where logical rules are learned separately and then applied to refine the model's predictions."}
{"hash_id": 8061455736357850487, "entities": ["speechtospeech translation", "parallel speech data", "speechtotext translation", "texttospeech"], "background": "1. The high reliance on large-scale parallel speech data for training speech-to-speech translation models, which is difficult and expensive to collect. 2. The potential to leverage the abundant data and pretrained models available for speech-to-text translation (S2TT) and text-to-speech (TTS) to reduce the need for parallel speech data."}
{"hash_id": 2045470791880559738, "entities": ["crossmodality selflearning", "eegtext integration", "intramodality selfreconstruction", "eegbased language decoding"], "background": "1. The need to integrate cross-modality self-learning between EEG and text with intramodality self-reconstruction of EEG features or textual sequences. 2. The under-utilization of large language models to enhance EEG-based language decoding."}
{"hash_id": 6246294875598222191, "entities": ["mitigation of inference latency", "cumulative layer latency", "performance drop reduction"], "background": "1. The need to mitigate the high inference latency that negatively affects user experience in large language models due to their increasing size and complexity. 2. The observation that existing methods focus on reducing per-layer latency but overlook the cumulative latency from the total number of layers, or they attempt to reduce layer count at the cost of significant performance drop."}
{"hash_id": 2355886584729753169, "entities": ["incontext learning", "lowresource settings", "adversarial learning", "optimize prompts"], "background": "1. The need to improve the performance of in-context learning, especially in low-resource settings where data is scarce. 2. The desire to leverage the success of adversarial learning in other domains to optimize prompts for in-context learning without the computational overhead of training two models."}
{"hash_id": 5444111768727354409, "entities": ["retrievethenread paradigm", "multihop question answering", "large language models"], "background": "1. Theretrieve-then-read paradigm is constrained by the performance of the retriever and the noise in the retrieved documents, which leads to suboptimal answers in multi-hop question answering tasks. 2. Large language models (LLMs) have extensive knowledge and reasoning capabilities that are underutilized in the current retrieval-augmented generation approaches."}
{"hash_id": 7966489054887208113, "entities": ["dynamic knowledge update", "multimodal inputs", "embodied interactions", "realworld framework", "medical image annotations"], "background": "1. The need to develop AI agents with the capability to dynamically update their knowledge based on multimodal inputs and prior contexts, similar to human communication. 2. The requirement for a structured, interpretable framework that can handle the complexity of real-world, embodied interactions, such as teaching a robot about its environment or assisting in medical image annotations."}
{"hash_id": 4455233448616478771, "entities": ["large language models", "personalized outputs", "nlp benchmarks", "usercentric models"], "background": "1. The need to develop and evaluate large language models that can produce personalized outputs to meet the unique needs and preferences of users, which is crucial for real-world applications. 2. The existence of a gap in the current NLP benchmarks that do not adequately address personalization, leading to a lack of user-centric models that can adapt to specific user requirements."}
{"hash_id": 4766402484709036803, "entities": ["data retention decisions", "data curation", "model biases", "social implications"], "background": "1. To scrutinize the underscrutinized decisions around data retention and removal during the initial data curation stage for large language models, which can lead to biases in model behavior. 2. To investigate the social implications of pretraining data curation practices, particularly in relation to the representation of diverse perspectives and knowledge."}
{"hash_id": 5547618851431427167, "entities": ["evaluation benchmarks", "llms", "multiturn dialogues", "deficiencies", "chat abilities"], "background": "1. The lack of comprehensive and fine-grained evaluation benchmarks for LLMs in multi-turn dialogues, which overlook the complexity and nuances of real-life conversations. 2. The need to identify specific deficiencies in LLMs' multi-turn chat abilities to guide future model improvements and better alignment with human-like dialogue interactions."}
{"hash_id": 8847780301763732358, "entities": ["eventlevel sentiment analysis", "financial text", "dedicated framework"], "background": "1. The need to improve the accuracy of sentiment predictions in financial text by focusing on event-level analysis, as events are central to sentiment expressions. 2. The absence of a dedicated event-level financial sentiment analysis framework that can effectively handle the complexity of financial text."}
{"hash_id": 558029172729864646, "entities": ["language model credibility", "controversial queries", "evidence quality alignment"], "background": "1. The need to understand and improve how language models assess the credibility and convincingness of real-world evidence, especially for controversial queries. 2. The requirement to align language model judgments more closely with human assessments of evidence quality and credibility."}
{"hash_id": 7570972164411357056, "entities": ["integration methods", "graph structural information", "visionlanguage models", "graph data understanding"], "background": "1. The limitations of current integration methods that lose crucial graph structural information when using sequence-based prompt representations and face alignment challenges when integrating GNN-learned features into LLMs. 2. The potential of using Vision-Language Models (VLMs) to overcome these limitations by leveraging their ability to process visual information and fuse it with text, providing a more direct and informative approach to graph data understanding."}
{"hash_id": 8767729886580688974, "entities": ["language models", "lowresource languages", "multilingual supervision", "scaling approaches", "targeted training corpora"], "background": "1. The need to enhance the performance of language models on reasoning tasks in low-resource languages without requiring multilingual supervision. 2. The challenge of scaling existing approaches to adapt language models to a large number of languages, due to the requirement of targeted training corpora for each language."}
{"hash_id": 7764879616628581060, "entities": ["bridging llms inferential gap", "manual rule curation challenge", "structurally diverse rules"], "background": "1. To bridge the gap between LLMs' basic understanding of inferential rules and human-level proficiency, particularly in complex and structurally diverse rules. 2. To address the challenge of manually curating a large and diverse set of inferential rules, which is labor-intensive and often results in overly simplified and specific rules."}
{"hash_id": 484299936934819364, "entities": ["subgoalbased methods", "problemsolving capabilities", "reinforcement learning", "large language models", "complex mathematical problems"], "background": "1. The success of subgoal-based methods in enhancing problem-solving capabilities in both Reinforcement Learning and Large Language Models. 2. The need to improve the performance of LLMs in solving complex mathematical problems by optimizing the subgoal breakdown process to increase the probability of finding a solution."}
{"hash_id": 5829355591962004741, "entities": ["opensource llm agents", "advanced models", "learning from trajectories", "exploration failure learning"], "background": "1. The need to enhance the performance of open-source LLM agents, which are currently less effective than advanced models like GPT-4 in constructing agents for various tasks. 2. The recognition that learning from successful trajectories alone may not capture the full range of experiences needed for improvement, and that exploration and failure can provide valuable learning opportunities."}
{"hash_id": 8104514827212849002, "entities": ["enhance generalization", "large language models", "multiple reasoning paths"], "background": "1. The need to enhance the generalization ability of Large Language Models in reasoning tasks, as Supervised Fine-Tuning with single annotated reasoning paths is insufficient. 2. The recognition that learning from multiple valid reasoning paths can improve the model's ability to solve problems across various domains."}
{"hash_id": 4132293252089165894, "entities": ["visual information alignment", "knowledgebased vqa tasks", "spatial relationships"], "background": "1. The current LMMs lack the ability to align visual information with relevant knowledge, which is crucial for improving accuracy in knowledge-based VQA tasks. 2. The visual knowledge dimension, which includes spatial relationships and symbols, is essential for human cognition and is not adequately integrated into existing multimodal models."}
{"hash_id": 54966712983309252, "entities": ["controllable text generation", "tradeoff", "learningfree method", "high performance"], "background": "1. To overcome the trade-off between computational expense and model efficacy in controllable text generation. 2. To provide a learning-free method that does not require extensive training or attribute-specific data yet maintains high performance."}
{"hash_id": 400411474117501496, "entities": ["prompt design bias", "singlecriterion evaluations", "hierarchical evaluation", "explainable judgments"], "background": "1. To address the limited alignment and trustworthiness of LLM judgments due to prompt design bias and the inability of single-criterion evaluations to capture the complexity of human judgments. 2. To empower LLM-based evaluators by incorporating the hierarchical and comprehensive evaluation mindset of human experts, which allows for more aligned and explainable judgments."}
{"hash_id": 4893164890833576689, "entities": ["generalization of aes models", "unseen prompts", "performance improvements", "complex neural architectures"], "background": "1. The need to improve the generalization of AES models to unseen prompts without the requirement for retraining on new prompt data. 2. The lack of understanding of what contributes to the performance improvements in state-of-the-art AES models, which often rely on complex neural architectures."}
{"hash_id": 5713759608681411549, "entities": ["gender norms", "societal biases", "emotion attribution", "llm gendered studies"], "background": "1. The need to understand and address the reflection of societal gender norms and biases within large language models, particularly in the context of emotion attribution. 2. The absence of comprehensive studies on gendered emotion attribution in state-of-the-art LLMs, despite the well-established link between emotion and gender in societal discourse."}
{"hash_id": 552935501026681885, "entities": ["hierarchical text classification", "labeled data scarcity", "unseen class classification"], "background": "1. The scarcity of labeled data and the high cost of manually annotating data samples in hierarchical text classification, which makes supervised methods difficult to implement in real-world applications. 2. The challenge of accurately classifying documents into unseen classes, especially at the deepest level of the label hierarchy, which is often the most important for practical applications."}
{"hash_id": 2749170702083589689, "entities": ["multimodal empathetic communication", "stickers", "nontextual modality", "comprehensive datasets"], "background": "1. The need to enhance multimodal empathetic communication by incorporating stickers, which are underexplored in current dialogue research, to provide more abundant and intuitive emotional information in online interactions. 2. The lack of comprehensive datasets that include stickers as a non-textual modality to support the development of multimodal empathetic dialogue systems."}
{"hash_id": 8973555454711898555, "entities": ["multiview learning", "consensus principles", "multihead selfattention", "attention heads diversity"], "background": "1. The need to balance the complementary and consensus principles in multi-view learning, as the current design of multi-head self-attention predominantly focuses on the former, neglecting the latter which is crucial for minimizing disagreement and improving outcomes. 2. The challenge of encouraging consensus in multi-head self-attention without compromising the diversity and sufficiency of information captured by different attention heads."}
{"hash_id": 6934153462589185872, "entities": ["prevent llm inaccuracies", "highstakes environments", "uncertainty estimation methods", "semantic token importance"], "background": "1. The need to prevent inaccurate or misleading outputs from generative LLMs, especially in high-stakes environments like medical advice applications, where such errors can have severe consequences. 2. The limitation of current Uncertainty Estimation (UE) methods that treat all tokens equally, without considering the varying semantic importance of each token in the generated response."}
{"hash_id": 5079089360404064172, "entities": ["benchmark complexity", "standardized testing framework", "vision language models"], "background": "1. The need for a more comprehensive and challenging benchmark that reflects the complexity of real-world information processing to accurately assess the capabilities of Vision Language Models (VLMs). 2. The absence of a standardized testing framework that evaluates VLMs on multi-disciplinary, multilingual, and multimodal tasks, which are crucial for advancing the field of AI."}
{"hash_id": 6914515850765478047, "entities": ["data augmentation methods", "lowresource settings", "unordered entities", "sequencetosequence ner systems"], "background": "1. Existing data augmentation methods either break semantic coherence or require substantial labeled data, which is not feasible in low-resource settings like few-shot NER tasks. 2. The unordered nature of entities in NER tasks is often overlooked, leading to a loss of viable samples and suboptimal performance in sequence-to-sequence NER systems."}
{"hash_id": 8116619762379725053, "entities": ["unlocking roleplay capabilities", "scalable enhancement method", "avoiding manual annotation"], "background": "1. The need to unlock the intrinsic role-play capabilities of large language models without imitating proprietary models, which can limit performance and introduce hallucinations. 2. The desire to create a scalable and efficient method for enhancing role-playing capabilities that does not rely on extensive manual annotation, which is costly and variable."}
{"hash_id": 349150269131376695, "entities": ["democratize closedsource chatgpt", "opensource alternative", "multiround dialogues", "humanlikeness conversational dynamics"], "background": "1. The need to democratize the closed-source ChatGPT by creating an open-source alternative that can perform well in multi-round dialogues. 2. The observation that current methods relying on ChatGPT roleplay simulations lack human-likeness, topic diversity, and genuine multi-round conversational dynamics."}
{"hash_id": 318646733112799450, "entities": ["texttosql tasks", "democratizing data access", "synthetic data", "domain generalization"], "background": "1. To bridge the performance disparity between open-source and closed-source LLMs in text-to-SQL tasks, which is crucial for democratizing data access and analysis. 2. To explore the potential of synthetic data in enhancing domain generalization and reducing dependency on proprietary models, thus addressing concerns of openness, privacy, and cost."}
{"hash_id": 3775823844906141288, "entities": ["online information structuring", "structured outputs", "comprehension aid"], "background": "1. The overwhelming amount of online information demands efficient structuring to facilitate faster comprehension and insight extraction. 2. Current large language models are not effective in generating structured outputs that aid in comprehension, particularly when dealing with complex or sparse content."}
{"hash_id": 3000855525536053391, "entities": ["causal masking", "generalisation properties", "pretraining efficiency", "distracting information"], "background": "1. The lack of systematic analysis on how the sequence composition strategy, specifically causal masking, affects the generalisation properties of language models. 2. The need to improve pre-training efficiency while reducing the negative impact of distracting information from irrelevant documents on model performance."}
{"hash_id": 7952030792751933764, "entities": ["kv cache memory consumption", "eviction strategy", "longcontext modeling tasks"], "background": "1. The high cost and memory infeasibility of hosting LLMs due to the large memory consumption of KV Cache, which hinders their deployment on fixed memory hardware. 2. The need for a more effective and less biased eviction strategy than the local statistics of accumulated attention scores, which are currently used but may lead to suboptimal performance on real-life long-context modeling tasks."}
{"hash_id": 800721790997504958, "entities": ["spiking neural networks", "energy efficiency", "texttospeech", "partialtime dependency"], "background": "1. The need to leverage the energy efficiency of Spiking Neural Networks (SNN) for generative tasks like Text-to-Speech (TTS) without compromising on quality. 2. The challenge of overcoming the \"partial-time dependency\" issue in SNNs, which hinders the capture of long-term dependencies necessary for TTS."}
{"hash_id": 3965605482075278071, "entities": ["multichange captioning", "viewpoint changes", "difference features", "genuine changes"], "background": "1. The existing methods for multi-change captioning are limited in their ability to handle complex and coupled changes, especially under viewpoint changes, leading to unreliable difference features for language decoding. 2. The need to capture all genuine changes in an image pair and to distinguish them from irrelevant changes, such as those caused by viewpoint or illumination variations."}
{"hash_id": 249419740850515833, "entities": ["pretrained code language models", "private repositories", "repospecific context", "code completion performance"], "background": "1. The limitations of pre-trained code language models in generating correct completions for code in private repositories due to insufficiently relevant cross-file context. 2. The need for a more precise retrieval method that can provide repo-specific context to enhance code completion performance."}
{"hash_id": 6241124426450378806, "entities": ["promptbased methods", "large language models", "text classification", "computational efficiency"], "background": "1. The need to reduce the high complexity and inference overhead associated with prompt-based methods using large language models in text classification. 2. The desire to achieve competitive performance in text classification without the need for large-scale LLMs, thus reducing computational costs and improving efficiency."}
{"hash_id": 4973910207408029492, "entities": ["costeffective language models", "scaling law limitation", "constrained resources"], "background": "1. The need to develop smaller language models that are cost-effective and flexible in deployment, without compromising on performance. 2. The challenge of breaking through the scaling law limitation that links model size to performance, especially when resources are constrained."}
{"hash_id": 1159074072831161502, "entities": ["homophones disambiguation", "endtoend speech translation", "translation accuracy", "crossmodal conversions", "homophone ambiguity"], "background": "1. The limited exploration of disambiguation strategies specifically for homophones in end-to-end speech translation, despite their significant impact on translation accuracy. 2. The need to improve the performance of end-to-end ST models, which currently struggle with managing concurrent cross-modal and cross-lingual conversions, especially in cases of homophone ambiguity."}
{"hash_id": 801911459264056389, "entities": ["bridging deep learning and natural languages", "structured prediction tasks", "binary representations", "large language models"], "background": "1. The need to bridge the gap between the continuous nature of deep learning and the discrete intrinsic property of natural languages, which is essential for effectively modeling structured prediction tasks. 2. The potential to reduce the parameter size and improve gradient propagation by using binary representations instead of large embedding matrices, especially in the context of large language models."}
{"hash_id": 1647693965702419701, "entities": ["textmusic connection", "music generation challenges", "long music generation", "model inefficiency", "diversity in music"], "background": "1. The lack of research exploring the connection between text and music, despite both being rich forms of communication capable of conveying emotions, stories, and ideas. 2. The existing challenges in music generation, including the inability to generate long pieces of music, inefficiency in model operation, lack of diversity in generated music, and limited controllability by text prompts."}
{"hash_id": 5606029453910870799, "entities": ["multihop question answering", "knowledge updating", "question decomposition limitations"], "background": "1. The need to update multi-hop question answering systems with new knowledge without the expensive and time-consuming process of retraining or fine-tuning. 2. The limitations of existing methods that couple question decomposition and knowledge editing, which can hinder the reasoning abilities of large language models (LLMs) and lead to noise in the question decomposition process."}
{"hash_id": 3084868511123010001, "entities": ["proactive intervention", "toxic content", "large language models", "visual language models"], "background": "1. The need for proactive intervention against toxic content in memes, which is currently limited and focuses mainly on text-based content, neglecting the widespread influence of multimodal memes. 2. The potential of Large Language Models (LLMs) and Visual Language Models (VLMs) to understand and generate interventions for toxic content, but their current limitations in grounding knowledge and handling the unique context of memes."}
{"hash_id": 8855779920304228807, "entities": ["sampleefficient ocr", "lowresource documents", "labeled sequences dependency"], "background": "1. The need for a more sample-efficient and extensible OCR solution to capture the diversity of documentary history, particularly for low-resource document collections. 2. The requirement to reduce dependency on extensive labeled sequences and compute resources, enabling wider accessibility and community engagement in digitizing historical documents."}
{"hash_id": 2092626401565877688, "entities": ["language models", "backdoor attacks", "defense methods"], "background": "1. The lack of interpretability in the internal mechanisms of language models (LMs) makes them vulnerable to backdoor attacks, which is a significant security concern. 2. Existing defense methods against backdoor attacks struggle with complex triggers in real-world scenarios and lack a thorough exploration of the learning mechanisms behind backdoor poisoning."}
{"hash_id": 4893319415919482993, "entities": ["language model scalability", "attribution methods", "parametric knowledge"], "background": "1. The increasing scalability of language models makes it difficult to understand their inner workings and update or correct the embedded knowledge without the expense of retraining. 2. There is a need to systematically compare and evaluate different attribution methods to gain a more comprehensive understanding of a language model's parametric knowledge."}
{"hash_id": 6686043273069877681, "entities": ["multistep multidomain reasoning", "mcot benchmarks", "vision large language models"], "background": "1. The current MCoT benchmarks lack complexity and do not fully capture the challenges of multi-step, multi-domain, and multi-modal reasoning, leading to an overestimation of model capabilities. 2. There is a need for a benchmark that can accurately evaluate the performance of Vision Large Language Models (VLLMs) in real-world scenarios that require multi-modal reasoning."}
{"hash_id": 1558667324427825916, "entities": ["longcontext modeling", "semantic dependencies", "finetuning data", "longrange dependencies"], "background": "1. The current approach of directly training LLMs with long context windows does not ensure the presence of strong semantic dependencies across these contexts, leading to suboptimal performance in long-context modeling. 2. The quality of fine-tuning data, particularly the presence of long-range dependencies, is crucial for enhancing the long-context modeling capabilities of LLMs, but such high-quality data is not commonly available or easily identifiable."}
{"hash_id": 1685947029932593842, "entities": ["qualitylatency tradeoff", "streaming reordering", "data sparsity alleviation", "monolingual textonly data"], "background": "1. The need to improve the quality-latency trade-off in simultaneous speech translation by incorporating streaming and re-ordering capabilities. 2. The requirement to alleviate data sparsity issues in end-to-end speech translation by utilizing monolingual text-only data."}
{"hash_id": 1908167088811922595, "entities": ["prompt tuning interpretability", "hard prompts naturalness", "computational overhead reduction", "discrete optimization approach"], "background": "1. The need to improve the interpretability and naturalness of hard prompts discovered through reinforcement learning in prompt tuning, as current methods often produce prompts that are not easily understood by humans and lack transferability between different pre-trained models. 2. The requirement to reduce computational overhead and the infeasibility of computing internal gradients for models, especially when access is limited to APIs, which necessitates a more efficient and discrete optimization approach."}
{"hash_id": 8458600620712332969, "entities": ["modular approaches", "summarization tasks", "evaluation metric"], "background": "1. The need for greater flexibility and interpretability in summarization tasks, which modular approaches can provide by allowing independent development and improvement of components. 2. The absence of a single, agreed-upon metric for automatically evaluating the quality of summaries, particularly in terms of factuality and the complexities of long narratives."}
{"hash_id": 7724235919814581233, "entities": ["prompting techniques", "theory of mind tasks", "cognitive science theories"], "background": "1. The limited applicability of existing prompting techniques like Chain-of-Thought (CoT) to Theory of Mind (ToM) tasks. 2. The need to bridge the gap between cognitive science theories and the capabilities of Large Language Models (LLMs) in understanding mental states."}
{"hash_id": 368862691561523623, "entities": ["improve llms performance", "financial business reasoning", "quantitative reasoning benchmark"], "background": "1. The need to improve large language models' (LLMs) performance in reasoning about financial and business questions, which require precision and technical knowledge. 2. The absence of a comprehensive benchmark that specifically evaluates quantitative reasoning capabilities in the context of business and finance."}
{"hash_id": 2197504996489879718, "entities": ["computational cost reduction", "learning stability", "reinforcement learning alternative"], "background": "1. The need to reduce the computational cost and enhance learning stability when directly optimizing final performance metrics in image captioning for large-scale vision language models (VLMs). 2. The desire to find an alternative to reinforcement learning that is as effective but more computationally efficient for fine-tuning and customizing VLMs."}
{"hash_id": 3189934823098074173, "entities": ["hateful content detection", "bengali language", "social entities", "hate speech spread"], "background": "1. The need to identify and address hateful content in low-resource languages, particularly Bengali, which is widely spoken but lacks sufficient research and resources in the realm of hateful meme detection. 2. The importance of recognizing not just hateful memes, but also the specific social entities they target, to better understand and counteract the spread of hate speech on social media."}
{"hash_id": 1959802919898896066, "entities": ["high cost annotation", "text generation models", "model selection", "efficiency comparison"], "background": "1. The high cost and resource intensiveness of extensive annotation required for model selection in text generation tasks. 2. The need for a more efficient and reliable method to compare and select between different text generation models, prompts, and configurations."}
{"hash_id": 2231668698382124955, "entities": ["ethical concerns", "legal challenges", "sensitive data", "retraining limitations"], "background": "1. Ethical concerns and legal challenges, such as copyright infringement, arising from the use of sensitive or private data in training large language models. 2. The practical limitations of retraining pre-trained LLMs from scratch due to the massive scale of the data and the costs involved."}
{"hash_id": 7632410781872150122, "entities": ["language model interpretability", "mechanism interaction", "model success failure"], "background": "1. The need to go beyond the current interpretability research focus on individual mechanisms and instead understand how multiple mechanisms interact within language models. 2. The desire to uncover the specific points within language models where mechanisms compete and how one becomes dominant, which can lead to a better understanding of model success or failure."}
{"hash_id": 7469151394590552825, "entities": ["accessible medical information", "health literacy", "factuality benchmark", "plain language summarization"], "background": "1. The need for accessible and accurate medical information for laypeople to improve health literacy and informed decision-making. 2. The lack of a standardized evaluation benchmark for factuality in plain language summarization of medical evidence, particularly RCTs."}
{"hash_id": 5258246390229972308, "entities": ["fewshot learning", "single template", "template correlations"], "background": "1. The need to adapt to unseen aspects quickly in real-world applications, which often involve a few labeled samples (few-shot learning). 2. The limitation of existing methods that focus on a single template or different template orders without considering the correlations among various templates."}
{"hash_id": 2707648351951435763, "entities": ["safety considerations", "large language models", "nlp tasks", "weaker safety alignment"], "background": "1. The need to ensure that Large Language Models (LLMs) are aligned with safety considerations across all NLP tasks, not just QA, to prevent the processing of harmful content. 2. The recognition of a previously unidentified vulnerability where tasks with weaker safety alignment, like summarization, can be exploited to compromise the integrity of more robust tasks."}
{"hash_id": 744143978406506041, "entities": ["language models", "brain information prediction", "modality differences", "stimulus feature impact"], "background": "1. The need to clarify what types of information language models truly predict in the brain, particularly in the context of different modalities (text vs. speech). 2. The desire to understand the specific impact of low-level stimulus features on the alignment between language models and brain activity, especially across different brain regions."}
{"hash_id": 8597368882940975387, "entities": ["improve llms performance", "document intelligence tasks", "multimodal llms complexity"], "background": "1. The need to improve the performance of large language models (LLMs) on document intelligence tasks which are inherently multi-modal, requiring the understanding of both text content and visual layout cues. 2. The desire to reduce the complexity and computational cost of existing multimodal LLMs which use complex vision backbone architectures to encode image information."}
{"hash_id": 8684332199456377425, "entities": ["attack method limitations", "watermark detection", "postprocessing robustness", "academic integrity"], "background": "1. The need to overcome the limitations of existing attack methods that fail to evade watermark detection for longer text segments and are not practical due to the requirement of an additional high-quality unwatermarked LLM for paraphrasing. 2. The importance of ensuring the robustness of watermarking approaches under realistic post-processing conditions, particularly in educational settings where academic integrity is a concern."}
{"hash_id": 6466522974168102039, "entities": ["incontext learning", "language models", "emergent abilities"], "background": "1. The need to explain the emergence of in-context learning in language models, which is crucial for understanding their emergent abilities and predicting their performance. 2. The gap in knowledge regarding what specific structures in pre-training data contribute to the in-context learning capability of language models."}
{"hash_id": 2579880447994911582, "entities": ["realistic benchmark", "theoryofmind reasoning", "diverse question types", "benchmark limitations", "overfitting estimation"], "background": "1. The need to create a more realistic and diverse benchmark that reflects real-life scenarios for evaluating Theory-of-Mind reasoning in LLMs, as current benchmarks lack personality traits, motivated actions, and diverse question types. 2. The requirement to overcome the limitations of existing benchmarks, such as ambiguous narratives and overstructured contexts, which can lead to overfitting and an underestimation of models' true capabilities."}
{"hash_id": 8201808041293194289, "entities": ["parallel corpora", "slt technology", "annotation requirements", "privacy risks", "sign language data"], "background": "1. The lack of large, clean, and labeled parallel corpora hinders the advancement of SLT technology, and current annotation requirements are not scalable due to the labor-intensive nature of the task. 2. The use of web-scraped sign language data for training poses significant privacy risks due to the biometric information present in the videos."}
{"hash_id": 6459486572908557730, "entities": ["scalarreward rlhf", "diverse user preferences", "individual preferences", "adaptable finetuning"], "background": "1. The limitations of scalar-reward Reinforcement Learning from Human Feedback (RLHF) in capturing diverse user preferences and its tendency to align LLMs towards an \"average-user\" preference, which may not represent the complexities of individual preferences. 2. The need for a more adaptable and controllable approach to fine-tune LLMs that can arithmetically specify desired trade-offs between different objectives (e.g., helpfulness and verbosity)."}
{"hash_id": 6812361694133000291, "entities": ["faked characters", "chinese writing assistance", "character errors dataset"], "background": "1. The need to address the more common and challenging issue of faked characters in Chinese writing assistance, which existing datasets and methods have largely ignored due to their focus on misspelled characters. 2. The requirement to develop a dataset and methods that can represent and handle the complexity of real-world Chinese character errors, including those that cannot be encoded in computer text systems."}
{"hash_id": 2610073268674587358, "entities": ["interpretability methods", "highlevel concepts", "distributed representations", "causal criteria"], "background": "1. The need to quantitatively compare existing interpretability methods on disentangling the representations of multiple high-level concepts within language models. 2. The requirement for a method that can identify distributed representations that satisfy multiple causal criteria, moving beyond neuron-level analyses."}
{"hash_id": 8714220288638822509, "entities": ["dialogue state tracking", "large language models", "zeroshot learning"], "background": "1. The need to improve dialogue state tracking in task-oriented dialogues using large language models without requiring extensive data for retraining or tuning. 2. The desire to leverage the zero-shot learning capabilities of LLMs to adapt to diverse domains without the need for specialized models for each domain."}
{"hash_id": 5921638425507164777, "entities": ["chart summarization models", "factual perceptual errors", "training datasets limitations"], "background": "1. The need to improve the faithfulness of chart summarization models, which often suffer from factual and perceptual errors, impacting their usefulness for applications like aiding the visually impaired and automating data interpretation in complex domains. 2. The limitations of existing training datasets and reference-based evaluation metrics, which can encourage models to hallucinate information and do not account for unreferenced but correct insights."}
{"hash_id": 5871605014495123601, "entities": ["data collection challenges", "dialogue state tracking", "dynamic domain adaptation"], "background": "1. The high costs and challenges associated with collecting and annotating real dialogue data for DST, especially due to the presence of personal or sensitive information. 2. The need to quickly adapt DST models to dynamic real-world demands and generate dialogues in new domains."}
{"hash_id": 6912491152268037966, "entities": ["factchecking social media", "automated tools", "summarization methods", "diverse resources"], "background": "1. The manual process of fact-checking claims, particularly on social media, is extremely time-consuming and labor-intensive, leading to a need for automated tools to assist in this task. 2. Existing summarization methods are ineffective for fact-checking as they do not adequately extract evidence from diverse resources and modalities, which is crucial for accurate assessment of claims."}
{"hash_id": 5666136319077604127, "entities": ["multistep reasoning", "chainofthought prompting", "intricate relationships"], "background": "1. Large language models struggle with multi-step reasoning due to implicit relationships among multiple entities, which hinders their performance in complex scenarios. 2. Existing Chain-of-Thought prompting methods are not sufficient for handling reasoning tasks that involve numerous entities and intricate relationships."}
{"hash_id": 5629601798036159006, "entities": ["context length limitation", "multiturn user instructions", "conversational tasks"], "background": "1. The limited context length of Large Language Models (LLMs) which hinders their ability to effectively handle multi-turn user instructions. 2. The context-dependency issue of conversational tasks, where the history of interactions can be long and noisy, affecting the performance of LLM-powered agents."}
{"hash_id": 379560812314383244, "entities": ["lowresource minority languages", "accessibility", "cultural awareness in nlp", "multiplicity of writing systems"], "background": "1. The need to improve the accessibility and understanding of low-resource minority languages in China, which are currently underserved by large language models due to a lack of suitable pre-training data. 2. The recognition of the importance of cultural awareness in NLP, particularly in relation to the multiplicity of writing systems within languages, which has been long neglected in corpus construction efforts."}
{"hash_id": 3146936237780844623, "entities": ["decoderonly architecture", "simultaneous machine translation", "positional information handling"], "background": "1. The need to improve the efficiency and performance of Simultaneous Machine Translation (SiMT) by exploring the potential of the Decoder-only architecture, which has shown superior performance in various tasks and is inherently compatible with SiMT. 2. The challenge of high training and inference costs in existing SiMT models, which require efficient methods to handle positional information and to assess the sufficiency of source information for translation."}
{"hash_id": 2859006367225865092, "entities": ["jailbreaking attacks", "large language models", "helpfulness vs safety"], "background": "1. The lack of effective defense methods against jailbreaking attacks, which significantly impedes the safe deployment of large language models. 2. The need to understand the root cause of jailbreaks, specifically the intrinsic conflict between the goals of helpfulness and safety in LLMs."}
{"hash_id": 7072371388866098858, "entities": ["large language models", "metalinguistic selfreference", "dedicated dataset"], "background": "1. The need to understand and evaluate the capabilities of large language models in handling complex metalinguistic self-reference, which is integral to human communication and higher-order reasoning. 2. The absence of a dedicated dataset and evaluation framework for testing metalinguistic self-reference in language models, given its significance in various domains such as mathematics, computer science, and philosophy."}
{"hash_id": 6782783181544731698, "entities": ["improve credibility", "reduce hallucinations", "truthful responses"], "background": "1. The need to improve the credibility of Large Language Models (LLMs) by reducing the occurrence of hallucinations, which generate untruthful responses. 2. The desire to fully unlock the knowledge potential of LLMs by ensuring their responses are truthful and reliable."}
{"hash_id": 7944998169977676394, "entities": ["versatile model", "proteincentric tasks", "proteinlanguage tasks", "diverse pretraining", "unstructured scientific articles"], "background": "1. The need for a versatile model that can handle both protein-centric and protein-language tasks without task-specific architecture redesign. 2. The requirement to scale up pre-training by incorporating more diverse and less annotated data, such as unstructured scientific articles, to enhance the model's knowledge and generalizability."}
{"hash_id": 1126974905670897199, "entities": ["simulsst model", "cascaded errors", "speechtospeech translation", "translation moments"], "background": "1. The need for a direct Simul-S2ST model to reduce cascaded errors and improve joint optimization of speech recognition, translation, and speech synthesis. 2. The challenge of handling the diverse representations of speech, timbre, and intonation in direct speech-to-speech translation, as well as the need for an effective policy to determine translation moments in simultaneous scenarios."}
{"hash_id": 7381817654632184624, "entities": ["improve llms reliability", "multihop reasoning", "factual shortcuts", "systematic reasoning"], "background": "1. The need to improve the reliability of LLMs in multi-hop reasoning by eliminating factual shortcuts that can cause inconsistencies after knowledge editing. 2. The desire to understand and control the reasoning processes of LLMs to ensure they engage in systematic, step-by-step reasoning rather than relying on pre-trained associations."}
{"hash_id": 9182727904034281654, "entities": ["promptbased metrics", "fairness assessment", "bias mitigation"], "background": "1. The need to improve the reliability and agreement of fairness assessment using prompt-based metrics to ensure that biases are effectively quantified and mitigated. 2. To address the lack of correlation between existing prompt-based fairness metrics, which hinders the ability to measure consistent model traits and advancements in bias mitigation."}
{"hash_id": 6596768171096442159, "entities": ["hate speech detection", "english dialects", "majority world", "evaluation biases"], "background": "1. The need to develop hate speech detection systems that can generalize to English dialects spoken in the Majority World, particularly in low-resource contexts like Nigerian Twitter. 2. The requirement to address biases in evaluation datasets that overestimate model performance and do not reflect the real-world challenges of hate speech detection."}
{"hash_id": 1491252472242309444, "entities": ["undesirable biases", "instructionfollowing language models", "mitigate biases"], "background": "1. The presence of undesirable biases in instruction-following language models, which can be exacerbated in real-world usage and harm the model's ability to follow user instructions accurately. 2. The need for a practical and effective method to mitigate these biases without sacrificing the model's task performance or existing knowledge."}
{"hash_id": 6724847724877368367, "entities": ["handling subjective questions", "integrating multiple viewpoints", "data scarcity", "domain imbalance", "lessrepresented product domains"], "background": "1. The need to handle subjective questions about products that require integrating multiple viewpoints and facts, which traditional extractive methods fail to do effectively. 2. The challenge of data scarcity and domain imbalance, which hinders the performance of existing methods on lessrepresented product domains."}
{"hash_id": 479400309486269331, "entities": ["incontext learning", "demonstration selection", "model aspect", "performance variance"], "background": "1. The sensitivity of in-context learning (ICL) to the choice of demonstrations and the need to understand the factors contributing to performance variance from the model aspect. 2. The overlook of the model's impact on demonstration selection by previous methods, which primarily focused on the data aspect."}
{"hash_id": 4387258057140309908, "entities": ["table understanding methods", "text sequences", "visual information processing"], "background": "1. The limitations of previous table understanding methods that rely on converting tables into text sequences, which can be inaccessible in real-world scenarios. 2. The need for a more practical approach that can directly process and understand tables from visual information, given the widespread use and abundance of tabular data."}
{"hash_id": 556424669004265959, "entities": ["kbqa systems", "annotating data", "transfer learning approach"], "background": "1. The high cost and time consumption of annotating large volumes of labeled data for KBQA systems hinders their rapid and low-cost deployment in new domains. 2. The need for a more effective transfer learning approach that can leverage limited labeled data in target domains, along with the abundance of data in source domains, to improve the performance of KBQA systems."}
{"hash_id": 1320596406102177076, "entities": ["ai text attribution", "misinformation dissemination", "academic dishonesty", "watermarking techniques", "machinegenerated text quality"], "background": "1. The need to reliably attribute generated text to AI systems to address potential misuse, including misinformation dissemination and academic dishonesty. 2. The challenge of preserving the quality of machine-generated text while embedding watermarks, as traditional methods significantly impact the model's expressiveness and output quality."}
{"hash_id": 3244590691253627524, "entities": ["collaborative information representation", "textbased processing", "language model efficiency", "integration performance"], "background": "1. The need to represent collaborative information in a format that aligns with the text-based processing capabilities of large language models. 2. The desire to improve the efficiency of integrating collaborative information without compromising the performance of the language models."}
{"hash_id": 1920918188337437789, "entities": ["multimodal large language models", "selfawareness in perception", "trustworthy ai systems"], "background": "1. The need to improve the reliability of multimodal large language models by reducing hallucinations caused by a lack of self-awareness in perception. 2. The recognition that self-awareness in perception has been overlooked in previous studies, which hinders the development of trustworthy AI systems."}
{"hash_id": 627637713497515681, "entities": ["toxic cot problem", "large language models", "shallow attention layers"], "background": "1. The need to interpret and mitigate the Toxic CoT problem that causes large language models to produce incorrect answers during high-level commonsense reasoning. 2. The desire to improve the overall commonsense reasoning performance of models by addressing the information loss from the question observed in the shallow attention layers."}
{"hash_id": 6865921832060024438, "entities": ["multiaspect controllable text generation", "stereotypes", "semantic balance", "attribute correlations"], "background": "1. Existing methods for multi-aspect controllable text generation neglect the complex correlations between different attributes, leading to stereotypes and imbalanced attribute control. 2. The need to improve the semantic balance and multi-aspect control in generated texts, especially in scenarios with imbalanced attribute correlations."}
{"hash_id": 2044020916551184365, "entities": ["crossdocument relation extraction", "document embedding extraction", "relational evidence identification"], "background": "1. Existing relation extraction methods are not effective for cross-document scenarios where target entities may not coexist within the same document, and documents are significantly longer, posing challenges for document embedding extraction. 2. The lack of supervision for selecting relevant sentences in long documents necessitates a new approach that can learn to identify relational evidence without direct guidance."}
{"hash_id": 3561309965022173084, "entities": ["improving deafhearing communication", "sign language gloss translation", "semisupervised translation methods"], "background": "1. The need to improve communication between the deaf and hearing communities by enhancing the translation of spoken language text into sign language gloss, which is currently limited by the scarcity of parallel data. 2. The success of using monolingual data to improve translation quality in neural machine translation, which inspired the exploration of semi-supervised methods for SLG."}
{"hash_id": 7200395910706488315, "entities": ["gui limitations", "adaptable gui automation", "crossplatform approach"], "background": "1. The need to overcome the limitations of current GUI agents, which depend on structured text that can be inaccessible or inefficient, omitting important visual information. 2. The requirement for a more adaptable and universal approach to GUI automation that can work across various platforms without relying on specific structured data formats."}
{"hash_id": 7437324772888908416, "entities": ["evaluation benchmarks", "llms", "pretraining abilities"], "background": "1. The current evaluation benchmarks for LLMs mainly focus on post-fine-tuning capabilities, neglecting the assessment of fundamental abilities that emerge during the pre-training stage. 2. Subjective evaluations often rely on API models for scoring, which can lead to unstable results and limited ability to discern quality without references."}
{"hash_id": 6404111275296840418, "entities": ["fairness gaps", "annotator demographics", "inclusive ai systems"], "background": "1. The need to understand and mitigate the significant gaps in fairness preferences that exist depending on the demographic identities of annotators, which can skew AI systems. 2. The requirement to develop AI systems that are more inclusive and aligned with a diverse range of human values and preferences in content moderation."}
{"hash_id": 6397104230931490559, "entities": ["improving llms performance", "multistep mathematical reasoning", "manual annotation bottleneck"], "background": "1. The need to improve the performance of LLMs in complex multi-step mathematical reasoning tasks where current models face challenges. 2. The desire to overcome the bottleneck of heavy reliance on manual annotation for training process reward models, which hinders the advancement and practical application of these models."}
{"hash_id": 839853551307153923, "entities": ["fair evaluation", "unbiased metrics", "robust evaluation metrics"], "background": "1. The need to ensure fair and unbiased evaluation of large language models, as current methods can be easily manipulated, affecting the quality ranking of candidate responses. 2. The requirement for more robust evaluation metrics that align with human judgments to accurately assess the performance of large language models."}
{"hash_id": 4366497570106227777, "entities": ["logical reasoning capabilities", "event relation prediction", "llms coherence"], "background": "1. The need to improve the logical reasoning capabilities of LLMs in event relation prediction tasks to enhance their coherence and accuracy. 2. The recognition that current LLMs struggle with understanding and applying event relation logic, resulting in incorrect and inconsistent predictions."}
{"hash_id": 8382438523006808192, "entities": ["synchronized narrations", "engaging storytelling", "structured storyline", "product storytelling"], "background": "1. The lack of effective methods to generate synchronized narrations for ongoing visual scenes in videos, which is crucial for engaging storytelling in applications like marketing and education. 2. The need for a structured storyline to maintain coherence and integrity in the generated narrations, especially for longer videos and in the context of e-commerce where product storytelling is key."}
{"hash_id": 6013352609730373865, "entities": ["adaptive imagetext matching", "bidirectional generation model", "medical imaging tasks"], "background": "1. The need to adaptively extract and match image patches of varying sizes and positions to textual words in medical reports to accurately represent lesions and provide explicit explanations. 2. The requirement for a bidirectional generation model that can cyclically generate chest X-ray (CXR) images from reports and reports from CXR images, enhancing explainability and alignment in medical imaging tasks."}
{"hash_id": 5177527001302443132, "entities": ["tool utilization assessment", "finegrained evaluation", "external factors mitigation", "evaluation variance"], "background": "1. The need to assess the tool utilization capability of large language models in a more fine-grained and comprehensive manner, rather than relying on holistic evaluations that omit intermediate steps and can lead to missed insights. 2. The requirement to mitigate the impact of external factors, such as the instability of API services or temporal information shifts, which can lead to evaluation variance and unfair comparisons between models."}
{"hash_id": 2747371129474576829, "entities": ["enhancing llm evaluators", "nlg quality evaluation", "evaluation criteria confusion"], "background": "1. To enhance the reliability of LLM-based evaluators in NLG tasks by addressing their confusion between different evaluation criteria. 2. To provide a more nuanced and fine-grained analysis of how LLMs evaluate various aspects of NLG quality."}
{"hash_id": 3900427232981456570, "entities": ["improved information retrieval", "retrieval models", "large language models", "collaborative approach"], "background": "1. The need to improve the accuracy of information retrieval by combining the strengths of retrieval models, which can index a vast number of up-to-date documents, with the context-awareness and answer-generation capabilities of large language models. 2. The recognition that both retrieval models and large language models have limitations that can be mitigated by a collaborative approach, such as the hallucination of information by LLMs and the dependence on precise keywords in RMs."}
{"hash_id": 6687379175566316254, "entities": ["subquadratic architectures", "incontext learning", "traditional transformers", "efficient language model", "long text sequences"], "background": "1. The need to improve the in-context learning capabilities of subquadratic architectures, which currently do not match the performance of traditional Transformers. 2. The requirement for a more efficient language model that can handle long text sequences without suffering from impractical computational complexity."}
{"hash_id": 2299985280623478899, "entities": ["multimodal interactions", "integrating modalities", "model architecture modifications"], "background": "1. The need to enhance the capabilities of large language models (LLMs) to process and generate information across diverse modalities beyond text, to mirror real-world multimodal interactions. 2. The challenge of integrating multiple modalities (N \u2265 3) within a single framework without bidirectional alignment and the complexity of existing methods that require substantial modifications to the model architecture or training processes."}
{"hash_id": 8239146271227961054, "entities": ["multimodal sarcasm", "large multimodal models", "multimodal sarcasm target identification"], "background": "1. The need to improve the understanding and explanation of multimodal sarcasm, which is complex and often implicitly expressed, requiring a more nuanced approach than current methods provide. 2. The desire to leverage the extensive prior knowledge embedded within Large Multimodal Models (LMMs) to enhance both the accuracy and explainability of Multimodal Sarcasm Target Identification (MSTI)."}
{"hash_id": 582958845289585255, "entities": ["high cost of annotation", "humanannotated data", "aligning language models", "selfgenerating preference data"], "background": "1. The high cost and challenges of obtaining human-annotated preference data for aligning large language models with human expectations. 2. The need to improve upon existing methods that self-generate preference data, which can be noisy and variable in quality."}
{"hash_id": 2194972997424287316, "entities": ["text encoders", "latent representations", "finegrained visual features", "image quality", "textimage alignment"], "background": "1. The lack of understanding of how text encoders in text-to-image pipelines produce latent representations, which is crucial for image quality and text-image alignment. 2. The need for a method specifically designed to explore fine-grained visual features within text encoders, as existing methods for analyzing general language model internals are not suitable for this purpose."}
{"hash_id": 8883180638022810499, "entities": ["multiturn instruction", "existing studies", "training strategies", "complex queries", "multiturn interactions"], "background": "1. The lack of focus on multi-turn instruction following in existing studies and benchmarks, despite being a common real-world demand. 2. The challenges in collecting multi-turn instruction tuning data and the need for more effective training strategies to handle complex queries in multi-turn interactions."}
{"hash_id": 6626918858047768943, "entities": ["ast model", "singing datasets", "textnote annotation", "svs models"], "background": "1. The need for a more accurate and robust AST model that can automatically annotate singing datasets for SVS without requiring extensive manual refinement. 2. The demand for synchronized text-note annotation to improve the performance of SVS models and reduce the need for post-processing alignment."}
{"hash_id": 6038047955104042408, "entities": ["affected package identification", "smaller models", "structured information extraction", "vulnerability ecosystem"], "background": "1. Existing work on affected package identification fails to achieve high precision, partly due to the use of smaller models that lack knowledge and semantic capabilities. 2. There is a need to improve the efficiency and accuracy of extracting structured information from vulnerability reports to better defend the vulnerability ecosystem."}
{"hash_id": 7822490254593056781, "entities": ["readwrite policy", "credit assignment", "decision paths", "optimal policy", "translation quality"], "background": "1. The need to precisely optimize the read/write policy in SiMT without the credit assignment problem that hinders precise decision-making. 2. The requirement to explore all potential decision paths efficiently to ensure the discovery of the optimal policy while maintaining translation quality."}
{"hash_id": 5844113458821950815, "entities": ["zeroshot agents", "visionandlanguage navigation", "topological information"], "background": "1. To overcome the limitations of existing zero-shot agents in vision-and-language navigation, which lack a global understanding of the environment and rely solely on local observations for decision-making. 2. To bridge the gap between zero-shot agents and multimodal LLMs like GPT-4V by providing a more adaptable and informative prompting method that preserves topological information."}
{"hash_id": 8356501120541533280, "entities": ["backdoor attacks", "llmbased agents", "untrusted llms", "external tools"], "background": "1. The increasing use of LLM-based intelligent agents in various applications, which could be compromised by backdoor attacks, thus highlighting the need for understanding and addressing their vulnerabilities. 2. The potential risks associated with using untrusted LLMs or data in constructing agents, especially when they have permissions to use external tools, necessitating the development of new attack methods to expose and mitigate these risks."}
{"hash_id": 8280329625628316374, "entities": ["llmbased reasoning", "humanlike advanced reasoning", "adaptability limitations", "historical reasoning experiences"], "background": "1. To bridge the gap between LLM-based reasoning and human-like advanced reasoning skills, which current LLMs lack. 2. To overcome the limitations of existing approaches in adaptability, precision, and consideration of historical reasoning experiences in LLMs."}
{"hash_id": 561479591248915428, "entities": ["improve medical qa accuracy", "leverage implicit domain knowledge", "enhance smaller models"], "background": "1. The need to improve the accuracy of medical open-domain question answering without relying heavily on external knowledge retrieval, which can be noisy and incomplete. 2. The desire to leverage the implicit domain knowledge encoded in medical large language models to generate contexts that can enhance the performance of smaller, more resource-efficient models."}
{"hash_id": 2385004233708791480, "entities": ["content preservation", "semantic information", "style polarity", "reader engagement"], "background": "1. The need to improve content preservation in long text style transfer to avoid loss of semantic information. 2. The requirement for maintaining consistent style polarity across multiple sentences in long texts for better reader engagement."}
{"hash_id": 1600267533250882299, "entities": ["scaling transformer models", "longer sequences", "computational complexity", "llama", "effective solution"], "background": "1. The need to scale transformer-based language models to longer sequences without the quadratic increase in computational complexity. 2. The requirement for an effective solution that can be applied to large language models like LLaMA, which is not covered by existing approaches."}
{"hash_id": 8020161480906395048, "entities": ["improve translation accuracy", "lowresource languages", "unsupervised settings", "leverage large language models", "translation errors"], "background": "1. The need to improve translation accuracy for low-resource languages (LRLs) in unsupervised settings, where traditional supervised translation methods fail due to limited parallel data. 2. The potential to leverage large language models (LLMs) to mitigate translation errors caused by linguistically biased synthetic data and auxiliary language pairs."}
{"hash_id": 2358743691795802254, "entities": ["language imbalance", "reasoning ability", "translation methods"], "background": "1. The reasoning ability of large-scale models varies across languages, with dominant languages like English showing superior performance due to the imbalance in multilingual training data. 2. Existing methods that translate reasoning processes from the dominant language to non-dominant languages are limited by the expense and quality of annotation, and they do not effectively narrow the gap between dominant and non-dominant languages."}
{"hash_id": 6823654494275235154, "entities": ["improving retrievalaugmented language models", "retrieval noises", "dynamic training approach"], "background": "1. The need to improve the robustness of retrieval-augmented language models against various retrieval noises that can lead to performance degradation and misinformation. 2. The lack of a comprehensive classification of retrieval noises and the subsequent need for a training approach that can dynamically adjust to diverse noise types encountered in real-world retrieval environments."}
{"hash_id": 6634175067827440200, "entities": ["improve comparative reasoning", "text preference prediction", "reduce human annotation", "leverage llms without overhead"], "background": "1. The need to improve the consistency and accuracy of comparative reasoning in large language models for text preference prediction, as existing methods struggle with inconsistencies and fail to effectively distinguish between complex texts. 2. The goal of reducing the reliance on massive human annotation and computation resources, which are required for pretraining or fine-tuning models, by leveraging the language generation capabilities of LLMs without significant overhead."}
{"hash_id": 4038727668041054114, "entities": ["constructional semantics", "generative models", "noncompositional meanings"], "background": "1. The need to improve the capture of constructional semantics by pre-trained language models, particularly in generative models which have not been explored extensively for construction integration. 2. The observation that current language models struggle to fully grasp the noncompositional meanings of constructions, which are essential for natural language generation."}
{"hash_id": 589548465810223870, "entities": ["scarcity of audiovideo datasets", "multimodal speech technologies", "crosslinguistic communication barriers"], "background": "1. The need to overcome the scarcity of datasets that pair audio with corresponding video in the field of speech synthesis, which hinders the development of multimodal speech technologies. 2. The urgency to break down barriers to cross-linguistic communication, particularly in the context of increasing use of short videos and online meetings."}
{"hash_id": 6736850410017574462, "entities": ["improving llm efficiency", "resourceconstrained environments", "calibration data effects"], "background": "1. The need to improve the efficiency of large language models (LLMs) for deployment in resource-constrained environments by reducing computational demands while maintaining performance. 2. The lack of systematic investigation into how calibration data, crucial for post-training quantization and pruning, affects the performance of compressed LLMs."}
{"hash_id": 1156072746050647733, "entities": ["kgspecific training data", "semantic parsing methods", "fewshot lf generation methods", "complex reasoning"], "background": "1. The lack of KG-specific training data for real-world applications, which hinders the applicability of existing semantic parsing methods for KGQA. 2. The limitations of current few-shot LF generation methods that are constrained by the pre-trained knowledge of LLMs and struggle with complex reasoning and grounding in a KG."}
{"hash_id": 3270511637839237899, "entities": ["sentence embeddings", "large language models", "multiple tasks generalization"], "background": "1. The need to extract meaningful sentence embeddings from Large Language Models without the need for fine-tuning, given the potential of LLMs for various NLP tasks. 2. The requirement for embeddings that can generalize well across multiple tasks, rather than being task-specific and thus limited in their applicability."}
{"hash_id": 9153632373977073755, "entities": ["llms information consolidation", "sentiment summarization", "scientific metareviews"], "background": "1. The current uncertainty of Large Language Models' (LLMs) ability to truly consolidate information when generating summaries of opinionated documents, such as scientific meta-reviews. 2. The need for improved sentiment summarization in the scientific domain, specifically for meta-reviews which require a complex understanding of multi-document information and resolution of conflicts and consensus among reviewers."}
{"hash_id": 3367782954190537261, "entities": ["bilexical dependency graph", "internal structures", "long spans", "ssa datasets"], "background": "1. The need to overcome the limitations of previous bi-lexical dependency graph parsing methods that neglect the internal structures of spans, leading to reduced model expressiveness. 2. The recognition that long spans in SSA datasets exacerbate the issue of ignoring internal structures, and that a method is required to explicitly model these structures for improved performance."}
{"hash_id": 6961145732703163122, "entities": ["faster robust speech model", "multiple tasks", "opensource speech foundation model", "diverse languages and tasks", "open science"], "background": "1. The need for a faster and more robust speech model that can perform multiple tasks without the risks of hallucination associated with autoregressive models. 2. The desire to create an open-source and publicly accessible speech foundation model that can be scaled to diverse languages and tasks, promoting open science in the field."}
{"hash_id": 5913201657736793747, "entities": ["multihop reasoning", "parameterefficient models", "model editing"], "background": "1. Understanding whether LLMs can perform multi-hop reasoning is crucial for improving their efficiency and controllability, which could lead to more parameter-efficient and inferentially capable models. 2. The ability to perform latent multi-hop reasoning has significant implications for model editing, as it determines whether complex facts can be inferred or must be recalled, affecting the propagation of changes in knowledge."}
{"hash_id": 7277200965624620118, "entities": ["performance gap", "opensource models", "proprietary models", "math reasoning tasks", "data augmentation effectiveness"], "background": "1. To bridge the performance gap between open-source and proprietary large language models in math reasoning tasks. 2. To investigate the key factors affecting the effectiveness of data augmentation for mathematical reasoning tasks and the scaling relationship between the amount of data augmentation and model performance."}
{"hash_id": 5305595830982474476, "entities": ["automate informal argument analysis", "web discourse", "debate technologies", "argument structures", "implicit reasoning"], "background": "1. The need to automate the analysis of informal arguments in web discourse and public forums, which are often unclear and from inexperienced writers, making manual interpretation challenging. 2. The potential to improve various applications, such as debate technologies, policymaking, and legal decision-making, by making argument structures and implicit reasoning explicit."}
{"hash_id": 7000751108172088412, "entities": ["unified word alignment", "span prediction", "contextualized word embeddings"], "background": "1. The need for a unified word alignment approach that performs well across both high and low-resource languages, reducing the complexity of deploying different models for different language pairs. 2. The limitations of current span prediction and contextualized word embedding methods in handling discontinuous alignments, untranslated words, and one-to-multiple alignments."}
{"hash_id": 2788088788452145991, "entities": ["improve llms diversity", "subjective nlp tasks", "explanatory power persona variables"], "background": "1. The need to improve the ability of LLMs to simulate diverse human perspectives, which is crucial for subjective NLP tasks where annotations are highly subjective and inter-annotator agreement is low. 2. The desire to understand the actual explanatory power of persona variables on the variance in human annotations, which has not been thoroughly investigated in existing studies."}
{"hash_id": 6248366435872695223, "entities": ["mcqa evaluations", "intended abilities", "llms decisionmaking", "partialinput settings"], "background": "1. To ensure that MCQA evaluations truly reflect the intended abilities of LLMs, such as comprehension, knowledge use, and reasoning, rather than relying on dataset artifacts. 2. To investigate and provide insights into the decision-making processes of LLMs in partial-input settings, which can lead to more transparent and robust benchmarking."}
{"hash_id": 9047394985980697481, "entities": ["fact verification", "smaller language models", "misinformation", "efficient factchecking"], "background": "1. The need to improve the performance of fact verification in the face of misinformation, especially when using smaller language models or dealing with unreliable context. 2. The requirement for a more efficient and less manual approach to fact-checking to keep up with the vast volume of information spreading online."}
{"hash_id": 1333506349028492435, "entities": ["automated answering", "logisticsrelated questions", "ai teaching assistants", "dataset privacy concerns"], "background": "1. To reduce the workload of human instructors by automating the answering of logistics-related questions, which are repetitive and important to students. 2. To address the lack of publicly available datasets for training and evaluating AI teaching assistants due to privacy concerns."}
{"hash_id": 5975580254612136379, "entities": ["inflexibility of llms", "catastrophic forgetting", "hallucinations in llm outputs", "transparency of reasoning"], "background": "1. To improve the inflexibility of LLMs by incorporating new knowledge without the need for costly fine-tuning and to prevent catastrophic forgetting. 2. To reduce hallucinations in LLM outputs, particularly for high-stake applications, and to enhance the transparency of the reasoning process."}
{"hash_id": 5513237388466353298, "entities": ["evaluation methods", "llms", "distributional biases"], "background": "1. The current evaluation methods for LLMs often overlook potential biases introduced by the distributional assumptions of benchmarks, which can lead to inaccurate model rankings and performance assessments. 2. The assumption that prompts within a benchmark are randomly sampled from a real-world distribution is generally not valid, as the distribution of interest can vary depending on the specific use case."}
{"hash_id": 3196350476570856485, "entities": ["large language models", "compositional tasks", "recursive nature"], "background": "1. Large language models do not effectively generalize to compositional tasks, which require solving smaller instances of the same problem. 2. Existing approaches have not explicitly captured the recursive nature of compositional tasks, which is essential for improved performance."}
{"hash_id": 6073829271849695071, "entities": ["preference gap", "retrievers", "llms", "rag systems", "ranking and selection processes"], "background": "1. The need to bridge the preference gap between retrievers, which are human-friendly, and LLMs, which have different preferences, to enhance the performance of RAG systems. 2. The challenge of aligning the ranking and selection processes of retrievers with the preferences of LLMs without the need for expensive finetuning of LLMs or adjustability of production-level retrievers."}
{"hash_id": 6989743112234125652, "entities": ["temporal reasoning", "large language models", "temporal graphs"], "background": "1. Large language models (LLMs) have shown limitations in temporal reasoning due to the complexity of temporal concepts and logic, which hinders their application in solving real-world problems. 2. Existing methods to improve LLMs' temporal reasoning either ignore or do not explicitly involve the intrinsic nature of temporal reasoning, such as the use of temporal graphs as a latent representation."}
{"hash_id": 2835063622069585568, "entities": ["table questionanswering", "generalization", "semantic parsingbased methods"], "background": "1. Current Table Question-Answering methods often struggle with generalization and structural reasoning, especially when dealing with complex queries and heterogeneous table data. 2. There is a need to bridge the gap between semantic parsing-based methods and direct answer generation approaches to leverage their strengths and overcome their limitations."}
{"hash_id": 5819436804037378106, "entities": ["conversational tones", "humancomputer interactions", "experimenter bias"], "background": "1. The need to understand and compare conversational tones between humans and LLMs to ensure effective communication and alignment in human-computer interactions. 2. The existence of experimenter bias and the lack of representativeness in existing taxonomies and text corpora for studying conversational modalities."}
{"hash_id": 3739897387378151919, "entities": ["quantitative error estimation", "llm performance", "missioncritical tasks"], "background": "1. The lack of a systematic way to efficiently quantify the likelihood of errors in LLM outputs, which is crucial for applications requiring precision and dependability, such as biomedical and healthcare fields. 2. The need to improve the performance of LLMs in mission-critical tasks by providing a quantitative error estimation that can be used to enhance the reliability of the responses."}
{"hash_id": 3603283368681250955, "entities": ["large language models", "simultaneous translation", "simulmt challenges"], "background": "1. The lack of research on applying large language models to the challenging task of simultaneous translation, despite their success in traditional neural machine translation. 2. The need to overcome the specific challenges faced by LLMs in adapting to SimulMT, such as the dynamic changes in the input prompt and the availability of only partial source context during translation."}
{"hash_id": 7116842143574069523, "entities": ["alignmentbreaking attacks", "defense strategy", "large language models"], "background": "1. The need to defend against alignment-breaking attacks that can bypass existing safety mechanisms in Large Language Models, potentially leading to the generation of harmful or malicious content. 2. The requirement for an effective defense strategy that does not rely on external detectors, avoids the need for expensive retraining, and can be universally applied to various types of alignment."}
{"hash_id": 1450566421518433448, "entities": ["large language models", "knowledge base question answering", "complex queries"], "background": "1. The need to leverage the reasoning capabilities of large language models (LLMs) in a more effective manner for knowledge base question answering, especially in low-resource scenarios where annotated data is scarce. 2. The requirement to handle complex queries that current KBQA systems struggle with, and to improve the interpretability and transparency of the reasoning process."}
{"hash_id": 1342016347206176477, "entities": ["low accuracy in tool use", "llms", "exploration and learning"], "background": "1. Existing LLMs have low accuracy in using tools, which is problematic for practical deployment where high accuracy is necessary for consequential actions. 2. There is a need to improve the depth and breadth of exploration and learning for LLMs to master tool use, inspired by successful cognitive functions in biological systems."}
{"hash_id": 3467331939670961214, "entities": ["improving moe performance", "leveraging unselected experts", "computational efficiency", "multitask learning knowledge transfer"], "background": "1. The need to improve the performance of MoE models by increasing the availability of expert knowledge without sacrificing the sparsity of expert selection, which is crucial for computational efficiency. 2. The desire to leverage the potential of unselected experts by transferring their knowledge to enhance the capabilities of the selected experts, similar to how multi-task learning transfers knowledge among tasks."}
{"hash_id": 9186066752610247416, "entities": ["align large language models", "broad spectrum preferences", "reinforcement learning methods", "instability rlhf methods"], "background": "1. The need to align large language models with a broad spectrum of human preferences without the use of complex and computationally expensive reinforcement learning methods. 2. The desire to overcome the instability and sensitivity to hyperparameters associated with existing reinforcement learning from human feedback (RLHF) methods."}
{"hash_id": 4586017322893776669, "entities": ["multimodal large language models", "contextdependent comprehension", "visual information disambiguation", "lack of benchmarks"], "background": "1. The need to comprehensively evaluate the capabilities of multimodal large language models in context-dependent visual comprehension, as this is essential for both scientific understanding and practical real-world applications. 2. The current lack of benchmarks that effectively assess how well MLLMs can use context to disambiguate visual information, given that human understanding is significantly influenced by context."}
{"hash_id": 7051528677399625248, "entities": ["data annotation", "analogical reasoning", "limited labeled data"], "background": "1. The need to reduce the high human effort required for data annotation, especially in scenarios with limited labeled data where annotation models are prone to making incorrect suggestions. 2. The inspiration from cognitive studies on efficient learning, which indicate that analogical reasoning can facilitate learning from few examples and improve the accuracy of suggestions in the annotation process."}
{"hash_id": 5682533030114404482, "entities": ["textualwsd datasets", "visualwsd datasets", "multimodal representations", "wsd performance"], "background": "1. The lack of images in Textual-WSD datasets and the lack of senses in Visual-WSD datasets, which hinders the joint processing and unification of these two subtasks. 2. The potential of multimodal representations to carry richer semantic information compared to unimodal representations, which could improve WSD performance if effectively utilized."}
{"hash_id": 8187123846906026557, "entities": ["key point analysis", "overlapping opinions", "hallucinated opinions", "annotated data dependency"], "background": "1. The need to overcome the limitations of existing Key Point Analysis (KPA) methods that generate key points with overlapping and hallucinated opinions, leading to inaccurate quantification. 2. The requirement to reduce the dependency on large amounts of annotated data for supervised training in KPA studies."}
{"hash_id": 5971890214097692099, "entities": ["large language models", "ambiguous user queries", "user trust satisfaction", "comprehensive taxonomy benchmark"], "background": "1. The limited practical utility of current large language models (LLMs) in handling ambiguous user queries, which risks user trust and satisfaction. 2. The need for a comprehensive taxonomy and benchmark to evaluate LLMs' capacity to identify and clarify various types of ambiguities in user queries."}
{"hash_id": 3849845672960355783, "entities": ["hallucinations reduction", "multimodal reasoning", "textual knowledge graphs"], "background": "1. The need to reduce hallucinations and improve the quality of knowledge in large language models for multimodal reasoning. 2. The limitation of existing approaches that use textual knowledge graphs, which do not provide comprehensive cross-modal understanding."}
{"hash_id": 7132935065498248098, "entities": ["confidence estimation", "rulebased tkgf", "temporal dynamics"], "background": "1. The need to improve the accuracy of confidence estimation in rule-based TKGF, which is currently hindered by heuristic and inaccurate methods. 2. The requirement to account for the temporal dynamics of rules, as existing methods often assume static or uniform temporal patterns, leading to temporally insensitive outcomes."}
{"hash_id": 6371672696767824290, "entities": ["improving language model reliability", "numerical reasoning tasks", "data scarcity issue"], "background": "1. The need to improve the reliability of reasoning processes generated by large language models in numerical reasoning tasks. 2. The requirement to address the data scarcity issue that arises when generating a single reasoning process for each answer formula."}
{"hash_id": 1566934898326136008, "entities": ["manual procedural graph construction", "automatic graph extraction", "large language models"], "background": "1. The high cost and complexity of manually constructing procedural graphs from documents, which hinders users from easily understanding complex procedures. 2. The potential of large language models to improve the task of automatic procedural graph extraction, which has not been fully explored."}
{"hash_id": 3804291347951792684, "entities": ["aitext detection", "robustness improvement", "comprehensive evaluation"], "background": "1. The need to improve the robustness of AI-text detection systems against perturbations, as current systems struggle with misclassifications in real-world scenarios. 2. The absence of comprehensive evaluations of AI-text detection performance across various writing contexts and perturbation techniques."}
{"hash_id": 6020769584551939248, "entities": ["hallucinations reduction", "large language models", "retrievalaugmented generation datasets"], "background": "1. The need to reduce the occurrence of hallucinations in large language models, which undermine their reliability in real-world applications. 2. The lack of high-quality, large-scale datasets specifically designed for hallucination detection in the context of retrieval-augmented generation."}
{"hash_id": 5158769875022299371, "entities": ["trustworthiness of llms", "hallucinations detection", "empirical study"], "background": "1. The need to enhance the trustworthiness and reliability of large language models in real-world applications by reducing their tendency to generate factually incorrect content (hallucinations). 2. The lack of a comprehensive and systematic empirical study that addresses the three key questions of detection, source, and mitigation of hallucinations in LLMs."}
{"hash_id": 6999417856223196305, "entities": ["knowledge distillation", "autoregressive language models", "teaching strategies"], "background": "1. The need to improve the performance of knowledge distillation in autoregressive language models, especially when using larger teacher models, which can lead to poorer student model performance. 2. The recognition that different tokens require different teaching strategies, which is not currently considered in standard knowledge distillation approaches."}
{"hash_id": 2443090322362717258, "entities": ["catastrophic forgetting", "neural machine translation", "domain adaptation"], "background": "1. The need to prevent catastrophic forgetting in neural machine translation when incrementally adding new domains without access to previous training data or the need for full retraining. 2. The requirement for a more efficient and effective approach that utilizes limited resources, such as unlabeled data, while maintaining distinct domain characteristics in the model."}
{"hash_id": 3600651549752795723, "entities": ["exploit large language models", "voice generation", "lowresource languages", "zeroshot generalization", "scalable voice system"], "background": "1. The need to exploit the advantages of large language models for voice generation in low-resource languages and zero-shot task generalization. 2. The desire to create a scalable and versatile voice generation system that can handle multiple tasks and languages, as opposed to the current single-task or monolingual models."}
{"hash_id": 6462118339345861653, "entities": ["language capabilities", "opensource llms", "nonenglish languages"], "background": "1. The limited language capabilities of open-source LLMs, which are primarily focused on English, restrict their application in other languages. 2. The computational expense and complexity of creating and aligning LLMs from scratch for non-English languages."}
{"hash_id": 4483129461101602764, "entities": ["guard models", "jailbreak attacks", "adaptive attack strategy", "defense mechanisms"], "background": "1. The need to identify and address vulnerabilities in Guard Models that are meant to protect Large Language Models (LLMs) from generating harmful content, as current methods are not foolproof against jailbreak attacks. 2. The requirement for a more adaptive and systematic attack strategy that can evaluate the effectiveness of Guard Models and expose potential weaknesses in the defense mechanisms of LLMs."}
{"hash_id": 5903336309030086219, "entities": ["human effort reduction", "noisy data annotation", "large language models", "denoising sensitivity"], "background": "1. The need to reduce human effort in annotating noisy data and improve the accuracy of learning from noisy labels. 2. The potential of large language models to assist in denoising without human intervention, but their sensitivity to the quality of input prompts."}
{"hash_id": 2676996842744397253, "entities": ["logical reasoning capabilities", "complex counterfactual reasoning", "internal inconsistencies"], "background": "1. The need to quantify and evaluate the logical reasoning capabilities of large language models, particularly in the context of complex counterfactual reasoning. 2. The observation of internal inconsistencies in the reasoning processes of LLMs, which are attributed to the misunderstanding and misapplication of logical relations."}
{"hash_id": 8854230449120269465, "entities": ["question answering", "diverse data sources", "modal transformations", "hybrid reasoning scenarios"], "background": "1. The challenges of reasoning over diverse sources and modalities of data in question answering due to the large scale of information and the complex interplay between different data types. 2. The need to surpass the limitations of existing approaches that either train specialized retrievers or perform modal transformations, which may not be scalable or adaptable to various hybrid reasoning scenarios."}
{"hash_id": 5756949656696426782, "entities": ["temporal knowledge graph", "completion methods", "geometric operation", "expressive capabilities", "embedding models"], "background": "1. Existing temporal knowledge graph completion methods have limitations in capturing complex temporal dynamics and relation patterns due to their reliance on a single geometric operation. 2. The expressive capabilities of current temporal knowledge graph embedding models need to be improved to better predict and estimate missing facts in temporal knowledge graphs."}
{"hash_id": 5090022671614462629, "entities": ["alignment strategies", "intrinsic uncertainty", "model degradation", "supervised finetuning sft", "catastrophic forgetting"], "background": "1. Existing alignment strategies overlook the intrinsic uncertainty of tasks, leading to suboptimal data efficiency and model performance. 2. The need to mitigate model degradation and catastrophic forgetting issues that can occur during supervised fine-tuning (SFT)."}
{"hash_id": 2789359022727334396, "entities": ["accurate classification", "interpretable insights", "conversational systems", "existing approach limitations", "diverse conversational patterns"], "background": "1. The need for both accurate classification and interpretable insights into user satisfaction to improve conversational systems. 2. The limitations of existing approaches in generalizing across diverse conversational patterns and providing interpretable rubrics for understanding user satisfaction."}
{"hash_id": 359379733489558919, "entities": ["fundamental capabilities llms", "benchmarkbased evaluation", "realworld applications"], "background": "1. The lack of clarity on which fundamental capabilities of LLMs contribute to success in specific domains, which hinders their effective application. 2. The existing benchmark-based evaluation methods do not adequately reflect the performance of LLMs in real-world applications."}
{"hash_id": 8801338922690418231, "entities": ["political biases", "large language models", "polarization harms"], "background": "1. The need to provide transparency to users about the political biases present in large language models, which can lead to polarization and other harms in downstream applications. 2. The absence of detailed and nuanced measures to capture the complex dynamics of political bias in AI-generated content beyond traditional political-orientation level analyses."}
{"hash_id": 796496400755271406, "entities": ["context awareness", "large language models", "attention allocation"], "background": "1. The need to improve the context awareness of large language models, especially when used for tasks that require a high degree of understanding, such as tool-use, to prevent the model from overlooking crucial information. 2. The observation of an inconsistent attention allocation pattern in LLMs that leads to performance fluctuations based on the position of information within the context."}
{"hash_id": 1680755515499618685, "entities": ["memory consumption reduction", "highthroughput language models", "inference throughput optimization"], "background": "1. The need to reduce memory consumption to enable efficient deployment of high-throughput large language models in real-world applications. 2. The desire to improve inference throughput while maintaining competitive performance in language modeling and downstream tasks."}
{"hash_id": 4126572080228069657, "entities": ["performance gap", "resourcerich languages", "resourcepoor languages", "imbalanced training data", "supervised finetuning"], "background": "1. Large language models (LLMs) have a performance gap between resource-rich and resource-poor languages due to imbalanced training data distribution, which needs to be bridged to enhance their multilingual capabilities. 2. Traditional methods of translating and then supervised fine-tuning (SFT) are limited by the quality of translations and can sometimes degrade the model's performance in the original resource-rich language."}
{"hash_id": 910236945551432796, "entities": ["chinese commonsense reasoning", "llms performance", "memorizationreasoning correlation"], "background": "1. The need for a comprehensive benchmark to evaluate the commonsense reasoning ability of LLMs in Chinese, considering unique cultural and linguistic aspects that are overlooked in existing English-based benchmarks. 2. The desire to understand the correlation between memorization and reasoning abilities in LLMs to enhance their performance and provide clear guidance for optimization."}
{"hash_id": 1754428148195813087, "entities": ["multimodal large language models", "modality isolation", "multiple image understanding", "singleimage methods", "imagetext isolation"], "background": "1. The current Multimodal Large Language Models (MLLMs) lack awareness of other images and multimodal instructions due to prior-LLM modality isolation, which hinders their ability to deeply understand context involving multiple images. 2. Existing methods effective in single-image scenarios do not address the concurrent fusion of multiple images, leading to issues like image-text isolation and inter-image isolation."}
{"hash_id": 4506465708364023844, "entities": ["multimodal language models", "joint training", "paired multimodal data", "adaptable method", "efficient mllm creation"], "background": "1. The current approach to training MLLMs requires extensive joint training with paired multimodal data, which is resource-intensive and difficult to extend to new modalities. 2. There is a need for a more adaptable and efficient method that can create versatile MLLMs without the need for additional training when incorporating new modalities."}
{"hash_id": 512329199478437739, "entities": ["transformerbased llms", "autoregressive decoding", "costeffective solution", "plugandplay", "latencysensitive applications"], "background": "1. The high inference costs and latency in Transformer-based LLMs, particularly during autoregressive decoding, which is inefficient and leads to long processing times. 2. The need for a cost-effective and plug-and-play solution that does not require additional model training or increase in memory footprint, especially for latency-sensitive applications."}
{"hash_id": 7075918522065933591, "entities": ["multimodal machine translation", "semantic gap", "visual information exploitation", "textual input limitations", "outofdomain instances"], "background": "1. The first motivation is to address the limitation of existing multimodal machine translation methods that do not fully exploit the benefits of both complete and limited textual inputs, which hinders overall translation performance. 2. The second motivation is to leverage visual information more effectively to bridge the semantic gap between images and texts, particularly in challenging scenarios with out-of-domain instances and ambiguous verbs."}
{"hash_id": 5934619902050583220, "entities": ["meaning composition", "computational metric", "large language models"], "background": "1. The lack of a suitable computational metric to quantify the extent of meaning composition, which complicates quantitative analyses of this process in the human brain. 2. The recent advancements in Large Language Models (LLMs) that have shown correlations between their internal states and human behavioral and neural data, suggesting the potential for developing a metric to understand meaning composition."}
{"hash_id": 5259183192656691031, "entities": ["short text clustering", "information drownout", "sequence representation", "semantic robustness"], "background": "1. The need to overcome the information drown-out in short text clustering where key information is obscured by noise due to the summarization of all local tokens into a sequence representation. 2. The requirement for a more semantically representative and robust sequence representation that can handle the varying signal-to-noise ratios in short text samples."}
{"hash_id": 4914991526838057877, "entities": ["computational demands", "emotional support chatbots", "resourceefficient models"], "background": "1. The need to reduce the computational demands and costs associated with deploying large language models for emotional support chatbots at scale. 2. The desire to enhance the emotional support response capabilities of smaller, more resource-efficient models without sacrificing performance."}
{"hash_id": 825908343521013489, "entities": ["conversational abilities", "computational efficiency", "posttraining quantization"], "background": "1. The need to maintain conversational abilities in large language models while improving their computational efficiency through quantization. 2. The desire to overcome the performance degradation caused by token-flipping and other issues that arise from post-training quantization."}
{"hash_id": 7468604292964274182, "entities": ["improving llm performance", "specialized domains", "expert llm diversity"], "background": "1. The need to improve the performance and reliability of general LLMs in specialized domains where they currently fall short. 2. The practical challenge of users having to switch between various expert LLMs, given the diversity and subtle differences in their capabilities."}
{"hash_id": 6639134690719874394, "entities": ["improved text faithfulness", "informationseeking scenarios", "accurate citations"], "background": "1. The need to improve the faithfulness, grounding, and controllability of generated text in information-seeking scenarios, where supporting evidence is crucial. 2. The requirement for more accurate citations and factually reliable output in long-form responses, given the inconsistency and frequent inaccuracies found in existing model performances."}
{"hash_id": 2489698759299069125, "entities": ["alignment of large language models", "exploration onpolicy learning", "parameter efficiency"], "background": "1. The need to simplify the alignment of large language models with human preferences by eliminating the requirement for a separate reward model, which can complicate the training process and introduce additional parameters. 2. The desire to improve the exploration capabilities of on-policy learning while maintaining parameter efficiency, as offline learning can lead to suboptimal results and off-policy learning can cause degeneration without an appropriate strategy."}
{"hash_id": 3169079669400129009, "entities": ["language gap", "linguistic inequality", "instructionfollowing datasets", "multilingual settings"], "background": "1. To bridge the language gap and reduce linguistic inequality in AI model performance by providing instruction-following datasets in a diverse range of languages. 2. To enhance the ability of AI models to follow instructions in multilingual settings by using human-curated data that better represents the nuances and variations of different languages."}
{"hash_id": 5684096223904392333, "entities": ["zeroshot performance", "novel task adaptation", "biological neural transfer learning"], "background": "1. The challenge of adapting large language models to novel tasks without in-context examples, especially when smaller models lack zero-shot performance and larger models are computationally demanding. 2. The potential to mimic biological neural abilities to transfer learning from one task to another, supported by the mechanistic interpretation of the Transformer architecture."}
{"hash_id": 3623121524321247462, "entities": ["simplifying complex texts", "natural language processing models", "sprp task"], "background": "1. The need to facilitate the processing of complex texts for both humans and machines by simplifying sentences without altering the original meaning. 2. The requirement for a better evaluation of natural language processing models through a task that involves complex grammatical aspects, such as the SPRP task."}
{"hash_id": 1792702113359466487, "entities": ["inference cost optimization", "selfattention module", "multitenant llm serving", "memory utilization"], "background": "1. The need to optimize the inference cost of large language models, particularly the self-attention module, which is memory-bound and becomes less efficient with increasing context lengths. 2. The opportunity to leverage the shared system prompts in multi-tenant LLM serving scenarios to reduce redundancy in key/value tensors and improve memory utilization."}
{"hash_id": 6682592641041175557, "entities": ["realscenario evaluations", "aligned chinese llms", "user intentions assessment"], "background": "1. The lack of real-scenario grounded, open-ended, challenging, and automatic evaluations tailored for alignment of emerging Chinese LLMs. 2. The need for a comprehensive and reliable benchmark to assess aligned LLMs' fulfillment of user intentions and preferences in real-world conditions and to differentiate between aligned and base LLMs."}
{"hash_id": 2438341259255758685, "entities": ["catastrophic forgetting", "knowledge transfer", "continual learning", "parameterefficient tuning"], "background": "1. The need to effectively handle catastrophic forgetting and knowledge transfer challenges simultaneously in continual learning for large language models. 2. The potential to improve the alignment and interaction between the learning and selection modules to enhance the efficiency and effectiveness of parameter-efficient tuning."}
{"hash_id": 3491217231903973200, "entities": ["parameterefficient finetuning", "lowrank adaptation", "adaptive strategy allocation"], "background": "1. The high computational cost and inefficiency of existing parameter-efficient fine-tuning methods like Low-Rank Adaptation (LoRA), which uniformly distribute parameters across all weight matrices without considering their differential importance. 2. The need for a more nuanced and adaptive strategy to allocate parameter budgets based on the varying contributions of different components to the fine-tuning performance."}
{"hash_id": 8217891484807787129, "entities": ["crosslingual knowledge editing", "large language models", "multilingual settings"], "background": "1. The need to adapt large language models to new knowledge without retraining them from scratch, especially in multi-lingual settings where models are used across different languages. 2. The absence of research on the cross-lingual effect of knowledge editing, which is crucial for understanding how edits in one language affect a model's performance in other languages."}
{"hash_id": 8051654183974844857, "entities": ["argument mining", "manually annotated data", "crosslingual transfer"], "background": "1. The limited availability of manually annotated data for argument mining in multiple languages hinders the development of robust NLP models. 2. Existing methods for cross-lingual transfer and few-shot learning may not be directly applicable to the task of argument mining due to the complexity and length of discourse structures involved."}
{"hash_id": 7585586663691023701, "entities": ["semantic information", "useritem pairs", "personalized explanations", "rating predictions"], "background": "1. The need to improve the semantic information in user and item embeddings to enhance the generalization of rating prediction models for unseen user-item pairs. 2. The requirement for more personalized and suitable explanations to accompany rating predictions to increase user engagement and satisfaction."}
{"hash_id": 4458036739783491759, "entities": ["intext questions", "human reading", "active thinking"], "background": "1. The need to understand the linguistic role and impact of in-text questions on human reading, which has been understudied. 2. The challenge of asking good questions that facilitate active thinking and deep engagement with the text during reading."}
{"hash_id": 994093063447604477, "entities": ["multimodal hallucinations", "lvlms", "detailed training data"], "background": "1. The paper aims to tackle the critical issue of multimodal hallucinations in LVLMs, which occur when models generate text that does not correspond to the visual input, compromising the reliability of the model's applications. 2. The methods are proposed to address the overlooked factor of excessively detailed training data, which can lead models to continue generation beyond their visual perception limits, causing hallucinations."}
{"hash_id": 6611147287743864656, "entities": ["selfconsistency methods", "freeform generation", "sample selection mechanisms"], "background": "1. Existing self-consistency methods struggle with free-form generation due to the difficulty of aggregating answers and overlook the fine-grained consensus knowledge within multiple candidate samples. 2. Current sample selection or voting mechanisms are suboptimal for free-form generation tasks, as they cannot fully utilize locally valuable information from unselected samples and cannot eliminate low-quality segments from selected samples."}
{"hash_id": 449531309286392068, "entities": ["organization of valency", "lexicon", "zipfs meaningfrequency law"], "background": "1. To investigate the organization of valency in the lexicon, which has been largely unaddressed in the literature on efficient language use, despite its potential impact on communication. 2. To test the hypothesis that more frequent verbs are associated with a greater diversity of valency frames, which is related to Zipf's meaning-frequency law and the principle of least effort."}
{"hash_id": 7563301499286533896, "entities": ["systematic assessment", "rpcas", "highquality dataset", "evaluating rpcas"], "background": "1. The need to systematically assess and compare the capabilities of RPCAs, which vary significantly in approach and objectives. 2. The lack of a high-quality dataset for evaluating RPCAs, with existing datasets being either generated by LLMs or suffering from significant noise."}
{"hash_id": 5009581318029130727, "entities": ["memorization capabilities", "generative language models", "crossmodal retrieval"], "background": "1. The need to leverage the memorization capabilities of generative language models for visual content retrieval. 2. The desire to create a new paradigm for cross-modal retrieval that is distinct from traditional discriminative approaches."}
{"hash_id": 7256850256764120876, "entities": ["scarcity labeled data", "aspect sentiment quad prediction", "data augmentation mismatches"], "background": "1. The scarcity of labeled data significantly limits the performance of existing methods in Aspect Sentiment Quad Prediction (ASQP). 2. Synthetic samples generated by existing data augmentation methods often contain mismatches between sentences and labels, which negatively impact model learning."}
{"hash_id": 4631428421769734249, "entities": ["improving llm reliability", "reducing factual hallucinations", "accurate citation production"], "background": "1. The need to improve the reliability and trustworthiness of Large Language Models (LLMs) in mission-critical situations by reducing factual hallucinations. 2. The challenge of accurately producing citations in responses to allow for verification of the information provided by LLMs."}
{"hash_id": 5133643490797551592, "entities": ["text embeddings", "multistage training", "large language models"], "background": "1. To simplify the process of obtaining high-quality text embeddings by reducing the need for complex multi-stage training pipelines and manually collected datasets. 2. To improve the performance and multilinguality of text embeddings by leveraging the power of large language models and synthetic data generation."}
{"hash_id": 3309682142972361138, "entities": ["knowledge distillation", "smallscale language models", "selftraining process"], "background": "1. The high cost and instability of knowledge distillation from large, proprietary language models make it challenging to enhance the reasoning abilities of small-scale language models. 2. There is a need to improve the self-training process for small-scale LMs, as their performance heavily depends on the pre-existing abilities of the models."}
{"hash_id": 5490418976790262102, "entities": [], "background": "1. The need to enhance the language-specific knowledge and cultural diversity in multilingual large language models. 2. The inefficiency and potential distortion caused by translating English supervised fine-tuning data directly into multiple languages."}
{"hash_id": 4541123329823192864, "entities": ["claim selection", "factchecking efficiency", "multisentence document analysis"], "background": "1. The overwhelming volume of claims requires efficient selection of check-worthy claims to maximize the impact of fact-checkers. 2. Existing claim extraction methods are primarily sentence-level and do not adequately address the complexities of extracting claims from multi-sentence documents, leading to inefficiency and potential ambiguity."}
{"hash_id": 5892060076110406655, "entities": ["overfitting mitigation", "counterfactually augmented data", "feature leverage", "robustness enhancement"], "background": "1. The need to mitigate overfitting on spurious correlations in counterfactually augmented data, which can lead to poor generalization on out-of-distribution datasets. 2. The requirement for a method that encourages models to leverage a broader range of features beyond those that have been modified, to enhance robustness and understanding of nuanced contexts."}
{"hash_id": 1337490075057807686, "entities": ["dependency on labeled data", "documentlevel event argument extraction", "incontext learning performance"], "background": "1. The need to alleviate the dependency on large-scale labeled data for document-level event argument extraction tasks. 2. The challenge of improving in-context learning performance, particularly for non-reasoning tasks, by overcoming issues such as example selection, context length limitations, and handling unseen event types."}
{"hash_id": 3924034215854309268, "entities": ["multimodal hallucinations", "lvlms", "incorrect responses", "model performance"], "background": "1. The concern that multimodal hallucinations generated by LVLMs can influence subsequent generations and lead to incorrect responses, even when ground visual information is present. 2. The need to improve the performance of LVLMs in the face of hallucinations, as current models are prone to accept these hallucinations and make false claims."}
{"hash_id": 7207858095109233581, "entities": ["improve reasoning consistency", "multilingual language models", "lesserresourced languages"], "background": "1. The need to improve the reasoning consistency of large language models across multiple languages, particularly for lesser-resourced languages. 2. The lack of research on multilingual reasoning capabilities of LLMs and the potential disparities in their performance across different languages."}
{"hash_id": 2621015887464251229, "entities": ["public perceptions", "gun control", "stance detection"], "background": "1. The need to understand public perceptions and opinions on controversial topics like gun control and regulation, especially in the wake of mass shooting events, to inform better strategies for addressing these concerns. 2. The lack of datasets and efficient methods for stance detection specifically tailored to the gun control and regulation context on social media."}
{"hash_id": 2539523268657821791, "entities": ["benchmark saturation", "training data contamination", "referencefree evaluation metrics", "semantic accuracy"], "background": "1. The need to assess open LLMs on data-to-text generation tasks using novel, real-world structured data to avoid benchmark saturation and training data contamination. 2. The requirement for reference-free evaluation metrics to better correlate with human judgment and to ensure the semantic accuracy of the generated text."}
{"hash_id": 2363095398438736981, "entities": ["fairness and trustworthiness", "implicit hate speech detection", "calibration performance"], "background": "1. The need to ensure fairness and trustworthiness in LLMs, particularly in the context of implicit hate speech detection, which is a significant challenge not adequately addressed by current models. 2. The requirement to understand and improve the calibration performance of LLMs, which is crucial for determining the reliability of their predictions and preventing the dissemination of hate speech."}
{"hash_id": 2224367662262525067, "entities": ["machine translation probabilities", "human preferences", "translation candidates", "complementary strengths"], "background": "1. The need to bridge the gap between machine translation probabilities and human preferences to enhance translation quality. 2. The opportunity to leverage the complementary strengths of multiple translation candidates to create a more accurate and reliable translation."}
{"hash_id": 7534176227850497496, "entities": ["conventional referencebased metrics", "human judgments", "large language models", "opinion summary evaluation"], "background": "1. The poor correlation of conventional reference-based metrics with human judgments in evaluating opinion summaries. 2. The lack of exploration into the potential of Large Language Models (LLMs) as reference-free metrics for opinion summary evaluation."}
{"hash_id": 2382170425091944042, "entities": ["catastrophic forgetting", "parameter interference", "selective finetuning", "relevant neurons"], "background": "1. The need to mitigate catastrophic forgetting and parameter interference issues that arise when finetuning large language models for multilingual machine translation. 2. The desire to improve translation quality by selectively finetuning only the relevant neurons for the specific language pairs without affecting the model's original knowledge."}
{"hash_id": 5710922410637945813, "entities": ["documentlevel absa", "coreference resolution", "evaluation bias"], "background": "1. The need to improve the practicality and holistic understanding of document-level aspect-based sentiment analysis (ABSA) by incorporating coreference resolution. 2. The desire to reduce reliance on manually annotated coreference information and to mitigate the evaluation bias caused by missing coreference information of opinion targets."}
{"hash_id": 8152691580461462329, "entities": ["hallucination problem", "large visionlanguage models", "dialogue history impact"], "background": "1. The need to investigate and diagnose the hallucination problem in large vision-language models (LVLMs) when exposed to long-term misleading textual history. 2. The absence of benchmarks that thoroughly evaluate the impact of dialogue history on hallucination in LVLMs, particularly with multi-turn visual and textual inputs."}
{"hash_id": 5737597570689980011, "entities": ["pretrained llms", "information retrieval", "retrievalspecific tuning"], "background": "1. The need to improve the performance of pre-trained LLMs in information retrieval tasks by leveraging retrieval-specific tuning data, especially when full access to model parameters is not available. 2. The requirement for a method that is computationally efficient and can prevent overfitting when using limited amounts of tuning data."}
{"hash_id": 2163429163554426522, "entities": ["proximal policy optimization", "computational cost", "online rl optimization"], "background": "1. The high computational cost and complexity of Proximal Policy Optimization (PPO) in RLHF, which is exacerbated by the size of modern large language models. 2. The need for a simpler and more efficient method that retains performance while reducing the expertise required for hyperparameter tuning in online RL optimization."}
{"hash_id": 8759043935945104236, "entities": ["uniform scalable evaluation", "conditional image generation", "explainable metrics"], "background": "1. The need for more uniform and scalable evaluation methods beyond subjective human judgment in conditional image generation tasks. 2. The requirement for explainable metrics that can provide insights into the rationale behind the evaluation scores."}
{"hash_id": 6758125594777874776, "entities": ["neural network models", "transformerbased architectures", "natural language understanding", "unsupervised parsing methods", "supervised approaches"], "background": "1. To improve the understanding of how ambiguity is handled by different neural network models, specifically Transformer-based architectures, in order to enhance the overall Natural Language Understanding (NLU) capabilities of such systems. 2. To compare the effectiveness of unsupervised parsing methods with traditional supervised approaches in disambiguating complex grammatical structures like prepositional phrase attachment and garden path effects."}
{"hash_id": 5018112288283984884, "entities": ["complement large language models", "knowledge graphs", "zeroshot reasoning"], "background": "1. The need to complement large language models with reliable, structured, domain-specific, and up-to-date knowledge from knowledge graphs without the computational expense and limitations of retraining or fine-tuning. 2. The desire to enable black-box language models to perform zero-shot reasoning over multiple knowledge graphs, which is crucial for various applications such as personalized knowledge integration and enhancing domain-specific knowledge."}
{"hash_id": 1770010836124563505, "entities": ["global identity groups", "associated stereotypes", "ti models", "visual depictions"], "background": "1. The need to evaluate and cover a broader range of global identity groups and their associated stereotypes in T2I models to prevent the under-representation and marginalization of certain groups. 2. The requirement to distinguish between stereotypes that are more likely to have visual depictions and those that are less visually concrete to better understand and address the biases in T2I generations."}
{"hash_id": 3000124692947779849, "entities": ["factuality detection transferability", "computational overhead reduction", "online consistency checking"], "background": "1. The need to improve the transferability of factuality detection methods across diverse data distributions without relying on human-annotated labels. 2. The requirement to reduce the computational overhead associated with online consistency checking by avoiding the generation of multiple responses during inference."}
{"hash_id": 5643397025722857498, "entities": ["incontext learning", "large language models", "consensus on processes"], "background": "1. The need to understand the mechanisms behind large language models' ability to perform in-context learning, which is crucial for enhancing their performance and applying them to new tasks. 2. The lack of consensus on the exact processes that enable in-context learning, which hinders the development of more effective and efficient language models."}
{"hash_id": 6373233972638826074, "entities": ["opensource agents", "complex interactive tasks", "generalizable agent learning"], "background": "1. The need for open-source alternatives to closed-source agents to overcome issues of cost, transparency, and reproducibility, particularly in complex interactive tasks. 2. The desire to enhance generalizable agent learning across diverse interactive tasks without relying on expensive and opaque closed-source large language model APIs."}
{"hash_id": 2301926472002552101, "entities": ["cultural knowledge representation", "llms bias", "culturally sensitive topics"], "background": "1. The need to ensure that Large Language Models (LLMs) accurately represent and understand the diverse knowledge of different cultures, as they are promoted as repositories of collective human knowledge. 2. The observation that LLMs may exhibit biased or culturally aligned responses, which can be problematic for underrepresented groups and when dealing with culturally sensitive topics."}
{"hash_id": 5505190276444415174, "entities": ["ai player cicero", "communicative capabilities", "strategic games", "persuasion deception cooperation"], "background": "1. To rigorously evaluate the communicative and strategic capabilities of the AI player Cicero beyond just its ability to win games, focusing on the nuanced aspects of negotiation, persuasion, and deception that are central to Diplomacy. 2. To address the open problem of measuring AI's ability to persuade, deceive, and cooperate in strategic games by providing a framework that maps communication to in-game actions."}
{"hash_id": 4802512575664248174, "entities": ["coded language discrimination", "social media detection", "dog whistles usage"], "background": "1. The need to reveal and combat the use of coded language for discrimination, particularly in social media where it can evade detection systems. 2. The importance of creating a resource for researchers and systems to identify and understand the usage of dog whistles in various contexts."}
{"hash_id": 2844542338098152517, "entities": ["chainofthought reasoning", "language model performance", "theoretical understanding"], "background": "1. The empirical success of chain-of-thought (CoT) reasoning in improving language model performance lacks a concrete theoretical understanding. 2. Existing theoretical treatments of CoT-augmented LMs have not fully addressed the differences between language recognition and language modeling, nor have they considered the additional information that needs to be encoded in outputs."}
{"hash_id": 1667182804962672091, "entities": ["faithfulness evaluation", "dialogue summarization", "automatic error detection"], "background": "1. The need to understand and evaluate the faithfulness of LLMs in dialogue summarization, as they often generate plausible inferences without direct evidence, leading to concerns about hallucinations. 2. The gap in knowledge regarding the differences in behavior between LLMs and fine-tuned models in dialogue summarization, and the lack of effective automatic error detection methods for LLM-generated summaries."}
{"hash_id": 5087956065782175397, "entities": ["video understanding", "openended conversations", "large language models", "visual encoder"], "background": "1. The need to improve video understanding by enabling models to engage in open-ended, detailed conversations about video content, which can benefit various video-related tasks and applications. 2. The opportunity to leverage the capabilities of Large Language Models (LLMs) for video understanding by integrating them with a visual encoder that can maintain temporal and spatial characteristics."}
{"hash_id": 3799964568598301656, "entities": ["arabic dialects asr", "efficiency improvement", "multilingual asr evaluation"], "background": "1. The need to improve the efficiency and accessibility of ASR models for Arabic dialects, particularly for populations with limited computational resources. 2. The requirement for comprehensive evaluation of multilingual ASR models on a linguistically diverse set of Arabic datasets, including under-represented dialects."}
{"hash_id": 8777512108502019459, "entities": ["image sequence generation", "textual step instructions", "storytelling approaches", "realworld applicability"], "background": "1. The current state-of-the-art models struggle with generating image sequences that accurately portray textual step instructions and maintain coherence between successive images, which is crucial for enhancing user experience in manual tasks. 2. Existing storytelling approaches are limited in their applicability to real-world scenarios due to a lack of informative prompts accompanying images and non-linear dependencies between steps."}
{"hash_id": 7917633444875933919, "entities": ["linguistic diversity", "digital divide", "natural language generation", "lowresource languages"], "background": "1. The need to foster linguistic diversity and bridge the digital divide by empowering African communities to express themselves in their native languages. 2. The lack of effective natural language generation solutions for low-resource African languages, despite recent advancements in language modeling and transfer learning techniques that have shown promise in high-resource languages."}
{"hash_id": 1241755171253620833, "entities": ["hallucinations reduction", "factual accuracy", "interpretable system", "modular reasoning"], "background": "1. The need to reduce hallucinations and improve factual accuracy in long-form table question answering, which is crucial for enhancing the trustworthiness of NLP systems in domains like finance and healthcare. 2. The requirement for a more interpretable and modular system that allows users to understand and potentially modify the reasoning process to improve answer quality."}
{"hash_id": 6996751615897052467, "entities": ["domainspecific math problems", "knowledgeintensive reasoning", "math reasoning benchmarks"], "background": "1. The need to evaluate LLMs' capabilities in solving domain-specific, knowledge-intensive math reasoning problems, which are crucial for real-world applications. 2. The recognition of a significant gap in existing math reasoning benchmarks that fail to require specialized domain knowledge, particularly in areas like finance."}
{"hash_id": 4097893211731592221, "entities": ["diverse training data", "outofdomain generalization", "api sequencing dataset"], "background": "1. The need for more diverse and realistic training data to improve the out-of-domain (OOD) generalization of LLMs when interacting with APIs. 2. The absence of a comprehensive dataset that includes the sequencing of APIs, which is crucial for LLMs to perform complex, conversational tasks."}
{"hash_id": 3455000102706380825, "entities": ["lora modules", "generative tasks", "dynamic skill switching"], "background": "1. Enhancing the reusability of learned LoRA modules by dynamically combining them for generative tasks, especially when dealing with limited annotated data. 2. Addressing the limitation of existing LoRA fusion approaches that use static task-level weights, which may not be suitable for generative tasks requiring dynamic skill switching."}
{"hash_id": 216992781379929207, "entities": ["robustness improvement", "speech recognition", "audiovisual data scarcity"], "background": "1. The need to improve the robustness of speech recognition and translation systems in realistic noisy environments, where performance degrades significantly. 2. The limited availability of audio-visual (AV) data for training, especially in multiple languages, compared to the abundance of audio-only data."}
{"hash_id": 909796980355852717, "entities": ["social intelligence gap", "language agents", "social learning processes"], "background": "1. The need to bridge the gap in social intelligence between large language models and human abilities, particularly in areas such as theory of mind, following social norms, and navigating diverse goal-driven social scenarios. 2. The understudy of social learning processes in building language agents, despite humans learning social skills through both imitation and interaction."}
{"hash_id": 7902148457901035223, "entities": ["instructiontuned code llms", "model size scaling", "inference costs", "sparse upcycling"], "background": "1. The need to improve the performance of instruction-tuned code LLMs, which have been overlooked in favor of focusing on high-quality instruction datasets. 2. The desire to efficiently scale up model size without incurring high inference costs, addressing the limitations of sparse upcycling in instruction tuning."}
{"hash_id": 8924203357502595715, "entities": ["crossdomain decoding", "language models fusion", "organic collaboration"], "background": "1. To enhance the decoding capabilities of language models by fusing their expertise and improving performance in diverse tasks, particularly in cross-domain scenarios. 2. To eliminate the need for direct supervision in combining multiple language models, enabling a more organic and task-appropriate collaboration."}
{"hash_id": 5786041652240386612, "entities": ["dynamic rag methods", "static rules", "retrieval timing", "llm information needs", "factual accuracy"], "background": "1. To overcome the limitations of current dynamic RAG methods that rely on static rules for retrieval timing and restrict queries to the most recent output, which may not capture the LLM's full information needs. 2. To enhance the factual accuracy and coherence of LLM-generated text by dynamically retrieving relevant information based on the LLM's real-time information needs."}
{"hash_id": 757132217464699072, "entities": ["temporal reasoning datasets", "concurrent events", "cotemporal reasoning skills"], "background": "1. The current temporal reasoning datasets are limited to single or isolated events and do not reflect the realistic temporal characteristics involving concurrent events and complex temporal interconnections. 2. Large Language Models (LLMs) have shown impressive capabilities in various natural language tasks, yet they significantly lack co-temporal reasoning skills, which are crucial for understanding the real world."}
{"hash_id": 3278053916700768080, "entities": ["informative critiques", "finegrained distinguishability", "large language models", "commercial apis limitations", "data leakage"], "background": "1. The need for more informative critiques that can provide fine-grained distinguishability on generated texts to improve evaluation performance of large language models. 2. The desire to avoid the limitations and risks associated with using commercial APIs for accessing the best-performing LLMs, such as high cost and data leakage."}
{"hash_id": 3351924249785049273, "entities": ["benchmarking gap", "multiagent interactions", "llm capabilities evaluation"], "background": "1. The need to bridge the gap in existing benchmarks that either use static datasets leading to potential data leakage or focus only on single-agent scenarios, missing the complexities of multi-agent interactions. 2. The desire to comprehensively evaluate the diverse capabilities of LLMs in multi-agent, dynamic environments to guide future research in enhancing these models for practical applications."}
{"hash_id": 3370074238557468297, "entities": ["improve language model efficiency", "transferring language abilities", "performance gap llms slms", "creative tasks humor generation"], "background": "1. The need to improve the efficiency and sustainability of language models by transferring complex language abilities from large to small models. 2. The gap in performance between Large Language Models (LLMs) and Small Language Models (SLMs) in creative tasks like humor generation, which requires more than just imitation."}
{"hash_id": 3437500859959047789, "entities": ["extending large language models", "symbolic knowledge comprehension", "catastrophic forgetting", "preserving model generality"], "background": "1. The need to extend the capabilities of Large Language Models (LLMs) to comprehend and express world knowledge beyond natural language, such as chemical molecular formulas or first-order logic, which are more effectively represented in symbolic forms. 2. The challenge of incorporating symbolic knowledge into LLMs without causing catastrophic forgetting and while preserving the model's generality."}
{"hash_id": 7240901163454432975, "entities": ["lexical diversity", "paraphrased text data", "diversity incentive methods"], "background": "1. The need to enhance the lexical diversity of paraphrased text data generated by LLMs for improved downstream model performance. 2. The potential to adapt and transfer successful diversity incentive methods from crowdsourcing to the LLM-based text augmentation context."}
{"hash_id": 1350264325839357097, "entities": ["scarcity of transcribed speech", "dialectal arabic", "speech processing applications", "standard orthographic sound sets", "texttospeech tts", "computerassisted pronunciation training"], "background": "1. The scarcity of phonetically correct transcribed speech resources for dialectal Arabic, despite the abundance of dialectal speech data, which hinders the development of speech processing applications. 2. The need to extend beyond standard orthographic sound sets to recognize and restore the unique dialectal and borrowed sounds that are essential for improving performance in applications like Text-to-Speech (TTS) and Computer-Assisted Pronunciation Training."}
{"hash_id": 4695533032362065480, "entities": ["improve translation quality", "document context", "discourse phenomena", "sentence level", "parallel corpora", "documentlevel machine translation systems"], "background": "1. The need to improve translation quality by capturing document context, which is essential for translating certain discourse phenomena that cannot be accurately handled at the sentence level. 2. The lack of publicly available large-scale parallel corpora at the document level, which hinders the development and training of document-level machine translation systems."}
{"hash_id": 5452307167626266213, "entities": ["neural network capabilities", "formal language learning", "training objectives"], "background": "1. The need to bridge the gap between theoretical predictions of neural network capabilities and the suboptimal empirical results in formal language learning. 2. The requirement to overcome the limitations of standard training objectives and regularization techniques that do not lead to the optimal network solutions for certain formal languages."}
{"hash_id": 139916040602014846, "entities": ["language models", "balance prior knowledge", "context", "empirical evidence", "model interpretability"], "background": "1. To investigate and formalize how language models balance prior knowledge with new context in answering questions, particularly regarding entities. 2. To provide empirical evidence and metrics to measure the reliance on context and the susceptibility to prior knowledge, which can help improve model interpretability and reliability."}
{"hash_id": 396614373140277670, "entities": ["domainspecific training", "large language models", "domain adaptation performance"], "background": "1. The need to enhance the adaptability and efficiency of Large Language Models (LLMs) in summarization tasks across different domains to avoid the waste of resources caused by domain-specific training. 2. The desire to explore and understand the specific impact of 'words' on domain adaptation performance, moving beyond general influencers like model size and training data scale."}
{"hash_id": 1683354245100946210, "entities": ["visualization recommendation", "large language models", "manual maintenance"], "background": "1. The need to improve the performance of visualization recommendation by overcoming the limitations of manual maintenance and feature extraction in traditional methods. 2. The potential to leverage the powerful reasoning capabilities of Large Language Models (LLMs) for the task of visualization recommendation, without the need for fine-tuning the entire model."}
{"hash_id": 275890749314979616, "entities": ["limitations llms multihop questions", "queryspecific contextaware approach", "efficiency question answering"], "background": "1. The limitations of LLMs in handling complex multi-hop questions due to the overhead of understanding questions, filtering, and aggregating unstructured information. 2. The need for a more query-specific and context-aware approach that reduces ambiguity in extracted facts and enhances the efficiency of LLMs in question answering."}
{"hash_id": 541062984224946748, "entities": ["tokenlevel metaphor detection", "word pairs", "explainable clues"], "background": "1. The need to enhance the explainability of token-level metaphor detection, which is currently challenging due to the complexity of processing full sentences. 2. The desire to leverage the simplified structure of word pairs and the intermediate explainable clues they provide for improving the accuracy of token-level metaphor detection."}
{"hash_id": 2213580583389898648, "entities": ["improve logical reasoning", "large language models", "chainofthought methods"], "background": "1. The need to improve the logical reasoning abilities of large language models, which is essential for achieving human-like reasoning and advancing towards Artificial General Intelligence (AGI). 2. The recognition that current Chain-of-Thought (CoT) methods struggle with logical reasoning tasks that heavily rely on symbolic expressions and rigid deducing rules."}
{"hash_id": 6705028272044415029, "entities": ["improve sentiment polarity recognition", "mitigate irrelevant context impact", "absa models prediction performance"], "background": "1. The need to improve the accuracy of sentiment polarity recognition for aspect words, especially in complex sentences with multiple aspects and contrasting polarities. 2. The requirement to mitigate the negative impact of irrelevant contexts and syntactic dependencies on the prediction performance of ABSA models."}
{"hash_id": 5448834814589803892, "entities": ["highperformance coreference resolution", "discriminative encoderonly approaches", "large autoregressive models"], "background": "1. The need for high-performance coreference resolution that is also efficient and accessible within academic budgets, as current state-of-the-art models are computationally expensive and require large resources. 2. The belief that discriminative encoder-only approaches have untapped potential and have been prematurely discarded in favor of large autoregressive models."}
{"hash_id": 3365406297946739632, "entities": ["emotional support dialogue systems", "interpretability", "reliable connections"], "background": "1. The need for emotional support dialogue systems to have better interpretability to establish reliable connections with users. 2. The absence of focus on interpretability in previous works, which primarily concentrated on response generation without considering the underlying emotional processes."}
{"hash_id": 5514261827207802628, "entities": ["inconsistent model predictions", "logical reasoning tasks", "enhancing perception", "logical structures"], "background": "1. The current lack of consistent model predictions on samples with equal logical semantics due to the scarcity of training samples in logical reasoning tasks. 2. The challenge in enhancing the model's perception for logical structures, as current LMs are pre-trained on fact corpus and are naturally weak in capturing logical structures."}
{"hash_id": 3597720195712798313, "entities": ["intellectual property protection", "eaas providers", "model extraction attacks", "watermarking technique", "cse attacks"], "background": "1. The need to protect the intellectual property of EaaS providers from model extraction attacks, which can lead to competitive services being offered with minimal investment by attackers. 2. The requirement for a more robust and stealthy watermarking technique that can withstand attacks like CSE, which compromise the existing watermarking methods."}
{"hash_id": 3849097263816355079, "entities": ["computational efficiency", "finetuning neural models", "hyperparameter selection", "peft methods complexity"], "background": "1. The need to reduce the computational cost and improve the efficiency of fine-tuning large-scale neural models on specific downstream tasks without compromising performance. 2. The desire to simplify hyperparameter selection and reduce the complexity introduced by existing PEFT methods, which often require a significant number of adjustable parameters."}
{"hash_id": 8208527891409982087, "entities": ["translation quality", "training loss", "exposure bias", "simt models"], "background": "1. The limited correlation between translation quality and training loss, which leads to suboptimal performance when the model is consistently trained with the same context as used in inference. 2. The exposure bias in SiMT models, where the model is trained on ground truth data but must infer from its own predictions during actual use, leading to a performance gap between training and inference."}
{"hash_id": 5069764650574929370, "entities": ["improving robustness", "large language models", "adversarial examples", "incontext learning", "humanwritten explanations", "domainspecific knowledge"], "background": "1. The first motivation is to improve the robustness of large language models (LLMs) against adversarial examples, as in-context learning (ICL) can produce inaccurate results when faced with such inputs. 2. The second motivation is to reduce the dependency on human-written natural language explanations (NLEs), which are difficult to collect due to their requirement of domain-specific knowledge."}
{"hash_id": 5289971620348187639, "entities": ["limitation of transformers", "handling long sequences", "computational costs", "existing methods", "fromscratch training"], "background": "1. The need to overcome the limitation of transformers in handling long sequences due to the high computational and memory costs of self-attention operations. 2. The desire to improve upon existing methods that either require from-scratch training or sacrifice performance on short sequences."}
{"hash_id": 4709131343376708925, "entities": ["large language models", "software requirements", "functional requirements", "nonfunctional requirements", "code generation"], "background": "1. The need to extend the capabilities of large language models to manage both functional and non-functional software requirements from textual descriptions, which are often verbose or incomplete. 2. The desire to improve the satisfaction of functional requirements and introduce a new evaluation for non-functional requirements in code generation by LLMs."}
{"hash_id": 973674340810877647, "entities": ["manual annotation", "multilabel tasks", "domain knowledge", "annotation bias", "supervised learning methods"], "background": "1. The challenge of fully manual annotation in multi-label tasks due to the need for domain knowledge and large class sets, which often results in incomplete annotations. 2. The severe imbalance and distribution bias caused by missing positive class annotations in multi-label classification tasks, which hinders the performance of traditional supervised learning methods."}
{"hash_id": 9199796559777857075, "entities": ["unified editing task", "freeform text", "bottleneck diagnosis"], "background": "1. The need to update large language models with new information in a practical and unified editing task that involves free-form text, as the existing methods are mainly explored in a restricted task form with triple-based edit requests. 2. The lack of nuanced benchmark designs and re-evaluation of existing methods in the context of free text model editing, which hinders in-depth diagnosis of the bottleneck of these methods."}
{"hash_id": 1550425292158236220, "entities": ["zeroshot texttospeech", "mobile deployment", "inference speed", "model size", "robustness"], "background": "1. The need for a zero-shot text-to-speech system that can operate efficiently on mobile devices, addressing the current lack of solutions that are both fast and of high quality for mobile deployment. 2. The requirement to bridge the gap between the capabilities of zero-shot TTS models and their practical usage, considering factors such as inference speed, model size, and robustness."}
{"hash_id": 6508329588883082938, "entities": ["navigational instructions", "ai generated diversity", "language evaluation metrics", "systematic bias", "genuine useful instructions"], "background": "1. The need to improve the quality and variety of navigational instructions generated by AI, as current models produce instructions that lack diversity and do not adequately reference objects and landmarks. 2. The desire to avoid the systematic bias introduced by language evaluation metrics, which can lead to models learning to evade evaluation criteria rather than producing genuinely useful instructions."}
{"hash_id": 4131303409014233079, "entities": ["performance issues", "large language models", "coderelated tasks", "hierarchical structure", "code models"], "background": "1. The need to overcome the performance issues that arise when large language models exceed their pre-trained context length, particularly in complex code-related tasks. 2. The desire to mimic how human programmers navigate and understand the hierarchical structure of source code to improve the efficiency and effectiveness of code models."}
{"hash_id": 8556196668773412599, "entities": ["lost in the middle issue", "qa ability", "knowledgeintensive scenarios"], "background": "1. The need to overcome the \"lost in the middle\" issue where language models perform poorly when the correct information is located in the middle of long contexts. 2. The desire to improve the general QA ability of language models in knowledge-intensive scenarios by enhancing their focus on relevant information amidst noise."}
{"hash_id": 5736317284632368366, "entities": ["repositorylevel code generation", "complex contextual dependencies", "human programming practices", "programming tools", "realworld software development"], "background": "1. The limitations of current Large Language Models (LLMs) in handling complex, repository-level code generation tasks, where code snippets have intricate contextual dependencies and require interaction with extensive documentation and external tools. 2. The need to enhance the practical applicability of LLMs in real-world software development scenarios by emulating human programming practices, which heavily rely on the use of various programming tools."}
{"hash_id": 3354453790710343748, "entities": ["multistep problems", "planning methods", "discriminator component"], "background": "1. The need to understand how LLMs can effectively solve multi-step problems using different planning methods. 2. The potential dependency of advanced planning methods' effectiveness on the accuracy of the discriminator component in a language agent framework."}
{"hash_id": 3181695059416674274, "entities": ["logical reasoning capabilities", "large language models", "diverse inference rules"], "background": "1. The need to comprehensively evaluate the logical reasoning capabilities of large language models, which are fundamental to intelligence and crucial for practical applications, but remain under-explored. 2. The absence of a dataset that assesses logical reasoning independently of other forms of reasoning and covers a diverse range of inference rules across different logics."}
{"hash_id": 2388632414984857747, "entities": ["zeroshot classification", "language styles", "style classification"], "background": "1. The open-ended and evolving nature of language styles makes it costly and impractical to annotate data for every possible style, necessitating a zero-shot classification approach. 2. Current large language models and their instruction-tuned variants have not been thoroughly investigated for their efficacy in style classification and struggle with generalizing to new styles without fine-tuning."}
{"hash_id": 3289165422636009821, "entities": ["user privacy", "online selfdisclosures", "privacy risks", "privacy protection", "utility context"], "background": "1. The need to protect user privacy in online self-disclosures, which are common but can lead to privacy risks. 2. The desire to balance privacy protection with maintaining the utility and context of the disclosed information."}
{"hash_id": 2069523519480169632, "entities": ["performance degradation", "llms", "memory editing", "logical reasoning"], "background": "1. The need to understand and mitigate the performance degradation of LLMs after multiple sequential edits, which is crucial for real-world applications where new knowledge continuously emerges. 2. The requirement to evaluate the impact of memory editing on a wide range of LLM capabilities beyond factual knowledge, to ensure the preservation of complex skills such as logical reasoning and reading comprehension."}
{"hash_id": 9035935677110420801, "entities": ["stable leaderboards", "llms selection", "overfit benchmark formats", "realworld applicability"], "background": "1. The need for stable and reliable leaderboards to guide practitioners in selecting the most suitable LLMs for their projects, considering the high cost of training and inferencing these models. 2. The concern over the potential for LLMs to be overfit to specific benchmark formats, leading to high scores that do not necessarily reflect real-world applicability or genuine language understanding."}
{"hash_id": 1125167608953720056, "entities": ["human error reduction", "automated text assessment", "language model alignment"], "background": "1. The need to reduce the time, cost, and potential for human error in manually evaluating large numbers of natural language texts. 2. The requirement to align the judgments of large language models more closely with human judgments to ensure reliable and accurate automated text assessment."}
{"hash_id": 3693716655675549526, "entities": ["longterm dialogues", "chatbot efficacy", "evaluation metrics", "agent comprehension"], "background": "1. The need to thoroughly evaluate the efficacy of LLMs and RAG techniques in handling very long-term dialogues, which are crucial for creating engaging chatbots capable of remembering key information from past interactions. 2. The absence of comprehensive evaluation metrics suitable for directly assessing an agent's comprehension of long-term contexts and their ability to maintain consistency and empathy over extended conversations."}
{"hash_id": 5424925233771906510, "entities": ["resourceintensiveness", "large language models", "limited feedback conditions"], "background": "1. The need to reduce the resource-intensiveness of collecting human feedback for RLHF, which can lead to scalability issues for Large Language Models (LLMs) and complex tasks. 2. The desire to improve the adaptability and accuracy of LLMs in interpreting human preferences under limited feedback conditions."}
{"hash_id": 961749437976209556, "entities": ["impact of neologisms", "large language models", "temporal drift", "comprehensive benchmark"], "background": "1. The need to understand and quantify the impact of neologisms on the robustness and performance of Large Language Models (LLMs), as they are a significant aspect of language change that has been understudied. 2. The requirement to develop a comprehensive benchmark to assess how LLMs handle neologisms, given that existing models struggle with the temporal drift between training data and newer test data distributions, including the appearance of new words."}
{"hash_id": 1144099921732585638, "entities": ["improve moral reasoning", "complex stakeholder scenarios", "enhance accountability", "decisionmaking process", "minimize risky tail events"], "background": "1. The need to improve moral reasoning in LLMs, particularly in complex scenarios with multiple stakeholders, where current models struggle. 2. The requirement to enhance accountability and responsibility in the decision-making process of LLMs to minimize risky tail events that can be detrimental to stakeholders."}
{"hash_id": 8479649272445845763, "entities": ["emotion detection models", "speech emotion recognition", "multiple modalities"], "background": "1. The lack of comprehensive natural datasets that accurately reflect genuine emotions in real-world scenarios hinders the development of accurate emotion detection models in speech emotion recognition. 2. The need to enhance the accuracy of emotion prediction by integrating multiple modalities, including speech, text, and physiological data, to capture the complexity of emotional expressions."}
{"hash_id": 6866612359480734010, "entities": ["freetext explanations", "incontext learning", "ensemble methods", "prediction inconsistency", "highquality explanations"], "background": "1. The need to robustly leverage free-text explanations to enhance the in-context learning performance of large language models, as current methods do not effectively prioritize high-quality explanations. 2. The requirement to address the inconsistency between explanations and predictions, which can lead to defective results when using ensemble methods that treat all predictions equally."}
{"hash_id": 7742336829797109721, "entities": ["machine translation systems", "modern poetry translation", "evaluation criteria"], "background": "1. The current machine translation systems, including ChatGPT, struggle with translating modern poetry into Chinese, particularly in capturing the nuances and poetic essence of the source text. 2. There is a need for improved evaluation criteria that are specifically tailored to the unique characteristics of modern poetry translation."}
{"hash_id": 8061651853444247865, "entities": ["compute costs reduction", "pretraining efficiency", "data quality scarcity"], "background": "1. The need to reduce the high compute costs and duration associated with pre-training large language models on massive, unstructured, and noisy web data. 2. The requirement to improve the efficiency of pre-training by addressing the scarcity of high-quality data on the web and the impending issue of data quality."}
{"hash_id": 7878357671682383957, "entities": ["language models", "biased information", "chainofthought reasoning", "prompt editing insufficiency"], "background": "1. Large language models can be biased due to the biased information in their pretraining data, which affects their chain-of-thought reasoning abilities. 2. Existing methods of simply editing factual information in prompts are insufficient to correct the logical reasoning when biased information misleads the chain-of-thought process."}
{"hash_id": 2261314877443931774, "entities": ["improve representation sufficiency", "address overcompression", "information bottleneck methods"], "background": "1. The need to improve the sufficiency and predictive ability of representations learned by pre-trained language models for target tasks, while also ensuring robustness to noise and redundancy. 2. The desire to address the over-compression issue that can arise when using information bottleneck methods, which may lead to insufficient representations for the target task."}
{"hash_id": 7679850753856138344, "entities": ["large language models", "data contamination", "code generation benchmarks"], "background": "1. The need to understand the robustness and reliability of large language models (LLMs) in programming contexts due to the risk of data contamination from training corpora. 2. The requirement for a deeper examination of data contamination in code generation benchmarks, which differ significantly from natural language benchmarks, given the unique characteristics of code such as syntax and naming conventions."}
{"hash_id": 3973413723979131737, "entities": ["compromise in safety", "finetuning language models", "harmful behavior"], "background": "1. Fine-tuning language models often leads to a compromise in their safety, even with benign datasets, which is a significant concern that needs to be addressed. 2. Existing methods for ensuring safety in language models are not effective enough, as evidenced by the increasing number of instances where fine-tuned models exhibit harmful behavior."}
{"hash_id": 8424398076283180937, "entities": ["citationbased qa systems", "large language models", "knowledge graph triples", "webbased information"], "background": "1. The need to improve the efficiency of citation-based QA systems by reducing the reliance on large language models (LLMs) for knowledge retrieval, which can be costly and raise privacy concerns. 2. The requirement to enhance the accuracy of answers by incorporating both web-based information and knowledge graph triples, addressing the limitations of current systems that rely solely on web sources or use fixed-length quotes."}
{"hash_id": 2405900739229771818, "entities": ["catastrophic forgetting", "multimodal large language models", "visual instruction tuning"], "background": "1. The need to mitigate \"catastrophic forgetting\" in multi-modal large language models (MLLMs) that occurs when integrating diverse data forms, such as visual and textual information. 2. The desire to improve generalizability and performance on tasks outside the training distribution, particularly after observing diminished language capabilities in MLLMs post-visual instruction tuning."}
{"hash_id": 3482988775313956657, "entities": ["specialized domain labeled data", "large language models", "fewshot prompted llms"], "background": "1. The scarcity of labeled data in specialized domains makes it challenging to train models with sufficient performance, and existing large language models (LLMs) do not always perform well in these domains or with structured output tasks. 2. Few-shot prompted LLMs, while useful, still have room for improvement, and their pseudolabels can be enhanced to train smaller, more efficient models."}
{"hash_id": 7029435733035761823, "entities": ["blackbox language models", "generated text attributes", "harmful content control", "fluency maintenance"], "background": "1. The need to control the attributes of generated text in black-box language models to avoid producing harmful or inappropriate content. 2. The challenge of balancing control over the generated text with the maintenance of fluency in the absence of access to model parameters."}
{"hash_id": 7237980275611245513, "entities": ["ancient logographic writing systems", "standard nlp pipelines", "symbol inventories", "unicode mapping", "pretrained models"], "background": "1. The scarcity of transcribed data for ancient logographic writing systems hinders the application of standard NLP pipelines, which typically rely on symbolic representations, requiring extensive expert knowledge and labor for transcription. 2. The unique symbol inventories of ancient logographic languages often do not map well to Unicode or high-resource languages, limiting the effectiveness of transferring knowledge from pre-trained models."}
{"hash_id": 1147957630320638496, "entities": ["data filtering", "instruction tuning", "large language models llms"], "background": "1. To reduce the extra cost and computation involved in data filtering for instruction tuning, which can be a bottleneck due to the use of large filter models. 2. To free human labor from the data curation process and accelerate the instruction tuning process for Large Language Models (LLMs)."}
{"hash_id": 6296247629696570010, "entities": ["hallucinations in llms", "constructive affordances", "creativityfactuality balance", "domainspecific applications"], "background": "1. The prevalent negative view of hallucinations in large language models ignores their potential constructive affordances and cognitive similarities to human confabulations. 2. The need to optimize the balance between creativity and factuality in LLMs for domain-specific applications, rather than solely focusing on eliminating hallucinations."}
{"hash_id": 34102929119730170, "entities": ["parameterefficient finetuning", "computationally inexpensive method", "multitenant settings", "soft prompt tuning limitations", "lowrank adaptation methods"], "background": "1. The need for a more parameter-efficient and computationally less expensive fine-tuning method for large language models, especially in multi-tenant settings where latency is a critical issue. 2. The desire to improve upon the limitations of soft prompt tuning, which requires many soft tokens and has been outperformed by Low-rank adaptation (LoRA) methods, by proposing a method that is both effective and efficient."}
{"hash_id": 6182764621022699201, "entities": ["autoregressive inference", "large language models", "decoding enhancement", "smaller language models"], "background": "1. The need to improve the efficiency and quality of auto-regressive inference in large language models due to their high computational requirements and exposure bias. 2. The potential to enhance decoding by utilizing the predictions of smaller language models, which have been shown to have complementary properties to larger LMs."}
{"hash_id": 1417188425406294502, "entities": ["llmbased recommenders", "user preferences", "interaction rationales", "recommendation performance"], "background": "1. The need to enhance the reasoning capabilities of LLM-based recommenders by incorporating the underlying rationales behind user-item interactions, such as user preferences and item attributes. 2. The potential to improve recommendation performance and reduce noise in user and item profiles by distilling and leveraging interaction rationales from reviews."}
{"hash_id": 1411688316510477334, "entities": ["enforcing isotropy", "embedding spaces", "cluster presence", "linear classification", "mathematical framework"], "background": "1. To resolve the conflicting evidence in the literature regarding the benefits of enforcing isotropy in embedding spaces, particularly in the context of cluster presence and linear classification objectives. 2. To provide a mathematical framework that explains the relationship between isotropy and the geometry of embedding spaces, which can inform better design choices for vector representations in NLP."}
{"hash_id": 4353051455352816244, "entities": ["large language models", "arithmetic tasks", "last digit prediction"], "background": "1. To understand the limitations and abilities of large language models in performing arithmetic tasks, particularly the discrepancy between their performance on easier and harder arithmetic problems. 2. To explore methods to improve the confidence and accuracy of large language models in predicting the last digit of multiplication problems, which is a simpler task but one that they struggle with."}
{"hash_id": 4903768376850963219, "entities": ["educational gap", "multimodal reasoning benchmarks", "ai reasoning capabilities", "curriculum learning"], "background": "1. The need to bridge the educational gap in existing multimodal reasoning benchmarks, which overlook the high school to pre-college phase of learning, a critical period in educational development. 2. The requirement for a more comprehensive evaluation of AI models' reasoning capabilities, particularly in scientific domains, to push the boundaries of their performance and facilitate curriculum learning for AI systems."}
{"hash_id": 8738610234550420759, "entities": ["knowledge retrieval", "longtail knowledge", "redundant computation"], "background": "1. The need to improve the efficiency of knowledge retrieval in RAG for LLMs by focusing on the specific type of knowledge (long-tail) that LLMs actually need. 2. The recognition that existing RAG methods perform redundant computation on common knowledge that LLMs have already learned well during pre-training."}
{"hash_id": 7436457668411444483, "entities": ["performance gap ie", "large language models", "ie corpus", "zeroshot generalization"], "background": "1. The need to bridge the performance gap in Information Extraction (IE) tasks when using Large Language Models (LLMs) due to the lack of high-quality, large-scale datasets. 2. The requirement for a unified and automated approach to collect instruction data to build a comprehensive IE corpus that can enhance zero-shot generalization in LLMs."}
{"hash_id": 827831623335103626, "entities": ["improve text generation", "avoid incorrect triples", "enhance finegrained information"], "background": "1. The need to improve the accuracy of text generation from knowledge graphs by avoiding the inclusion of incorrect triples for each sentence. 2. The requirement to enhance the fine-grained information in each sentence to produce more coherent and engaging text, especially with the growing size of knowledge graphs."}
{"hash_id": 8113008557240998951, "entities": ["crosslingual slu frameworks", "sentence structure", "codeswitched sentences", "zeroshot", "globalization application"], "background": "1. Existing methods disrupt the inherent structure of sentences and ignore the correlation between original and code-switched sentences, which hinders further alignment of contextualized embeddings. 2. There is a need to improve the robustness and performance of zero-shot cross-lingual SLU frameworks for better globalization application of dialog systems."}
{"hash_id": 2588141482459043000, "entities": ["computational costs reduction", "efficient finetuning", "trainable parameters reduction"], "background": "1. The need to reduce the high computational costs and over-fitting that occur during fine-tuning of large pre-trained language models. 2. The requirement for a more efficient fine-tuning method that retains or improves model performance while significantly reducing the number of trainable parameters."}
{"hash_id": 2378610451801506346, "entities": ["improve reasoning llms", "diverse prompts", "reduce manual labor"], "background": "1. The need to improve the reasoning ability of Large Language Models (LLMs) on complex tasks by using diverse prompts tailored to different question types. 2. The desire to reduce manual labor and time-intensive efforts required to design prompts for LLMs, while enhancing their performance."}
{"hash_id": 1133786337212447590, "entities": ["language models", "numeric properties", "internal representations", "monotonic relationship"], "background": "1. The first motivation is to explore how numeric properties are encoded in the internal representations of language models, as this understanding is currently not well understood. 2. The second motivation is to develop a method for identifying and manipulating these representations to demonstrate a monotonic relationship between the interventions and the numeric quantities expressed by the model."}
{"hash_id": 8281876600116850044, "entities": ["chinese spelling correction", "performance evaluation", "benchmark datasets", "performance bottleneck", "accuracy improvement"], "background": "1. The need to accurately evaluate the performance of Chinese spelling correction models due to the presence of mistakes in widely-used benchmark datasets. 2. The desire to overcome the performance bottleneck and improve the accuracy of spelling correction in existing models."}
{"hash_id": 5129792772406512906, "entities": ["incorporating gaze data", "pretrained language models", "enhance model performance"], "background": "1. The need to improve the learning of text representations in pre-trained language models by incorporating human-like cognitive signals from gaze data. 2. The desire to enhance model performance without incurring additional computational costs during the application stage."}
{"hash_id": 5233456967075707568, "entities": ["constrained decoding", "blackbox llms", "nlp tasks"], "background": "1. The need to apply constrained decoding to blackbox LLMs without requiring access to logits, which is often not feasible with commercial models. 2. The desire to enhance the performance and flexibility of blackbox LLMs for complex NLP tasks by refining their outputs to adhere to specific constraints."}
{"hash_id": 7333754720272608378, "entities": ["semantic capabilities", "diffusionbased tts", "precise editing", "realworld applications", "control methods"], "background": "1. The need to uncover the semantic capabilities of diffusion-based TTS models to enable precise editing of synthesized speech properties, which is essential for real-world applications like human-machine interaction. 2. The absence of methods that allow for the control of TTS models without requiring additional data, retraining, or architectural changes, thus enhancing the efficiency and usability of TTS systems."}
{"hash_id": 6481229252878469759, "entities": ["llm memorization", "privacy protection strategies", "pii leakage", "privacy risks mitigation"], "background": "1. The lack of understanding of how LLMs memorize PII, which hinders the development of effective privacy protection strategies. 2. The need to mitigate privacy risks associated with PII leakage while maintaining the model's performance on non-sensitive tasks."}
{"hash_id": 7873003748363266975, "entities": ["crossdocument event coreference resolution", "challenging dataset", "lexically diverse", "figurative language", "metaphors"], "background": "1. To create a more challenging and lexically diverse dataset for Cross-Document Event Coreference Resolution (CDEC) that better reflects the complexity of real-world scenarios. 2. To provide a resource for research in event comprehension that includes figurative language, specifically metaphors, which is an underexplored area in CDEC datasets."}
{"hash_id": 4833013442601851917, "entities": ["improving large language models", "sample and select methods", "flexible scoring criteria"], "background": "1. The need to improve the performance and efficiency of large language models in interactive tasks where there are multiple distinct and valid answers, as current \"sample and select\" methods like self-consistency become prohibitively expensive in such scenarios. 2. The recognition that strict scoring criteria based on majority voting fail to provide consistent gains in tasks with sparse action spaces, necessitating a more flexible and sample-efficient approach."}
{"hash_id": 1861863305272599827, "entities": ["flexibility scalability", "recommendation systems", "data sparsity", "textual understanding", "large language models"], "background": "1. The need to improve the flexibility and scalability of recommendation systems to handle a large number of users and extensive datasets, while also addressing data sparsity issues. 2. The potential to leverage the textual understanding capabilities of large language models (LLMs) to more effectively represent items by their descriptions and users by their interaction history, thereby enhancing the alignment with LLMs' strengths."}
{"hash_id": 8944879922646142578, "entities": ["critical assessment", "large language models", "intrinsic grammatical structures"], "background": "1. To critically assess the assumption that large language models represent linguistic structures in line with familiar linguistic formalisms, without the bias of pre-defined target labels. 2. To investigate and reveal the intrinsic grammatical structures of large language models like BERT, which may deviate from human linguistic theories."}
{"hash_id": 1578054571220905156, "entities": ["tailored lay summaries", "generation control", "scientific summaries adaptability"], "background": "1. The need for tailored lay summaries that consider the specific goals and expertise of different audiences, as current approaches assume a uniform level of technical understanding. 2. The absence of fine-grained control over the generation of scientific summaries, which limits their adaptability and readability for various readers."}
{"hash_id": 2967936250855175922, "entities": ["spatial understanding", "embodied tasks", "realworld environments"], "background": "1. The current capacity of Large Vision-Language Models (LVLMs) in spatial understanding for embodied tasks is unexplored and needs comprehensive evaluation. 2. There is a gap in qualified embodied intelligence that can be bridged by improving LVLMs' ability to understand spatial relationships in real-world environments."}
{"hash_id": 374956325599541309, "entities": ["improve birdbench reliability", "noise impact on performance", "robust texttosql models"], "background": "1. The need to improve the reliability of the BIRD-Bench benchmark by understanding and addressing the impact of noise on model performance. 2. The desire to develop more robust Text-to-SQL models that can handle real-world noisy inputs effectively."}
{"hash_id": 5410935890523980755, "entities": ["positional biases", "language models", "dense retrieval", "text representation learning"], "background": "1. The need to understand and mitigate positional biases in language models, which can lead to the loss of important information from the middle of long documents in web retrieval tasks. 2. The desire to improve the effectiveness of text representation learning in dense retrieval, which is crucial for various applications such as retrieval-augmented generation and recommendation systems."}
{"hash_id": 3458294157581459956, "entities": ["uid hypothesis", "syntactic reduction", "computational models", "entropy measures"], "background": "1. The need to empirically validate the UID hypothesis in a contemporary linguistic context using large-scale data and advanced computational models. 2. The desire to expand the understanding of UID's role in syntactic reduction by incorporating new measures, such as entropy, which can provide a more nuanced explanation of language use."}
{"hash_id": 2162942315876195620, "entities": ["isolate personal attributes", "biography content", "causal effect", "controlled setting"], "background": "1. The need to isolate the effect of personal attributes on biography content to measure the causal effect of such attributes on text generation. 2. The absence of identical biographies that only differ in the specific personal attribute of interest, making it difficult to study bias in biography generation without a controlled setting."}
{"hash_id": 8291746436418531045, "entities": ["dependency on glosslabelled data", "translation performance", "sign language translation systems"], "background": "1. The need to overcome the dependency on gloss-labelled data, which is scarce and requires time-consuming manual annotation. 2. The desire to improve translation performance in the absence of glosses and to bridge the gap between gloss-dependent and gloss-free sign language translation systems."}
{"hash_id": 7615951115259828920, "entities": ["debate evaluation", "large language models", "zeroshot capabilities"], "background": "1. The need to understand and mitigate biases in large language models like GPT-3.5 and GPT-4 when used for debate evaluation, as their judgments can impact fairness and reliability. 2. The desire to improve the generalizability of debate evaluation methods by leveraging the zero-shot capabilities of large language models, which could potentially surpass existing state-of-the-art methods that rely on extensive data training and feature engineering."}
{"hash_id": 6350356682565613290, "entities": ["robustness of translation metrics", "finetuned metrics", "stringmatching metrics", "bleu", "nuanced validation"], "background": "1. The need to understand the robustness of fine-tuned machine translation metrics in domains not seen during training, as their performance may be attributed to the artificial domain match between training and test data. 2. The importance of moving beyond string-matching metrics like BLEU to more nuanced fine-tuned metrics, and the necessity to validate the supposed robustness of these metrics across different domains."}
{"hash_id": 8047513804260531371, "entities": ["neural information retrieval", "indian languages", "largescale datasets"], "background": "1. The need to advance Neural Information Retrieval for Indian languages, which are underrepresented in the field, to support a diverse range of linguistic communities. 2. The recognition that large-scale datasets are crucial for training effective neural IR models, yet such resources are scarce for many low-resource Indian languages."}
{"hash_id": 2801021886360799184, "entities": ["degeneration accumulation", "optimization approach", "cooperative game dynamics"], "background": "1. The need to prevent degeneration accumulation in rationalization models, which occurs because current methods do not effectively control the learning direction of the model and can lead to the model overfitting on uninformative or flawed rationales. 2. The requirement for a comprehensive optimization approach that considers the causality during rationale learning, as existing methods fail to directly optimize the cooperative game dynamics between the generator and predictor during training."}
{"hash_id": 692387322898587090, "entities": ["access to research artefacts", "reproducibility of studies", "nlp openness", "equitable resource distribution"], "background": "1. The lack of access to research artefacts in NLP hinders the reproducibility of studies and the advancement of the field. 2. There is a need to measure the level of openness in the NLP community to improve sharing practices and promote equitable resource distribution."}
{"hash_id": 1503276880750560822, "entities": ["ability vllms", "humanlike variability", "naming describing objects", "contextsensitive expressions"], "background": "1. To understand and improve the ability of VLLMs to capture human-like variability in naming and describing visual objects, which is crucial for natural language generation in real-world scenarios. 2. To explore and address the limitations of current VLLMs in handling context-sensitive expressions, such as non-numerical quantifiers, which require more accurate and high-level reasoning."}
{"hash_id": 183844531029162367, "entities": ["large language models", "nonmonotonic reasoning", "belief consistency"], "background": "1. To bridge the gap in understanding whether large language models (LLMs) exhibit human-like nonmonotonic reasoning or classical reasoning when faced with generics and exceptions. 2. To assess the consistency and reliability of LLMs in maintaining beliefs about generics in the presence of additional information, which is crucial for their application in real-world scenarios."}
{"hash_id": 4152614823409547455, "entities": ["manual prompt writing", "prompt optimization methods", "large language models"], "background": "1. The difficulty and ambiguity involved in manually writing effective prompts for large language models, which often requires significant experimentation and effort. 2. The limitations of existing prompt optimization methods that produce hard-to-interpret prompts and assume a single, optimized prompt should be applied at inference."}
{"hash_id": 8199725491830533434, "entities": ["knowledge updating", "llms", "locateandedit methods", "realworld applications"], "background": "1. The need to keep the knowledge in LLMs up-to-date for time-sensitive information in real-world applications. 2. The inefficiency and limitations of existing locate-and-edit knowledge editing methods in terms of run-time and capability to handle complex queries."}
{"hash_id": 192056370459240545, "entities": ["manual prompt engineering", "large language models", "automated prompt optimization"], "background": "1. The inefficiency and sub-optimality of manual prompt engineering for large language models, which often relies on trial and error and lacks guiding principles. 2. The need for automated methods to adapt and optimize prompts as large language models evolve and are widely adopted in various applications."}
{"hash_id": 2556461738774292345, "entities": ["negation in nlu tasks", "limited negation corpora", "model training performance"], "background": "1. Language models consistently underperform in natural language understanding tasks that involve negation, which is a fundamental linguistic phenomenon. 2. Existing corpora for natural language understanding tasks often have limited negations, and those present are frequently unimportant, which hinders model training and performance."}
{"hash_id": 7900720030226190230, "entities": ["pinyin input methods", "onetomany mappings", "beam search", "human behavior alignment"], "background": "1. The need to improve the efficiency and accuracy of pinyin input methods in handling one-to-many mappings, especially in low-resource environments where computational resources are limited. 2. The observation that existing methods like beam search do not align with human behavior in selecting diverse and surprising candidates, and they can be computationally intensive for resource-constrained devices."}
{"hash_id": 4707028438461796774, "entities": ["historical information modeling", "conversational questionanswering", "gold answers reliance"], "background": "1. The need to effectively model historical information without the noise caused by irrelevant history in conversational question-answering. 2. The desire to improve upon existing methods that rely on gold answers of history, which do not align with real-world scenarios where models must rely on their own predictions."}
{"hash_id": 6530642167718088656, "entities": ["lowresource languages", "reranking effectiveness", "zeroshot reranking"], "background": "1. The need to improve reranking effectiveness in low-resource languages, where information retrieval is challenging due to limited data and resources. 2. The exploration of the potential of large language models to perform zero-shot reranking in cross-lingual settings, given their success in other reranking tasks."}
{"hash_id": 5423436512610188685, "entities": ["crossmodal projection layer", "llm parameters", "domainspecific visual data"], "background": "1. To understand the roles of different components in MLLMs, specifically the cross-modal projection layer and the LLM parameters, in order to improve the model's ability to handle domain-specific visual data. 2. To provide insights that can inform future design choices and enhance the interpretability of MLLMs for domain-specific applications."}
{"hash_id": 8273345263694958624, "entities": ["alignment large language models", "sequencelevel feedback", "enhance reward model training", "finegrained preferences optimization"], "background": "1. The need to improve the alignment of large language models with human preferences by addressing the lack of precision in existing sequence-level feedback. 2. The requirement to enhance the training process of reward models to capture fine-grained preferences, which can lead to better optimization of language models."}
{"hash_id": 337484004750629232, "entities": ["translation performance", "lowresource language pairs", "selfreflective abilities", "feedback mechanisms", "translation quality"], "background": "1. The need to improve translation performance, especially in low-resource language pairs, by enhancing the self-reflective abilities of large language models. 2. The recognition that existing self-reflection methods lack effective feedback, which is crucial for the continuous improvement of translation quality."}
{"hash_id": 8378277013890571054, "entities": ["lvlms comprehension", "explaining artworks", "standardized task evaluation"], "background": "1. The need to understand the extent to which LVLMs can comprehend and integrate knowledge necessary for explaining images, particularly in the context of artworks. 2. The lack of a standardized task and evaluation metric to quantitatively assess LVLMs' performance in generating explanations for artworks."}
{"hash_id": 7556380580622193577, "entities": ["hallucination in simt", "translation quality", "targetside reliance"], "background": "1. The severe issue of hallucination in Simultaneous Machine Translation (SiMT) due to the absence of complete source-side information, which is not well understood or analyzed in existing research. 2. The need to improve translation quality and reduce hallucinations in SiMT by controlling the model's reliance on target-side information, given the high entropy and memorization difficulty of hallucination words."}
{"hash_id": 6711671437396377495, "entities": ["unsupervised word translation", "lowerresource languages", "leveraging llms", "incontext learning"], "background": "1. The need to improve the performance of unsupervised word translation, especially for lower-resource languages, where current large language models (LLMs) do not match the performance of traditional mapping-based approaches. 2. The potential of leveraging LLMs for in-context learning, which has shown promising results in supervised and semi-supervised settings, but lacks effectiveness in a fully unsupervised scenario."}
{"hash_id": 2826661914460931501, "entities": ["diffusion models", "chinese texttoimage", "domainspecific applications", "lora controlnet", "finegrained style transfer"], "background": "1. The lack of robustness and context-specificity in existing diffusion models for Chinese text-to-image synthesis, which hinders their performance in domain-specific applications. 2. The unexplored potential of techniques like LoRA and ControlNet for fine-grained image style transfer and editing in the context of Chinese language."}
{"hash_id": 6212406222769765133, "entities": ["opensource noise injection", "evaluation benchmarks", "mlu methods", "simulated scenarios", "realworld multimodal noise"], "background": "1. The lack of open-source noise injection toolkits and evaluation benchmarks for researchers to quantitatively assess the global robustness of MLU methods, leading to inequitable comparisons and overfitting on specific noise patterns. 2. The need to bridge the gap between simulated scenarios and real-world multimodal noise to ensure that MLU systems operate effectively in practical usage scenarios."}
{"hash_id": 1051713183619880520, "entities": ["unify nlp tasks", "xnlp framework", "model generalization", "current limitations"], "background": "1. The need to unify various NLP tasks under the XNLP framework to leverage shared characteristics among tasks for better model generalization and improved performance in real-world scenarios. 2. The existence of limitations in current XNLP demonstration systems, such as being limited to specific tasks, lacking interactivity and extensibility, and not being universal systems, which require separate models for each task."}
{"hash_id": 1805782567809011724, "entities": ["prompt engineering", "nonaiexperts", "large language models", "everyday users", "llms routine tasks"], "background": "1. The need to simplify prompt engineering for non-AI-experts who struggle with creating effective prompts for large language models. 2. The absence of tools that cater to everyday users who want to leverage LLMs for routine tasks, not just AI application developers."}
{"hash_id": 6868633157234229437, "entities": ["prediction process transparency", "transformer language models", "highstakes settings"], "background": "1. The need for a comprehensive tool that can make the entire prediction process of Transformer language models transparent to improve safety, reliability, and trustworthiness, especially in high-stakes settings. 2. The requirement to analyze large models efficiently by focusing on only the relevant model components that are important for specific predictions."}
{"hash_id": 542952509893789813, "entities": ["emotional resonance ai", "multimodal inputsoutputs", "empathetic responses"], "background": "1. The need to enhance emotional resonance in AI systems by incorporating multimodal inputs and outputs beyond text, to better emulate human-like empathy. 2. The gap in current research on generating multimodal empathetic responses that fully capture the nuances of human emotions expressed through voice and visual cues."}
{"hash_id": 8981501043729934047, "entities": ["knowledge editing methods", "parametric knowledge update", "computational expense", "standard implementation framework"], "background": "1. The need to efficiently update the parametric knowledge within LLMs to correct specific behaviors without the computational expense and potential overfitting of full retraining. 2. The lack of a standard implementation framework for knowledge editing methods, which hinders the adoption of these techniques by practitioners in various applications."}
{"hash_id": 6588657883949300437, "entities": ["instruction processing", "standardized framework", "instruction data quality", "llm performance enhancement"], "background": "1. To facilitate the development and advancement of instruction processing for LLMs by providing a standardized and easy-to-use framework. 2. To improve the quality and diversity of instruction data for LLMs, which is crucial for enhancing their performance in specific tasks or domains."}
{"hash_id": 7144124305277362986, "entities": ["nlp model evaluation", "complex interactive tasks", "evaluation tools"], "background": "1. The need for more accurate evaluation of NLP models, particularly in complex interactive tasks, where traditional evaluation methods overlook factors such as the perspective of human evaluators and the dynamic nature of interactions. 2. The absence of flexible and user-friendly evaluation tools that support a variety of interactive scenarios and can be easily customized for different use cases."}
{"hash_id": 5145162135436709974, "entities": ["scholarly document processing", "scientific papers", "nlp community", "unified system", "state of the literature"], "background": "1. The rapidly growing number of scientific papers makes it difficult for researchers to stay up-to-date with the state of the literature. 2. The need for a unified system that integrates different Scholarly Document Processing (SDP) technologies to efficiently consume a large number of papers in the NLP community."}
{"hash_id": 3154354131907103516, "entities": ["nlp literature", "exploratory searches", "literature search systems"], "background": "1. The rapid growth in NLP literature has made it difficult for researchers to efficiently explore and retrieve relevant information, especially when they are not familiar with a specific field or concept. 2. Existing literature search systems are primarily designed for targeted keyword-based searches, which do not adequately support exploratory searches that require a broader understanding of a field."}
{"hash_id": 8299698454742966336, "entities": ["costeffective customizable solution", "retrievalaugmented questionanswering", "toolkit support absence"], "background": "1. The need for a more cost-effective and customizable solution for researchers and developers to train, test, and deploy retrieval-augmented question-answering systems. 2. The absence of toolkits that support modifying models/training algorithms, comparing against prior work, and obtaining human evaluation for RQA systems."}
{"hash_id": 8809944134998549869, "entities": ["improving llm scalability", "rag applications", "peft support", "distributed settings"], "background": "1. The need to improve the scalability and feasibility of fine-tuning LLMs for complex RAG applications, especially on systems with limited GPU resources. 2. The absence of support for Parameter Efficient Fine-Tuning (PEFT) in existing machine learning frameworks when using tensor-parallel training in distributed settings."}
{"hash_id": 8139035074359221342, "entities": ["efficient llm inference", "mobile devices", "data privacy"], "background": "1. The need to perform efficient LLM inference on mobile devices without compromising data privacy. 2. Overcoming the limitations of individual mobile device capacities and bandwidth constraints for handling large language models."}
{"hash_id": 4173627249765845162, "entities": ["comprehensive platform", "drug safety information", "adverse drug reactions context"], "background": "1. The need for a comprehensive platform that allows researchers and practitioners to access and analyze detailed drug safety information from various sources. 2. The requirement to improve the understanding of the context in which adverse drug reactions occur, beyond the statistical data provided by spontaneous reporting systems."}
{"hash_id": 7263852351763215651, "entities": ["evaluation initiatives", "chinese llms", "alignment safety issues"], "background": "1. The current evaluation initiatives for Chinese LLMs primarily focus on capabilities, neglecting alignment and safety issues, which are crucial for the responsible development and deployment of these models. 2. There is a lack of a comprehensive, up-to-date, and unbiased evaluation platform that can assess both open-source and proprietary Chinese LLMs across multiple dimensions."}
{"hash_id": 760260926704699665, "entities": ["sentre", "docre", "reflective approach", "realworld scenarios"], "background": "1. The limitations of existing methods in Sentence-level Relation Extraction (SentRE), which do not effectively handle multiple relations and facts distributed across a document in Document-Level Relation Extraction (DocRE). 2. The need for a more reflective approach of real-world scenarios where relation options are not assumed to be known in advance."}
{"hash_id": 6180091215161154835, "entities": ["record linkage", "traditional string matching", "transformer llms"], "background": "1. The need to bridge the gap between the ease of use of traditional string matching packages and the enhanced performance of modern large language models for record linkage in academic applications. 2. The requirement for a more accessible and extensible tool that allows researchers without deep learning expertise to benefit from the advancements in transformer LLMs for diverse language and historical datasets."}
{"hash_id": 4119537577261333058, "entities": ["evaluation framework", "comprehensive evaluation", "multiple tasks", "llms development"], "background": "1. The need for a swift and easy-to-use evaluation framework to keep up with the rapid development of LLMs, considering their increasing complexities and uncertainties. 2. The requirement for a comprehensive evaluation that covers multiple tasks and provides a complete evaluation pipeline, from model deployment to metrics calculation, to ensure responsible and beneficial applications of LLMs."}
{"hash_id": 6062905834182896912, "entities": ["fabricated stories", "generative artificial intelligence", "comprehensive pipeline"], "background": "1. The increasing challenge of distinguishing genuine news from fabricated stories due to the advancement of generative artificial intelligence. 2. The lack of a comprehensive end-to-end pipeline that fully utilizes external resources like the Internet and includes news comprehension, search optimization, verification, and reasoning."}
{"hash_id": 6294261755478168862, "entities": ["unified platform", "stringtostring algorithms", "efficiency improvement", "modularity enhancement"], "background": "1. The need for a unified and accessible platform that combines a broad collection of string-to-string algorithms for various domains, improving efficiency and user experience. 2. The requirement to enhance flexibility, modularity, and documentation for existing string-to-string problem solutions to facilitate broader adoption and easier integration into diverse applications."}
{"hash_id": 6464998086863549476, "entities": ["fast sloppy typing", "manual error correction", "large language models"], "background": "1. The need to improve the typing experience for users who prefer fast and sloppy typing by reducing the need for manual and multi-step error correction. 2. The opportunity to leverage the advanced capabilities of Large Language Models (LLMs) to provide high-quality sentence-or-higher-level error correction on mobile devices."}
{"hash_id": 252645413888174349, "entities": ["democratize ai", "linguistically diverse regions", "performance gap", "cultural homogenization"], "background": "1. The need to democratize AI and empower linguistically diverse regions, particularly Southeast Asia, where languages are rich and diverse but lack extensive dataset support. 2. The urgency to bridge the performance gap between high-resource languages like English and low-resource regional languages, to prevent cultural homogenization and the loss of linguistic diversity."}
{"hash_id": 7020974663184164227, "entities": ["poetry generation system", "chinese classical poetry", "format errors", "content control"], "background": "1. The need for a poetry generation system that can effectively control both the strict format of Chinese classical poetry and the richness of content. 2. The limitations of traditional systems that use keywords and token-based LLMs, which often result in format errors or insufficient control over content."}
{"hash_id": 9003933922391980628, "entities": ["structured knowledge extraction", "unstructured text data", "humanmachine collaboration", "lowresource contexts", "model performance"], "background": "1. The need to improve the efficiency and accuracy of structured knowledge extraction from diverse and complex unstructured text data. 2. The desire to enhance the collaboration between human annotators and machine learning models, particularly in low-resource contexts, to reduce costs and improve model performance."}
{"hash_id": 8546578906709799949, "entities": ["leveraging generalizable knowledge", "embodied ai", "opensource platforms", "language grounding"], "background": "1. The need to leverage the extensive generalizable knowledge present in Large Language and Multimodal Models for embodied AI. 2. The lack of open-source platforms that facilitate the integration of language grounding in embodied environments and the generation of large-scale training data for embodied agents."}
{"hash_id": 2116532368207548236, "entities": ["hallucinations reduction", "knowledge graph challenges", "questionanswering applications"], "background": "1. To reduce the propensity of large language models to generate hallucinations and factually inaccurate content in question-answering applications. 2. To overcome the challenges of incomplete knowledge coverage and knowledge update misalignment in existing knowledge graphs used in conjunction with LLMs."}
{"hash_id": 6764809801140557597, "entities": ["hallucination reduction", "legal advice accuracy", "llm interpretability"], "background": "1. To reduce the occurrence of hallucination and improve the accuracy of legal advice provided by LLMs. 2. To enhance the interpretability and informativeness of LLM responses by providing a clear legal basis for the advice."}
{"hash_id": 3731222603932213243, "entities": ["unified implementation frameworks", "reproducibility", "comprehensive evaluation"], "background": "1. The lack of standardized and unified implementation frameworks hinders reproducibility and fair comparison of large language models. 2. Existing libraries are limited in scope, focusing on certain stages of LLM development, and do not provide comprehensive evaluation across various tasks and datasets."}
{"hash_id": 6318234604278119209, "entities": ["training cost reduction", "efficient finetuning methods", "systematic framework"], "background": "1. The need to reduce the training cost and resource requirements when fine-tuning large language models on downstream tasks. 2. The absence of a systematic framework that unifies and simplifies the application of various efficient fine-tuning methods across different language models."}
{"hash_id": 199296520765877738, "entities": ["improve llms pragmatic meaning", "scalar implicature research gap", "nlp pragmatic inference"], "background": "1. The need to improve the ability of Large Language Models (LLMs) to understand and infer pragmatic meaning, which is crucial for achieving more natural and accurate language processing. 2. The gap in research on pragmatic inference, particularly scalar implicature, within the context of NLP, despite its importance in conveying intended meaning in human communication."}
{"hash_id": 6594831509615696721, "entities": ["conventional topic models", "short texts", "large language models"], "background": "1. The limitations of conventional topic models in handling short texts due to their reliance on word co-occurrence, which is less prevalent in short texts. 2. The potential of Large Language Models (LLMs) to overcome these limitations by contextually learning word meanings through pretraining on massive text corpora."}
{"hash_id": 710898572987652342, "entities": ["developer productivity", "code editing tasks", "large language models", "instructiontuning data"], "background": "1. The need to enhance developer productivity by automating monotonous and complex code editing tasks, which are currently underexplored by deep learning models. 2. The potential to leverage Large Language Models (LLMs) for code editing by addressing the scarcity of relevant instruction-tuning data that hampers their performance."}
{"hash_id": 5936178851412555471, "entities": ["gender racial religious biases", "language models", "rlhf limitations"], "background": "1. The need to mitigate gender, racial, and religious biases in language models to ensure ethical and socially responsible use. 2. The limitations and challenges faced by existing methods like Reinforcement Learning from Human Feedback (RLHF) in effectively reducing biases and the complexity involved in training."}
{"hash_id": 4316689645516478770, "entities": ["extending large language models", "catastrophic forgetting", "visionlanguage understanding"], "background": "1. The need to extend the capabilities of large language models to include vision-language understanding without the high cost and time of training from scratch on multimodal data. 2. The challenge of preventing catastrophic forgetting when adapting existing models to new modalities, especially with the increasing size and complexity of LLMs."}
{"hash_id": 2013586239131212552, "entities": ["ai model interpretability", "classroom collaboration", "educational ai implementation"], "background": "1. To cultivate trust among teachers in AI-powered models for classroom collaborative argumentation analysis by enhancing model interpretability. 2. To facilitate the practical implementation of AI in educational contexts by addressing the challenge of complex deep learning models appearing as \"black boxes.\""}
{"hash_id": 9003527920974805927, "entities": ["document alignment", "webcrawled corpora", "sentence segmentation limitations"], "background": "1. The need to enhance the quality and efficiency of document alignment in web-crawled parallel corpora, especially for languages with different sentence segmentation and long-text encoding challenges. 2. The desire to improve upon existing methods by addressing the limitations of sentence-based segmentation, which may miss crucial information and struggle with boilerplate text."}
{"hash_id": 1911056541954219631, "entities": ["language acquisition", "example sentences", "proficiency levels", "pretrained language models", "personalized experience"], "background": "1. The need to enhance the effectiveness of language acquisition by providing example sentences that match learners' proficiency levels and offer diversity. 2. The desire to leverage the capabilities of Pretrained Language Models to improve the adaptability and personalized experience of language learning resources."}
{"hash_id": 1044235325008434166, "entities": ["instruction tuning", "llm performance", "instruction datasets optimization"], "background": "1. The need to improve the performance of large language models (LLMs) across various tasks through instruction tuning, while the procedure for optimizing the mixing of instruction datasets is not well understood. 2. The desire to determine the most effective combinations of instruction datasets to enhance overall LLM performance without negatively impacting specific task areas."}
{"hash_id": 5332027755351301984, "entities": ["crosslingual transfer", "tokenlevel detection", "german datasets"], "background": "1. The need to extend token-level reference-free hallucination detection beyond English to other languages, given the importance of this task for ensuring reliability in text generation. 2. The lack of existing datasets and benchmarks in German that enable the study of cross-lingual transfer for this task, which is crucial for real-time applications."}
{"hash_id": 3272016541048686348, "entities": ["fewshot text classification", "midtolow resource languages", "verbalizers limitations"], "background": "1. The need to improve few-shot text classification performance in mid-to-low resource languages where manual selection of labels may not yield the optimal words for a given language model. 2. The requirement to address the limitations of existing verbalizers which may not generate relevant words for the classes of interest, especially when language models are smaller and less trained on comprehensive data."}
{"hash_id": 4130014485356173902, "entities": ["modeling linguistic diversity", "multiword expressions", "nlp resource evaluation"], "background": "1. The need to model linguistic diversity in Multiword Expressions (MWEs) due to their idiosyncratic nature, which makes them an interesting case study. 2. The lack of previous research on disparity in the context of MWEs, particularly in evaluating the quality of NLP resources along dimensions orthogonal to efficiency."}
{"hash_id": 1172030772312158235, "entities": ["framing patterns", "disinformation campaigns", "multilingual datasets"], "background": "1. The need to understand and detect framing patterns in disinformation campaigns beyond the English-speaking world, particularly in the context of Russian-backed campaigns targeting multiple languages and countries. 2. The underperformance and high disagreement between the two most prominent models for automatic frame analysis, which highlights the need for improved methods suitable for multilingual datasets."}
{"hash_id": 3829907490584284006, "entities": ["blackboxed internal processing", "local modificationbased methods", "nonsvo languages"], "background": "1. The difficulty in directly editing the knowledge within Large Language Models due to their blackboxed internal processing. 2. The limitations of previous local modification-based knowledge editing methods, which assumed SVO language structures and were not applicable to non-SVO languages like Japanese."}
{"hash_id": 142025456053458661, "entities": ["intermediatetask transfer learning", "beneficial tasks", "optimal intermediate task", "nlp tasks", "finetuning"], "background": "1. The need to improve consistency and effectiveness in selecting beneficial tasks for intermediate-task transfer learning in NLP. 2. The high costs and exhaustive process of searching for the optimal intermediate task, especially with the growing array of available NLP tasks and the requirement for fine-tuning."}
{"hash_id": 1995603172917049860, "entities": [], "background": "1. The need to improve the efficiency of automatic summarization models, particularly in handling long sequences without incurring high computational costs. 2. The recognition that long texts have inherent structure (paragraphs, sections) that can be leveraged to enhance summarization performance and reduce the need for extensive computational resources."}
{"hash_id": 747275707930830538, "entities": ["llm safety", "nonenglish languages", "manyshot jailbreaks", "unsafe behaviors", "multilingual safety assessment"], "background": "1. The limited research on large language model (LLM) safety in non-English languages, despite the global adoption of LLMs and the need for multilingual safety assessment. 2. The urgent requirement to understand the escalation of unsafe behaviors in LLMs when exposed to many-shot jailbreaks, given the potential harm such behaviors could cause in real-world applications."}
{"hash_id": 1484035299032114535, "entities": ["multimodal learning", "biomedical data", "clinical decisionmaking", "structural information", "biomedical graph data"], "background": "1. The need to leverage the power of multimodal learning to enhance AI's ability to process and utilize diverse types of biomedical data for improved clinical decision-making. 2. The recognition that existing multimodal LLMs have not fully exploited the structural information present in biomedical graph data, which is crucial for advancing computational biology and clinical research."}
{"hash_id": 339598108003990914, "entities": ["high cost", "expertannotated data", "llms finetuning", "subjective ranking consensus"], "background": "1. The high cost and limited availability of large volumes of expert-annotated data for fine-tuning LLMs. 2. The challenge of achieving consensus on the perfect order of candidate responses due to the subjective nature of ranking."}
{"hash_id": 3360521900582981392, "entities": ["knowledge distillation", "computational demands", "prompt engineering", "opensource models"], "background": "1. The need to reduce computational demands and enhance accessibility for knowledge distillation in LLMs, particularly for average consumers. 2. The requirement for a more adaptable and scalable approach to knowledge distillation that can leverage prompt engineering to improve performance in smaller, open-source models."}
{"hash_id": 2407418202081155399, "entities": ["lowresource domains", "machine reading comprehension", "data annotation automation"], "background": "1. The need to enhance the performance of machine reading comprehension in low-resource domains where annotated data is scarce. 2. The potential to save time, money, and effort by automating the data annotation process using large language models."}
{"hash_id": 6906282906849499320, "entities": ["lowresource languages", "large language models", "impactful applications"], "background": "1. To improve the performance of large language models (LLMs) in low-resource languages such as Thai without the need for extensive and costly datasets. 2. To enable the development of LLMs for low-resource languages that can be applied to impactful applications like medical chatbots, intelligent tutoring systems, and content moderation tools."}
{"hash_id": 7917027674583808125, "entities": ["codeswitching research", "lowresource languages", "synthetic dataset approaches", "intonation", "prosodic aspects"], "background": "1. The need to expand language representation in code-switching research, particularly for low-resource languages, due to the limited number and biased nature of available code-switched datasets. 2. The recognition that previous synthetic dataset approaches based on word replacement do not fully capture the intonation and prosodic aspects of code-switching phenomena."}
{"hash_id": 3158154574893028578, "entities": ["injecting human knowledge", "neural network performance", "output constraints", "learning algorithms", "constraint categorization"], "background": "1. The need to inject human knowledge into neural networks to enhance performance and reduce violations of output constraints. 2. The lack of a unified categorization and analysis of existing learning algorithms that incorporate output constraints."}
{"hash_id": 3778846772876116292, "entities": [], "background": "1. The need to improve sentence embeddings without the requirement for large manually annotated datasets, which are time-consuming and expensive to create. 2. The potential to enhance the performance of decoder-based large language models like PromptEOL in sentence embedding by using automatically generated training data."}
{"hash_id": 3977767527344664677, "entities": ["emergence world model", "language models", "gptstyle model learning", "strategic depth game analysis"], "background": "1. To investigate the emergence of a world model in language models, which could provide insights into their ability to understand and predict complex causal relationships beyond surface-level statistics. 2. To extend the understanding of GPT-style model learning capabilities from board games like Othello and Chess to the game of Checkers, which has a different set of rules and strategic depth."}
{"hash_id": 7673762658909110954, "entities": ["large language models", "symbolic regression", "incontext learning", "taskspecific finetuning", "function discovery process"], "background": "1. The first motivation is to exploit the strong mathematical prior and generalization abilities of Large Language Models (LLMs) to improve the performance and simplicity of symbolic regression, which could lead to better interpretable models and out-of-distribution generalization. 2. The second motivation is to leverage the in-context learning capabilities of LLMs to avoid the need for task-specific fine-tuning and to provide a more flexible and information-rich interface for guiding the function discovery process."}
{"hash_id": 954650950778497318, "entities": ["computational resources", "pretraining", "large language models", "memory requirements", "relora"], "background": "1. The vast amount of computational resources required for pre-training large language models, which is a significant obstacle to their research and development. 2. The need to reduce memory requirements while maintaining comparable performance to traditional pre-training methods, as existing techniques like ReLoRA can degrade performance."}
{"hash_id": 6540346506617757318, "entities": ["fewshot text mining", "controllable data augmentation", "interpretability enhancement"], "background": "1. The need for a controllable data augmentation method that can efficiently find decision boundaries in few-shot text mining tasks, without inducing spurious correlations. 2. The desire to improve the interpretability of data augmentation by directly manipulating user-provided, task-specific attributes rather thanlatent representations."}
{"hash_id": 8686558949424817398, "entities": ["keyphrase extraction methods", "document relevance", "document semantics", "nuanced approach"], "background": "1. Existing keyphrase extraction methods ignore explicitly modeling the relevance between candidate phrases and their corresponding document, leading to less accurate extractions. 2. The complexity of document semantics and the presence of redundant information necessitate a more nuanced approach to capture the importance of candidate keyphrases."}
{"hash_id": 2765571931269126670, "entities": ["large language models", "memory bandwidth efficiency", "symmetric floatingpoint quantization", "sub bit quantization"], "background": "1. The need to improve the efficiency of large language models (LLMs) in terms of memory and bandwidth during deployment, which is crucial for their practical use. 2. The suboptimal performance of existing symmetric floating-point (FP) quantization methods for LLMs, particularly at small group sizes or sub-4 bit quantization, due to the asymmetric distribution of weight tensors."}
{"hash_id": 2855372278622718496, "entities": ["emotional expressions", "intricate relationships", "emotional dynamics", "realworld applications"], "background": "1. The need to capture the intricate relationships between emotion constituents beyond just the \"cause\" to improve the nuanced understanding of emotional expressions in text. 2. The potential to enhance the model's capability to comprehend emotional dynamics and their influence, which is crucial for real-world applications like emotional support and healthcare surveillance."}
{"hash_id": 868241950176437538, "entities": ["catastrophic forgetting", "taskoriented dialogue systems", "exemplar selection strategy"], "background": "1. The need to continuously acquire new knowledge in task-oriented dialogue systems without suffering from catastrophic forgetting, which significantly degrades model performance over time. 2. The requirement for an efficient exemplar selection strategy that considers the influence of training data on model performance, especially in the context of large pre-trained models used in ToDs."}
{"hash_id": 5621143946747105748, "entities": ["impact of noisy data", "language models", "imitation learning", "controlled dataset", "data quality"], "background": "1. The need to quantify the impact of noisy data on language models during imitation learning to understand the potential harm and improve model performance. 2. The requirement for a controlled dataset that can simulate varying levels of data quality to study the effects of noise on large language models."}
{"hash_id": 6057973156119541964, "entities": ["literature review", "assistive approaches", "domain experts", "review process"], "background": "1. The increasing volume of scientific literature makes the process of literature review time-consuming and challenging for researchers. 2. Existing literature review tools tend to focus on automation rather than assistive approaches, which domain experts prefer for maintaining control and accuracy in the review process."}
{"hash_id": 1181370386491874577, "entities": ["automated debate systems", "endtoend process", "cascading errors", "comprehensive dataset", "argument mining summarisation systems"], "background": "1. The need to improve the efficiency and effectiveness of automated debate systems by integrating multiple tasks into an end-to-end process, reducing cascading errors typical of multi-component systems. 2. The lack of a comprehensive dataset that supports the development and evaluation of end-to-end argument mining and summarisation systems, particularly those that align with human preferences and traditional argumentative quality criteria."}
{"hash_id": 4086966355693278533, "entities": ["factually consistent llms", "highquality preference annotations", "model alignment", "expensive human annotations"], "background": "1. The need to reduce factual inconsistency and hallucination in LLMs, which undermine their usability for high-stake applications. 2. The requirement for high-quality preference annotations for model alignment without relying on expensive human annotations or large proprietary models."}
{"hash_id": 3938653360925586936, "entities": ["large language models", "hallucinations", "graph information", "benchmark datasets"], "background": "1. Large language models (LLMs) often suffer from hallucinations due to their parametric memorization of knowledge, and existing methods of augmenting them with individual text units fail to leverage the interconnected knowledge present in graphs. 2. There is a lack of benchmark datasets and methods to evaluate and develop approaches for augmenting LLMs with graph information, which is essential for capturing the structured context of knowledge."}
{"hash_id": 5717291609778006125, "entities": ["information extraction systems", "database integration", "dynamic adaptation"], "background": "1. The need to bridge the gap between the structured knowledge extracted by information extraction (IE) systems and the specific requirements of downstream applications, particularly in terms of database integration. 2. The requirement for IE systems to dynamically adapt to varying database schemas and user instructions to ensure the extracted information is relevant and actionable."}
{"hash_id": 5749123492555710197, "entities": ["computational efficiency", "large language models", "domainspecific pretraining"], "background": "1. The need to enhance computational efficiency in training large language models for specialized domains like biomedicine to make them more accessible and cost-effective. 2. The recognition that domain-specific pre-training can significantly improve model performance on specialized tasks, but the current methods are still computationally intensive."}
{"hash_id": 7934252667719891985, "entities": ["ecologically plausible supervision", "sampleefficient language models", "humanlike language learning"], "background": "1. The need to make language models' representations and predictions more accurate and human-like by incorporating ecologically plausible supervision from other sensory modalities, such as vision. 2. The desire to develop language models that are more sample-efficient and better cognitive models, by aligning with the way humans ground language learning in multiple perceptual signals."}
{"hash_id": 4313343833784071128, "entities": ["tabular data augmentation", "commonsense errors", "feature combination strategies"], "background": "1. The need to overcome the limitations of existing tabular data augmentation methods, such as GANs producing samples with common-sense errors and LLMs' limited capacity to capture disparities between synthetic and actual data distributions. 2. The desire to improve the quality of synthesized tabular data for model training by integrating external knowledge and exploring various feature combination strategies."}
{"hash_id": 3575919541541148068, "entities": ["high cost llm querying", "imperfect llm outputs", "student model learning impact"], "background": "1. The high cost associated with querying large language models (LLMs) for a sufficient number of demonstrations, particularly in scenarios where access is limited or expensive. 2. The potential for imperfect outputs from LLMs, which can negatively impact the learning process of smaller student models."}
{"hash_id": 6708941470232658836, "entities": ["large language models", "finetuning", "incontext learning"], "background": "1. The immense size of large language models like GPT-3 and GPT-4 makes them difficult to fine-tune with common hardware and large-scale supervised data, limiting their adaptability to specific tasks. 2. In-Context Learning (ICL) is restricted by the maximum context length of LLMs, preventing the full utilization of supervised data when numerous examples are available."}
{"hash_id": 8860374481496726926, "entities": ["increased confidence in large language models", "interpretabilityfaithfulness", "inaccessible ground truth"], "background": "1. The risk of increased confidence in Large Language Models due to convincing but potentially incorrect self-explanations. 2. The need for a measure of interpretability-faithfulness when ground truth is inaccessible and models only provide an inference API."}
{"hash_id": 8932954380492589277, "entities": ["gap in datasets", "implicit attribute values", "multimodal dataset"], "background": "1. The need to address the gap in existing datasets that predominantly focus on explicit attribute values, neglecting the more challenging and commonly encountered implicit attribute values in real-world scenarios. 2. The requirement for a publicly available, multimodal dataset that includes product images and covers diverse domains to improve the applicability and accuracy of benchmarks for implicit attribute value extraction."}
{"hash_id": 6337433619701986826, "entities": ["prompt engineering", "metaprompting methods", "complex reasoning", "automatic prompt engineering"], "background": "1. The complexity of prompt engineering requires manual effort and extensive trial-and-error, which is inefficient and time-consuming. 2. Existing meta-prompting methods lack sufficient guidance for complex reasoning, limiting the potential of large language models in automatic prompt engineering."}
{"hash_id": 568006880467033109, "entities": ["neural image classifiers", "spurious correlations", "minority group images", "largescale datasets"], "background": "1. The need to improve the robustness of neural image classifiers against spurious correlations, which lead to poor performance in real-world scenarios with atypical data. 2. The absence of practical methods to manually collect and label minority group images that lack spurious features, especially in large-scale and complex datasets."}
{"hash_id": 2959269237805045155, "entities": ["structured data", "llms effectiveness", "prompting methods"], "background": "1. The limited exploration of LLMs' effectiveness on structured data like tables, despite their widespread use in various applications. 2. The need to understand how different representations and prompting methods can influence the performance of LLMs on table-related tasks."}
{"hash_id": 8710482754103958197, "entities": ["hybrid question answering", "interpretability", "scalable approach", "prompt tokens reduction"], "background": "1. The need to enhance the interpretability and determinism of intermediate reasoning steps in hybrid question answering tasks. 2. The requirement for a scalable approach that can handle massive datasets and reduce the number of prompt tokens in end-to-end systems."}
{"hash_id": 2592582310018910034, "entities": ["computational cost reduction", "environmental footprint", "posttraining quantization", "model quality degradation"], "background": "1. The need to reduce the computational cost and environmental footprint of large language models (LLMs) to make them more accessible for widespread use. 2. The limitations of existing post-training quantization methods at lower bit precision, which result in a significant degradation in model quality."}
{"hash_id": 6158866494013687992, "entities": ["visionlanguage models", "flamingostyle architectures", "computational efficiency"], "background": "1. To improve the efficiency and effectiveness of vision-language models by overcoming the limitations of existing architectures, such as high computational costs and potential information loss. 2. To promote the adoption of Flamingo-style architectures by enhancing their performance through the use of stronger language models and more extensive training data."}
{"hash_id": 1518463333276707954, "entities": ["factually accurate llm answers", "llmgenerated hallucinations", "industry precision", "structured knowledge evaluation"], "background": "1. The need to minimize the factual errors and \"hallucinations\" in LLM-generated answers, which can have severe consequences in industries requiring precision and factual knowledge. 2. The absence of suitable benchmarks and evaluation metrics for attributing LLMs to structured knowledge, limiting the verification and trustworthiness of generated text."}
{"hash_id": 6475932394310839997, "entities": ["taskspecific benchmarks", "cognitive biases", "machine preferences alignment"], "background": "1. The current task-specific benchmarks are insufficient to measure the quality of generated texts in the wild, and LLMs exhibit cognitive biases that compromise their role as unbiased evaluators. 2. There is a need to align machine preferences with human judgment to improve the reliability of automatic evaluation by LLMs."}
{"hash_id": 2111103377409470674, "entities": ["lowresource language performance", "instruction tuning", "translation errors"], "background": "1. The need to improve the performance of large language models in low-resource languages, where they currently struggle due to a lack of high-quality instruction following data. 2. The requirement to overcome the limitations of direct translation of English instruction tuning samples, which can introduce translation errors and ignore language-specific knowledge and cultural nuances."}
{"hash_id": 3043196170878927200, "entities": ["emotional support", "conversation models", "unhelpful responses"], "background": "1. The need to consider multiple facets of emotional support when developing conversation models to ensure helpfulness. 2. The necessity of directly reducing the probability of generating unhelpful responses to improve the effectiveness of emotional support conversations."}
{"hash_id": 5851921699801987841, "entities": ["improving llm generalization", "tstl scenarios", "outofdistribution positions"], "background": "1. The need to improve the generalization of LLMs in train-short-test-long (TSTL) scenarios, where models struggle with out-of-distribution (OOD) token positions in longer sequences during testing. 2. The recognition that existing methods focus on minimizing extrapolation of position embeddings but neglect the equally important issue of interpolation at OOD positions."}
{"hash_id": 70409764602451899, "entities": ["unlock intrinsic medical knowledge", "enhance reasoning proficiency", "multiagent collaboration", "optimize collective intelligence"], "background": "1. The need to unlock the intrinsic medical knowledge embedded within large language models and enhance their reasoning proficiency without additional training. 2. The success of multi-agent collaboration in optimizing collective intelligence and simulating human activities, which can be applied to the medical domain for improved decision-making."}
{"hash_id": 5812129229730439399, "entities": ["symbolic methods", "human reasoning habits", "learning efficiency", "finite symbolic systems"], "background": "1. The need to broaden the applicability and adaptability of symbolic methods in the real world by aligning them with human reasoning habits. 2. The requirement to enhance learning efficiency and reasoning accuracy in large language models by transitioning from infinite semantics systems to finite symbolic systems."}
{"hash_id": 5404032640676870122, "entities": ["pretraining data legal compliance", "practical detection method", "blackbox llm services"], "background": "1. The need to ensure that pre-training data used in large language models is legal and does not infringe on copyrights or contain personal information. 2. The lack of a practical method to detect pre-training data in LLMs due to the black-box nature of existing LLM services and the infeasibility of using output probabilities for detection."}
{"hash_id": 1752245288763139156, "entities": ["instruction following", "incontext examples", "incontext instruction tuning"], "background": "1. The need to enhance the instruction following ability of large language models, especially when dealing with in-context examples. 2. The desire to improve upon existing in-context instruction tuning methods by making the model actively engage with and learn from examples, rather than passively reading them."}
{"hash_id": 4910147565418494476, "entities": ["improve correction fidelity", "llms awareness", "input redundancy reduction", "generative error correction", "nbest hypotheses miscorrections"], "background": "1. The need to improve the fidelity of correction output by making LLMs aware of the source speech during the generative error correction process. 2. The desire to reduce input information redundancy in N-best hypotheses, which could confuse LLMs and lead to increased miscorrections."}
{"hash_id": 1072260963458353821, "entities": ["nonbridge entities", "prediction bias", "na instances", "model learning"], "background": "1. The first motivation is to leverage the semantic information provided by non-bridge entities, which are currently ignored by existing models but could potentially enhance the prediction of relations between target entities. 2. The second motivation is to mitigate the prediction bias caused by the high proportion of NA instances in the training dataset, which hinders the model's ability to learn non-NA relations effectively."}
{"hash_id": 6437704314635350606, "entities": ["imagesharing behavior", "dialogue context", "emergent imagesharing ability", "llms zeroshot setting"], "background": "1. To overcome the limitations of existing studies that oversimplify the complexity of image-sharing behavior by reducing it to a binary decision and fail to fully understand dialogue context. 2. To explore and validate the emergent image-sharing ability of LLMs in a zero-shot setting, which is crucial for enhancing human-bot interaction and dataset augmentation."}
{"hash_id": 4899136938277486178, "entities": ["high cost training", "downstream scenarios", "versatility code llms"], "background": "1. The high cost and effort required to construct and train instruction data for all possible downstream scenarios in code large language models. 2. The need to improve the versatility of code LLMs while reducing the amount of training data required to achieve this."}
{"hash_id": 4333492240054734560, "entities": ["factuality in chart captions", "factual errors", "correction methods"], "background": "1. The urgent need to mitigate factual errors in chart captions generated by large vision-language models, which are crucial for decision-making and reporting in various domains. 2. The absence of a comprehensive study on the factuality of generated chart captions and the lack of methods specifically designed to correct these errors."}
{"hash_id": 391839042288493593, "entities": ["improve answer quality", "address inconsistencies", "enhance retrievalaugmented llms"], "background": "1. The need to improve the quality of answers generated by large language models in knowledge-intensive tasks by addressing inconsistencies between retrieved knowledge and the essential knowledge required by the models. 2. The requirement to reduce the noise and length of retrieved documents to enhance the efficiency and effectiveness of retrieval-augmented LLMs."}
{"hash_id": 2458849928420701103, "entities": ["intentiondriven expressions", "realworld scenarios", "multiscene environments", "egocentric perspectives", "visual grounding"], "background": "1. The need to interpret intention-driven expressions rather than literal descriptions to better align with human intentions in real-world scenarios. 2. The requirement for a dataset and methods that can handle diverse multi-scene environments and egocentric perspectives to enhance the practical application of visual grounding."}
{"hash_id": 2412116675737242542, "entities": ["continuous learning", "sequence labeling", "catastrophic forgetting", "eo oe problems"], "background": "1. The need to continuously learn new classes in sequence labeling without suffering from catastrophic forgetting, which leads to mislabeling of old entities as non-entities (E2O) or incorrectly labeling non-entities or old entities as new entities (O2E). 2. The absence of a comprehensive solution that addresses both E2O and O2E problems, with existing methods primarily focusing on E2O and neglecting the bias towards new classes."}
{"hash_id": 5898540584545402027, "entities": ["language proficiency", "kbqa scenarios", "formal language understanding"], "background": "1. To understand the varying levels of proficiency that large language models have in different formal languages, which is crucial for choosing appropriate models and formal languages for specific KBQA scenarios. 2. To determine the upper limits of LLMs' capabilities in formal language understanding and generation, which can inform the development of better reasoning approaches based on LLMs."}
{"hash_id": 934383333882329217, "entities": ["humanannotated distractor labels", "reading comprehension tasks", "resourceefficient approach"], "background": "1. The need to reduce the dependency on expensive human-annotated distractor labels in reading comprehension tasks. 2. The requirement for a more resource-efficient approach that can generate high-quality distractors without the need for large-scale language models, which are computationally demanding and often closed-source."}
{"hash_id": 4053534312098086401, "entities": ["conversational question answering", "multiturn questions", "large language models", "context and intent"], "background": "1. The need to improve the performance of conversational question answering over knowledge graphs, specifically when dealing with multi-turn questions that contain inexplicit information. 2. The desire to leverage the power of large language models to generate reformulations that can better capture the context and intent of the questions, bridging the gap between machine interpretation and human understanding."}
{"hash_id": 2726442155040807192, "entities": ["code generation", "large language models", "debugging capabilities", "human debugging techniques"], "background": "1. The current approaches in code generation using large language models lack effective debugging capabilities, especially when dealing with complex logic flows and data operations. 2. Human debugging techniques, such as setting breakpoints and examining runtime execution information, are underutilized in existing literature and could significantly enhance the refinement process of generated code."}
{"hash_id": 6666297387094660539, "entities": ["humorous sentence generation", "pun generation", "preference learning methods", "multiobjective alignment"], "background": "1. The limited ability of large language models (LLMs) to generate humorous sentences and puns, which are important for various human interactions and learning. 2. The need to improve the efficiency of preference learning methods in pun generation by addressing the challenge of multi-objective alignment."}
{"hash_id": 79365414266056649, "entities": ["domainspecific qa", "userfriendly responses", "llm preferences alignment"], "background": "1. The need to ensure that LLM responses in domain-specific QA are both user-friendly and effectively leverage domain-specific knowledge bases. 2. The challenge of aligning LLM preferences with human preferences in real-world, domain-specific scenarios to improve the quality of automated QA services."}
{"hash_id": 335369489051257849, "entities": ["mathematical reasoning", "artificial general intelligence", "common sense", "math word problems"], "background": "1. Large language models (LLMs) have not yet achieved strong mathematical reasoning abilities, which are essential for advancing towards true artificial general intelligence. 2. Existing methods struggle to capture the precise nature of mathematical reasoning, often missing common sense in math word problems, and lack reproducibility in their pipelines."}
{"hash_id": 5274030573732210436, "entities": ["labor expenses reduction", "social media poll automation", "internet water armies", "diverse viewpoints inclusion"], "background": "1. To reduce labor expenses and enhance productivity by automating poll generation on social media platforms. 2. To address the issue of biased polls caused by internet water armies, ensuring the inclusion of diverse and minority viewpoints."}
{"hash_id": 8814232537757399821, "entities": ["multistep reasoning", "mathematical contexts", "abstract mathematical extrapolation"], "background": "1. Large Language Models (LLMs) struggle with complex multi-step reasoning, particularly in mathematical contexts, due to a lack of high-quality data and the challenge of extrapolating reasoning skills to unseen problems. 2. Existing mathematical reasoning benchmarks and models have not adequately addressed the issue of abstract mathematical extrapolation, which is crucial for real-world application of LLMs."}
{"hash_id": 3017193268541102736, "entities": ["small language models", "toxicity detection", "large language models", "embedded biases"], "background": "1. The need to overcome the limited knowledge of small language models (SLMs) in toxicity detection, which results in suboptimal performance. 2. The requirement to mitigate the potential biases embedded in large language models (LLMs) that could lead to incorrect predictions and new safety issues."}
{"hash_id": 5763713952382197988, "entities": ["prompt compression", "large language models", "taskagnostic methods", "endtoend latency", "computational costs"], "background": "1. The need to improve the efficiency and generalizability of prompt compression for large language models by overcoming the limitations of existing task-agnostic methods that rely on suboptimal metrics like information entropy and unidirectional context. 2. The requirement to accelerate the end-to-end latency of language models and reduce computational costs while maintaining the faithfulness of the compressed prompt to the original content."}
{"hash_id": 4592486669609679563, "entities": ["versatile modelagnostic approach", "clinical text summarization", "realworld clinical scenarios"], "background": "1. The need for a versatile and model-agnostic approach to mitigate hallucinations in clinical text summarization without incurring significant computational costs. 2. The requirement to improve the applicability of existing methods to real-world clinical scenarios, specifically by addressing the issue of hallucinations that can mislead clinicians and patients."}
{"hash_id": 1753612060723005563, "entities": ["multimodal llms", "crossmodal representations", "interpretability", "identifying multimodal neurons", "costly gradient computation"], "background": "1. The need to understand how multi-modal LLMs interpret different modalities and integrate cross-modal representations for improved interpretability and application safety. 2. The inefficiency and limited applied range of current methods for identifying multi-modal neurons due to the requirement of costly gradient computation."}
{"hash_id": 6688557781035956205, "entities": ["controllable text generation", "flexibility", "control granularity", "nonintrusive method"], "background": "1. The need to improve the flexibility and control granularity of controllable text generation without compromising generation efficiency. 2. The desire to develop a method that can control the generation process in a non-intrusive manner, not disturbing the original generative stream of large-scale causal language models."}
{"hash_id": 7131670427137139555, "entities": ["length control", "text generation", "gptstyle models", "user expectations", "diverse user instructions"], "background": "1. The need for more accurate length control in text generation to match user expectations and improve efficiency in GPT-style models. 2. The absence of methods that can handle multiple types of length control beyond simple equality, and the challenge of parsing diverse user instructions."}
{"hash_id": 3365341134252183352, "entities": ["user preferences", "conversational recommendation", "personalization", "explanations", "domain knowledge"], "background": "1. The need for more specific user preferences in conversational recommendation datasets to enhance the personalization of recommendations. 2. The requirement for improved explanations and domain knowledge in recommendations to aid users in understanding the rationale behind suggestions."}
{"hash_id": 6570693984021480718, "entities": ["objectlevel image understanding", "vision language models", "instruction tuning"], "background": "1. The need to improve the object-level image understanding capabilities of Vision Language Models (VLMs) to enhance their zero-shot performance on vision language tasks. 2. The desire to develop a model that can preserve object-level image understanding during instruction tuning without forgetting this knowledge."}
{"hash_id": 208291812651018792, "entities": ["human evaluation variability", "fairness inclusivity", "scalable annotation alternative"], "background": "1. The need to capture the variability present in human evaluation to avoid biases and overrepresentation, ensuring fairness and inclusivity in simulated annotations. 2. The requirement for a cost-effective and scalable alternative to human annotators for large-scale dataset evaluation and system comparisons."}
{"hash_id": 3832125387775388830, "entities": ["commonsense reasoning", "reporting bias", "visual data", "language models capabilities"], "background": "1. Language models struggle with commonsense reasoning due to the reporting bias, where commonsense information is underrepresented in text compared to its actual existence. 2. Visual data inherently captures commonsense knowledge and can be a complementary source to text for enhancing language models' commonsense capabilities."}
{"hash_id": 7243204042119520253, "entities": ["knowledge conflicts", "internal memory", "external context", "text generation reliability"], "background": "1. The need to understand and mitigate the mechanism of knowledge conflicts that arise from the clash between internal memory and external context in language models. 2. The desire to enhance the reliability and faithfulness of language models in generating text by precisely controlling their use of internal memory and external context."}
{"hash_id": 5455353605347651917, "entities": ["molecular structures", "iupac names", "generalized model", "multiple tasks"], "background": "1. The need to enhance the understanding of molecular structures through the integration of IUPAC names, which are widely used in scientific literature but not fully exploited by previous models. 2. The requirement for a more generalized model that can perform multiple tasks without the need for task-specific training, reducing costs and increasing efficiency."}
{"hash_id": 2311193962765670618, "entities": ["high computational cost", "parameterefficient finetuning", "oversmoothing problem"], "background": "1. The high computational cost and time required for fine-tuning all parameters of large language models (LLMs) make it impractical for many applications. 2. Existing parameter-efficient fine-tuning (PEFT) methods are not designed to address the over-smoothing problem, which hampers the performance of deep Transformer-based LLMs."}
{"hash_id": 5581698867078162974, "entities": ["standardized benchmark", "geometry math problems", "llms mms performance"], "background": "1. The lack of a standardized and diverse benchmark to thoroughly evaluate the abilities of LLMs and MMs in solving geometry math problems, which require an integrated understanding of textual and visual information. 2. The need to assess the performance of LLMs and MMs on geometry problems they have not been pre-trained on to avoid overfitting and to gain insights into their genuine problem-solving capabilities."}
{"hash_id": 3761921693004035541, "entities": ["improving ner disambiguation", "contextual image information", "multimodal ner corpora", "noisy entity coverage"], "background": "1. The need to improve the disambiguation of entities in NER tasks by incorporating contextual information from images, which is not fully realized by existing text-based methods. 2. The challenge of acquiring large-scale and high-quality text-image paired corpora for multimodal NER (MNER) tasks, and the issue of noisy or insufficient entity coverage in crawled images."}
{"hash_id": 1500608388559424030, "entities": ["neural language models", "interpretability", "computational barrier", "lightweight methods"], "background": "1. The need to interpret the functions of neurons within large language models, especially as models scale up and become integral to various applications, which increases the importance of understanding their internal mechanisms. 2. The high computational barrier for interpretability researchers to analyze large models, which necessitates lightweight methods that can be performed with limited resources."}
{"hash_id": 1411941515458177328, "entities": ["gmner methods", "mner performance", "visual object features", "object detection techniques", "coarsegrained referring expressions", "finegrained named entities", "ungroundable text inputs", "imagetext correlations"], "background": "1. The existing GMNER methods suffer from suboptimal MNER performance due to the introduction of visual object features, and they are limited by the unsatisfactory performance of object detection techniques in recognizing candidate visual objects. 2. There is a need to bridge the gap between coarse-grained referring expressions used in similar tasks and fine-grained named entities, as well as to address the issue of ungroundable text inputs due to weak image-text correlations in social media."}
{"hash_id": 702470834926969323, "entities": ["job title representation", "skillbased approaches", "job descriptions", "learning process"], "background": "1. The need to improve the accuracy of job title representation without the limitations of skill-based approaches, which can be error-prone and require extensive manual updates. 2. The desire to leverage the rich and diverse content of job descriptions directly to enhance the learning process, instead of relying on extracted skills that might miss important nuances."}
{"hash_id": 4194469619397589997, "entities": ["knowledge distillation", "catastrophic forgetting", "modelagnostic strategies", "continual learning scenarios"], "background": "1. The limitations of existing CNER methods that rely solely on knowledge distillation, which still result in catastrophic forgetting of old entity types. 2. The need for model-agnostic strategies that can be seamlessly integrated with existing state-of-the-art models to improve their performance in continual learning scenarios."}
{"hash_id": 6669464109103758164, "entities": ["nlg evaluation metrics", "adversarial attacks", "automated scalable method"], "background": "1. The robustness of existing NLG evaluation metrics against adversarial attacks is critically under-explored, and traditional adversarial attack techniques are not suitable for NLG evaluation tasks. 2. There is a need for an automated and scalable method to generate high-quality adversarial data that can expose the weaknesses in well-established automatic evaluation metrics for NLG."}
{"hash_id": 2458263447893395382, "entities": ["computational inefficiency", "debiasing techniques", "large language models", "permutation invariance"], "background": "1. To overcome the computational inefficiency at inference time caused by existing debiasing techniques for Large Language Models. 2. To ensure that LLMs maintain invariances, such as permutation invariance, which are crucial for certain tasks to improve performance and reliability."}
{"hash_id": 5362822848725596782, "entities": ["evaluation methods", "llms capabilities", "table information seeking", "pseudorelevant tables"], "background": "1. The lack of thorough and reliable evaluation methods for assessing LLMs' capabilities in Table Information Seeking (TIS). 2. The need to improve LLMs' understanding of table structures and their ability to handle pseudo-relevant tables, which are common in retrieval-augmented systems."}
{"hash_id": 3518421318566827974, "entities": ["privacy security issues", "training data", "sensitive domains", "rehearsalfree methods", "relation extraction", "pretrained language model"], "background": "1. The need to address privacy and security issues by avoiding the storage of training data, which is particularly important in sensitive domains such as finance and biology. 2. The unsatisfactory performance of existing rehearsal-free methods in task identification for relation extraction, due to the reliance on a single pre-trained language model which may not possess sufficient task-related knowledge."}
{"hash_id": 7932326355858584961, "entities": ["contextual information", "temporal validity", "temporal commonsense reasoning", "large language models"], "background": "1. The need to account for contextual information that can alter the temporal validity of statements, which is crucial for various applications like recommender systems, conversational AI, and user status tracking. 2. The current underperformance of large language models in temporal commonsense reasoning, particularly when dealing with underspecified temporal expressions in a larger content stream."}
{"hash_id": 8395956096769642016, "entities": ["improving finetuning performance", "pretrained language models", "plm control techniques"], "background": "1. The need to improve the performance of fine-tuning large pre-trained language models without the computational expense of training all parameters end-to-end. 2. The recognition that altering the input text can influence the model's output and that this has not been fully exploited in current PLM control techniques."}
{"hash_id": 9000786427833377367, "entities": ["commonsense properties", "concept embeddings", "inductive generalization"], "background": "1. The need to capture a more diverse range of commonsense properties in concept embeddings for improved inductive generalization in knowledge engineering tasks. 2. The limitation of traditional concept embeddings in reflecting only basic taxonomic categories, which hinders the identification of more specific commonalities."}
{"hash_id": 270136182939034934, "entities": ["highdimensional encoding schemes", "attention mechanisms", "multimodal data context"], "background": "1. The need for appropriate encoding schemes for numerical features in high-dimensional settings and the importance of expressing the ordered and various distributions of these features. 2. The relevance of considering the whole context, rather than partial contexts, in attention mechanisms when dealing with multimodal data."}
{"hash_id": 3625267010363949346, "entities": ["temporal relations", "tre systems", "unified framework"], "background": "1. The current TRE systems struggle with understanding the structured meaning of temporal relations and the temporal calculus governing them, leading to poor performance. 2. Innovations in TRE are not being reused effectively due to the lack of a unified framework that can integrate various techniques."}
{"hash_id": 2048435970484539897, "entities": ["annotation artifacts", "faithdial benchmark", "conversation history", "comprehensive evaluation method", "hallucinations adherence"], "background": "1. The existence of annotation artifacts in the FaithDial benchmark that bias models towards ignoring conversation history, leading to poor performance in real-world dialogues. 2. The need for a more comprehensive evaluation method that can accurately measure both hallucinations and a model's adherence to the conversation task."}
{"hash_id": 2384300379682290839, "entities": ["critical reasoning skills", "comprehensive benchmark", "inconsistent findings"], "background": "1. The lack of a comprehensive benchmark to systematically evaluate LLMs' critical reasoning skills, which are essential for self-evaluation, feedback provision, and self-improvement. 2. Inconsistent findings from prior research due to a narrow focus on models and datasets, which hinders the understanding of LLMs' critique and correction capabilities."}
{"hash_id": 6068857077609642711, "entities": ["fact rectification", "hallucination generation", "semantic interaction", "coldstart parameters"], "background": "1. The need to rectify factual mistakes continuously in large language models, as they\u503e\u5411 to generate hallucinations, leading to factually unsupported content. 2. The lack of attention to the semantic interaction and mutual influence among sequentially input facts in existing model editing methods, as well as the need to address the issue of cold-start parameters in auxiliary networks."}
{"hash_id": 9067388484390475530, "entities": ["text summarization", "user objectives", "controllable text summarization"], "background": "1. The need for text summarization approaches that can be tailored to meet specific user objectives and preferences. 2. The absence of a comprehensive survey that thoroughly explores the diverse controllable attributes and challenges in the field of Controllable Text Summarization (CTS)."}
{"hash_id": 1027188978474549893, "entities": ["communication skills", "nlp techniques", "patient care", "large language models", "healthcare education"], "background": "1. The need to enhance the communication skills of medical learners, which is critical for better patient care and professional development, through the use of advanced NLP techniques. 2. The absence of publicly available datasets and effective systems for communicative medical coaching that leverage Large Language Models (LLMs), which could potentially fill a significant gap in healthcare education."}
{"hash_id": 6865467451512380881, "entities": ["diverse heterogeneous knowledge sources", "opendomain question answering", "exactmatch metric inadequacy"], "background": "1. The need to leverage diverse and heterogeneous knowledge sources for more comprehensive open-domain question answering. 2. The inadequacy of the exact-match metric for evaluating the accuracy of LLM-based QA systems."}
{"hash_id": 5546546631415491985, "entities": ["improve model performance", "costly data collection", "artificial general intelligence", "data efficiency", "learning paradigms"], "background": "1. The need to improve model performance without the costly and time-consuming process of collecting more high-quality human-annotated data. 2. The potential to advance towards Artificial General Intelligence (AGI) by increasing data efficiency and exploring innovative learning paradigms."}
{"hash_id": 4796781381895892084, "entities": ["robust watermarking technique", "paraphrase attacks", "detection efficiency", "semstamp method", "semantic space partitioning"], "background": "1. The need for a more robust watermarking technique that can withstand paraphrase attacks and maintain detection efficiency in machine-generated text. 2. The desire to improve upon the arbitrary partitioning of semantic space by LSH in the existing SEMSTAMP method, which could separate semantically similar sentences."}
{"hash_id": 496580068388689716, "entities": ["smallscale llms", "efficacy improvement", "prompt engineering strategies", "computational costs", "resource usage"], "background": "1. The need to improve the efficacy of small-scale LLMs as optimizers, as current methods like OPRO show limited effectiveness in these models. 2. The requirement to develop prompt engineering strategies that consider the computational costs and capabilities of small-scale LLMs to enhance their performance without excessive resource usage."}
{"hash_id": 1342269129832911079, "entities": ["inference latency", "pretrained language models", "domain shifts", "zeroshot setting"], "background": "1. The need to reduce inference latency in large-scale pre-trained language models like BERT without significant loss in accuracy, as these models are computationally expensive and inefficient for real-world applications. 2. The requirement for a method that can adapt to domain shifts in a zero-shot setting, where the distribution of the input data changes, and traditional early exit methods may become less efficient."}
{"hash_id": 3823071388087786645, "entities": ["adversarial manipulation", "unsafe responses", "unintended hallucinations", "safety alignment"], "background": "1. The need to prevent adversarial actors from manipulating large language models into generating unsafe responses. 2. The requirement to overcome the challenge of unintended hallucinations caused by excessive safety alignment in language models."}
{"hash_id": 7250393949757559754, "entities": ["training data quality", "lowquality translation", "selection bias", "multilingual models performance"], "background": "1. The need to improve the quality of training data for multilingual models, which is currently compromised by the prevalence of low-quality, machine-translated content on the web. 2. The concern that the selection bias in the type of content being translated en masse into lower resource languages could negatively impact the performance and reliability of multilingual models."}
{"hash_id": 1191242773211143501, "entities": ["finetuning large language models", "computational costs", "multitaskperforming model", "inference time preservation"], "background": "1. The high computational costs and resource demands of fine-tuning large language models on multiple tasks. 2. The need to leverage existing fine-tuned models without additional training to create a multi-taskperforming model, thus preserving inference time and resources."}
{"hash_id": 1365173628455623851, "entities": ["evaluation framework", "visionlanguage models", "hallucinations", "open vocabulary settings"], "background": "1. The need to comprehensively evaluate large vision-language models for hallucinations beyond just object existence, including attributes and relations, to ensure reliability and trustworthiness. 2. The requirement for an evaluation framework that can handle open vocabulary settings and consider both faithfulness and coverage to avoid favoring models that produce precise but uninformative outputs."}
{"hash_id": 1165369153623131518, "entities": ["zeroshot crosslingual slu", "consistency", "contrastive learning", "label indiscrimination", "representation modeling"], "background": "1. The need to improve consistency between different languages in zero-shot cross-lingual SLU, as existing methods do not effectively establish this consistency, affecting knowledge transfer. 2. The requirement to address the issue of indiscriminately pushing away utterances with only one different label, which hampers the representation modeling of contrastive learning."}
{"hash_id": 4376275103454232598, "entities": ["harmful content prevention", "computational efficiency", "reinforcement learning from human feedback"], "background": "1. The need to prevent Large Language Models from generating harmful content while ensuring they maintain utility in responding to normal prompts. 2. The desire for a more computationally efficient and easier-to-implement approach than existing methods such as Reinforcement Learning from Human Feedback (RLHF)."}
{"hash_id": 5192664516948937359, "entities": ["chain of thought prompts", "reasoning step length", "large language models"], "background": "1. To investigate the\u672a\u77e5effect of reasoning step length on the performance of large language models using Chain of Thought (CoT) prompts. 2. To provide practical guidance on how to optimize the use of LLMs in complex problem-solving scenarios by manipulating the number of reasoning steps in prompts."}
{"hash_id": 6898920942326956319, "entities": ["taskagnostic debiasing", "social biases", "plm parameter changes", "debiasing effectiveness"], "background": "1. To mitigate the risk of relearning social biases from downstream task-specific data after task-agnostic debiasing of PLMs. 2. To understand the impact of PLM parameter changes on debiasing effectiveness and downstream generalization."}
{"hash_id": 5561890977769547532, "entities": ["automatic summarization", "structured knowledge", "document types"], "background": "1. The need to improve the quality of automatic summarization by incorporating structured knowledge that reflects the inherent structures of different genres of texts. 2. The requirement to overcome the limitations of existing methods that rely on predefined structure labels or fail to capture the diversity of summary points across various document types."}
{"hash_id": 4539856790630685437, "entities": ["chinese spoken ner datasets", "realworld scenarios", "asr systems errorproneness"], "background": "1. The current Chinese Spoken NER datasets do not reflect real-world scenarios, leading to a performance gap between research and practical applications. 2. Existing Spoken NER models are not specifically tailored for real-world scenarios and struggle with the error-prone outputs from Automatic Speech Recognition (ASR) systems."}
{"hash_id": 3159447984395383076, "entities": ["singleagent llmbased evaluators", "nlg evaluation", "human alignment"], "background": "1. The need to overcome the inherent biases and limitations of single-agent LLM-based evaluators in NLG evaluation, which can lead to suboptimal performance and inconsistent scoring. 2. The requirement for an NLG evaluation framework that can align better with human-generated responses and improve the correlation with human ratings without heavy reliance on costly annotated data."}
{"hash_id": 4368908196802626769, "entities": ["traditional documentlevel event extraction", "fragmented information", "comprehensive perspective", "event information aggregation"], "background": "1. The limitations of traditional document-level event extraction in providing a complete representation of events due to fragmented and sometimes biased information across multiple documents. 2. The need to aggregate and integrate event information from different sources to obtain a comprehensive perspective on events."}
{"hash_id": 4005755895940993073, "entities": ["harness structural information", "unified framework", "interaction mechanisms", "collective insights", "limited labeled data"], "background": "1. The need to harness the structural information of molecular graphs that is not fully exploited by current LLM-based MRL methods. 2. The requirement for a unified framework that allows for the sharing and integration of interaction mechanisms learned across different datasets to improve the collective insights and prevent overfitting in tasks with limited labeled data."}
{"hash_id": 9136814586198554551, "entities": ["keyphrase systems evaluation", "semantically equivalent phrases", "phrase semantics complexity"], "background": "1. The reliance on exact matching for evaluating keyphrase systems fails to recognize semantically equivalent keyphrases and diverse keyphrases that have practical utility in applications like information retrieval. 2. Existing evaluation metrics and heuristics have not been validated by systematic meta-evaluation and do not accurately capture the complexity of phrase semantics."}
{"hash_id": 6847636935220753729, "entities": ["mitigate overfitting", "parameterefficient finetuning", "dropout methods for transformers"], "background": "1. The need to mitigate overfitting in large language models, especially when using parameter-efficient fine-tuning methods like LoRA, which have limited trainable parameters. 2. The absence of a unified framework to compare and understand the effectiveness of different dropout methods specifically tailored for transformers in the context of fine-tuning."}
{"hash_id": 6091861852084014497, "entities": ["large language models", "grounding knowledge", "hallucinations", "knowledge misalignment"], "background": "1. Large language models often ignore correct groundings and rely on biases or wrong information, leading to hallucinations and unreliable answers. 2. There is a common misalignment between human questions and the grounding knowledge in external databases, causing the models to fail in providing accurate answers."}
{"hash_id": 4034773203505171049, "entities": ["inefficiencies in knowledge retrieval", "kbqa tasks", "large language models integration"], "background": "1. To overcome the inefficiencies and mistakes in knowledge retrieval that negatively affect semantic parsing in KBQA tasks. 2. To simplify the complexity of previous KBQA methods by integrating large language models with knowledge graphs for improved interpretability and performance."}
{"hash_id": 8440136870736114327, "entities": ["lowresource settings", "neural network evaluation", "spurious correlation bias"], "background": "1. The need to accurately evaluate the learning ability of neural networks in low-resource settings, as current benchmarks may over-estimate performance due to the use of random or manual selection methods that do not capture real-world challenges. 2. The recognition that existing models, even pre-trained ones, still face significant challenges in areas like spurious correlation and bias, which are less common in human learning."}
{"hash_id": 3921738284936393293, "entities": ["code generation repair", "large language models", "degenerationofthought problem"], "background": "1. The need to improve the code generation and repair abilities of Large Language Models, as they struggle to produce entirely correct code, even in the hands of proficient programmers. 2. The requirement to alleviate the Degeneration-of-Thought problem that arises when models rely too heavily on their self-evaluation capabilities, leading to overlooked bugs due to cognitive inertia."}
{"hash_id": 2755442305650720110, "entities": ["social intelligence", "conversational agents", "group dynamics"], "background": "1. The lack of a standardized benchmark to evaluate the social intelligence of role-playing conversational agents, which is crucial for understanding their behavior in social environments. 2. The need to assess these agents not only at the individual level but also within the context of group dynamics, as individual performance does not necessarily reflect group interaction capabilities."}
{"hash_id": 2604554511331914777, "entities": ["effective prompts", "large language models", "prompt optimization methods"], "background": "1. The slow progress in discovering effective prompts for large language models, which hinders their full potential in various applications. 2. The lack of general prompt optimization methods that are automatic, discrete, black-box, gradient-free, and interpretable."}
{"hash_id": 1174173767797723438, "entities": ["assessment of language models", "decisionmaking under incomplete information", "adaptive strategies"], "background": "1. The need to assess large language models' ability to handle incomplete information, which is common in real-world scenarios, and enhance their robustness and quality of decision-making. 2. The lack of existing games or evaluation methods that adequately address the complexities of decision-making under incomplete information, including recognizing misleading cues and formulating adaptive strategies."}
{"hash_id": 6075827492374236685, "entities": ["knowledge distillation", "outofdistribution data", "natural language inference"], "background": "1. The need to enhance the robustness of student models trained via knowledge distillation when faced with out-of-distribution data. 2. The desire to improve generalization beyond the target domain, particularly in the context of natural language inference tasks where previous methods have shown limited success."}
{"hash_id": 5993083409411534292, "entities": ["tokenization compression", "tokenization quality", "intrinsic indicator"], "background": "1. The need to theoretically and empirically establish the importance of compression in tokenization, which is crucial for pre-trained language models. 2. The requirement for an intrinsic indicator of tokenization quality that can be measured without the need for expensive large-scale model pre-training."}
{"hash_id": 7493934693349912874, "entities": ["oversegmentation mitigation", "korean machine translation", "subwordunit segmentation", "morphological richness"], "background": "1. The need to mitigate over-segmentation in Korean machine translation, which erodes word semantics and causes semantic confusion during training, leading to poor translation quality, especially in domain shift scenarios. 2. The observation that existing subword-unit segmentation methods like BPE do not adequately account for the morphological richness of Korean, resulting in the loss of meaning and increased semantic ambiguity."}
{"hash_id": 6637245496045867681, "entities": ["global applicability", "large language models", "instruction tuning"], "background": "1. The need to enhance the global applicability of large language models by improving their ability to follow instructions in multiple languages. 2. The challenge of curating naturally occurring instructions and responses for every language, which prompts the search for more efficient and resource-light methods of instruction tuning."}
{"hash_id": 8239741842143003861, "entities": ["versatile embedding model", "multilinguality", "semantic retrieval", "realworld ir systems"], "background": "1. The need for a versatile embedding model that can support multi-linguality, multi-functionality, and multi-granularity to enhance the semantic retrieval capabilities across various languages and information retrieval tasks. 2. The limitation of existing embedding models in terms of language coverage, functionality, and their ability to handle long documents, which hinders their effectiveness in real-world IR systems."}
{"hash_id": 6861037924937588632, "entities": ["improving llmgenerated code", "realworld software projects", "projectspecific context", "integrating code generation", "manual prompt engineering"], "background": "1. The need to improve the accuracy and reliability of LLM-generated code in real-world software projects where project-specific context is crucial. 2. The desire to bridge the gap between the capabilities of LLMs and the practical requirements of integrating code generation into existing codebases without extensive manual prompt engineering."}
{"hash_id": 4181976426993218978, "entities": ["align models", "social media tasks", "cognitive abilities"], "background": "1. The need to align models with the unique speaking style and context of social media tasks, given the prevalent informal language and multimodal nature of the content. 2. The requirement to enhance cognitive abilities in models to address complex social media tasks that involve multiple objectives and levels of information processing."}
{"hash_id": 7553519924315053331, "entities": ["commonsense reasoning", "korean language", "sociocultural knowledge", "benchmark datasets", "llms performance"], "background": "1. The need to improve thecommonsense reasoning capabilities of large language models, particularly in the context of Korean language and sociocultural knowledge. 2. The recognition of the limitations of existing benchmark datasets in evaluating the performance of LLMs in socioculturally sensitive scenarios."}
{"hash_id": 9191243401735378609, "entities": ["computational cost reduction", "transformerbased language models", "sparsity in model architecture"], "background": "1. The need to reduce the high computational cost associated with training and inference in Transformer-based Language Models. 2. The desire to explore the potential of sparsity in model architecture, particularly through the lens of network topology, to improve efficiency without significant performance trade-offs."}
{"hash_id": 6139154119778644441, "entities": ["evaluation methods", "large language models", "lowresource mechanism"], "background": "1. The current evaluation methods for ranking large language models are either expensive due to the need for human responses or unreliable due to the use of model pairs to evaluate each other. 2. There is a need for a low-resource mechanism to rank LLMs that does not rely on ground truth or reference responses, especially for generative tasks where collecting such labels is challenging."}
{"hash_id": 93811409947535646, "entities": ["diagnostic accuracy", "conversational coherence", "medical dialogues", "annotator preferences", "diagnostic logic"], "background": "1. The need to enhance the diagnostic accuracy and conversational coherence of large language models in medical dialogues to ensure patient safety. 2. The desire to bridge the gap between the divergent diagnostic approaches\u504f\u597d\u6570\u636e provided by annotators and the actual diagnostic logic followed by physicians."}
{"hash_id": 4790328393243697645, "entities": ["catastrophic forgetting", "finetuned language models", "compatibility", "integration method"], "background": "1. The need to prevent catastrophic forgetting in fine-tuned language models, where performance on general tasks beyond the targeted domain significantly degenerates. 2. The requirement for a practical and compatible method that can be easily integrated into existing fine-tuning pipelines without the need for extensive additional training or data collection."}
{"hash_id": 3513365718417973432, "entities": ["counterfactual generation", "llms", "human episodic memory", "relation extraction"], "background": "1. To bridge the gap between LLMs and human commonsense during counterfactual generation, which often results in misalignments and commonsense-violated counterfactuals. 2. To improve the capability of LLMs in generating high-quality counterfactuals for complex tasks like relation extraction by aligning their generation process with human episodic memory retrieval."}
{"hash_id": 6873372278098569083, "entities": ["semantic relatedness", "nlp resources", "sentence representation", "nlp tasks"], "background": "1. The need to expand the understanding of semantic relatedness beyond similarity, particularly in languages with limited NLP resources. 2. The importance of enhancing the representation of meaning in text for various NLP tasks, such as sentence representation evaluation and summarization."}
{"hash_id": 8162972761210572381, "entities": ["overcorrection issues", "autoregressive generative models", "grammatical error correction", "decoderonly llms", "nuanced error identification"], "background": "1. The need to reduce overcorrection issues that arise when using autoregressive generative models for Chinese grammatical error correction (CGEC), which existing methods fail to effectively address in decoder-only large language models (LLMs). 2. The desire to improve the ability of CGEC models to identify nuances in grammatical errors and avoid unnecessary corrections, thereby enhancing overall correction performance."}
{"hash_id": 5511941237080200916, "entities": ["improving transformer performance", "integrating graph neural networks", "static graph construction limitations"], "background": "1. The need to improve the performance of transformers in NLP tasks by integrating graph neural networks (GNNs) without compromising scalability and the ability to handle unseen data. 2. The desire to overcome the limitations of static graph construction methods that require extensive text preprocessing and are not adaptable to dynamic changes in the dataset."}
{"hash_id": 2483399301286719515, "entities": ["emotional state patterns", "utterance emotion dynamics", "individual character arcs"], "background": "1. The need to computationally capture the longitudinal patterns of a character's emotional states and Utterance Emotion Dynamics, which have been under-explored in computational literary studies. 2. The desire to move beyond the simplification of considering the entire novel as a single emotional trajectory and instead differentiate between the arcs of individual characters and the narration."}
{"hash_id": 7554635594954435053, "entities": ["openqa evaluations", "semantic understanding", "evaluation metric", "nuanced answer correctness"], "background": "1. Current Open-QA evaluations are criticized for ambiguity in questions and the lack of semantic understanding, leading to deviations from human judgments. 2. There is a need for a more informative and fairer evaluation metric that can capture the true capabilities of Open-QA systems, especially in terms of nuanced answer correctness."}
{"hash_id": 3400148390136737589, "entities": ["multimodal understanding", "geometry problem solving", "positional information", "spatial relationships"], "background": "1. The need for improved multi-modal understanding and reasoning in geometry problem solving, which is essential for capturing the complex layout information of geometry diagrams. 2. The shortcomings of existing neural solvers in representing the positional information of elements within geometric diagrams, leading to a loss of spatial relationships and reduced problem-solving performance."}
{"hash_id": 5381185166269437880, "entities": ["language models", "complex fact networks", "geometric knowledge reasoning"], "background": "1. The need to assess and enhance large language models' ability to reason over complex, non-linear fact networks, which is beyond their current capabilities limited to atomic or linear QA tasks. 2. The requirement for new methods that can help LLMs overcome the challenges posed by geometric knowledge reasoning, such as backtracking, verifying facts and constraints, and reasoning with uncertainty."}
{"hash_id": 1032032876112186386, "entities": ["misinformation detection", "large language models", "context enrichment"], "background": "1. The need to improve the reliability of misinformation detection using large language models, which are currently limited by issues of factuality and hallucinations. 2. The requirement for a method that can enrich the context of news articles and simulate diverse user interactions to enhance the performance of misinformation detection systems."}
{"hash_id": 2137671270598916392, "entities": ["multilingual llm safety", "highresource discrepancy", "lowerresource languages"], "background": "1. The need to ensure the safety and alignment of LLMs across multiple languages, especially as their influence expands globally and they are exposed to diverse user instructions. 2. The observed discrepancy in LLM performance and safety between high-resource and lower-resource languages, which highlights the urgency to understand and mitigate safety risks in multilingual contexts."}
{"hash_id": 6053692836791473021, "entities": ["dataefficient specialization", "large language models", "domain expertise"], "background": "1. The need for a data-and parameter-efficient way to specialize large language models in expert domains where acquiring human labels is challenging and computationally costly. 2. The observation that existing models, even after self-alignment, exhibit only a modest degree of improvement within specialized domains, necessitating a more focused approach to leverage domain expertise."}
{"hash_id": 7981378923761055800, "entities": ["semantic concepts uncertainty", "set representation", "set operations", "large universes discourse"], "background": "1. The need to capture uncertainty and ambiguity in semantic concepts, which traditional set representations cannot effectively model. 2. The requirement for a set representation that is closed under all set operations and can efficiently handle large universes of discourse."}
{"hash_id": 5797242883048544673, "entities": ["improve reasoning capabilities", "legal settings", "complex compositional rules", "effective prompting method"], "background": "1. The need to improve the reasoning capabilities of language models in legal settings to transform the legal industry and enhance efficiency, as well as to broaden access to justice. 2. The challenge of handling complex compositional rules in legal reasoning, which current language models struggle with, leading to the need for a more effective prompting method."}
{"hash_id": 5710190817405336730, "entities": ["factuality evaluation", "entity ambiguity", "factscore", "citation recall"], "background": "1. The need to accurately evaluate the factuality of long-form generations that may contain a mix of factual and non-factual claims due to entity ambiguity. 2. The recognition that existing factuality metrics like FActScore and citation recall do not properly account for entity ambiguity, leading to overestimation of the factual accuracy of such generations."}
{"hash_id": 7936040584421696809, "entities": ["language models", "semantic relationships", "sentence cooccurrence", "theoretical assumptions"], "background": "1. The need to understand whether language models (LMs) can infer semantic relationships like entailment from sentence co-occurrence probabilities, which is essential for improving the linguistic capabilities of LMs. 2. The recognition that the assumptions made in previous theoretical work about human speakers' avoidance of redundancy may be too simplistic and need to be revisited to reflect real-world language use."}
{"hash_id": 4433528972616689163, "entities": ["social intelligence", "ai systems", "research gaps", "dataset development"], "background": "1. The need for a standardized and comprehensive definition of social intelligence in AI systems to organize and synthesize existing research and datasets. 2. The desire to identify research gaps and emerging trends in social intelligence data efforts to guide future dataset development and improve the capabilities of NLP systems."}
{"hash_id": 748263259239667376, "entities": [], "background": "1. The need for a more efficient approach to optimize pre-trained language models for downstream tasks than fine-tuning, which is time-consuming and memory-inefficient. 2. The observation that prefix tokens carry context-specific information and the hypothesis that enhancing their specialization can improve model performance."}
{"hash_id": 5953701917725515515, "entities": ["adaptable summarization method", "aspectbased summaries", "multiple objectives"], "background": "1. The need for a more adaptable and versatile summarization method that can automatically identify and summarize various aspects without prior knowledge, due to the limitations of existing approaches that require predefined aspects. 2. The desire to enhance the quality of aspect-based summaries by incorporating multiple objectives, including the prediction of aspect numbers and minimizing overlap between summaries of different aspects."}
{"hash_id": 397499940221521570, "entities": ["noncompositional expressions", "continual learning", "catastrophic forgetting"], "background": "1. The limited ability of contemporary pre-trained language models to generate non-compositional expressions, which are integral to natural language, and the need to improve their capability for continual learning without requiring retraining. 2. The rise of \"catastrophic forgetting\" in neural models, which calls for effective continual learning algorithms specifically tailored for the generation of non-compositional expressions."}
{"hash_id": 6291807670504570639, "entities": ["medical dialogue systems", "systematic review", "large language models", "translational gap"], "background": "1. The need for a systematic and technical review of medical dialogue systems to fill the gap in the understanding of their categories, methods, and evaluation, which is essential for their further improvement. 2. The potential of large language models to transform medical dialogue systems, despite the translational gap between cutting-edge techniques and practical application in various medical scenarios."}
{"hash_id": 1029233623808228543, "entities": ["chainofthought reasoning", "multihop question answering", "large language models"], "background": "1. The need to directly evaluate the correctness of chain-of-thought reasoning steps in multi-hop question answering, rather than solely focusing on answer accuracy. 2. The requirement to understand the reasoning process of large language models and to determine if their strong performance is due to true reasoning ability or superficial heuristics."}
{"hash_id": 4550325686611012269, "entities": ["formal task definition", "opensource evaluation scheme", "large language models", "online comments clustering"], "background": "1. The lack of a formal task definition, an open-source evaluation scheme, and accessible data for the Clustering and Abstractive Summarization of online news Comments (CASC) task hinders the evaluation and progress of diverse methods. 2. The limitations of existing Large Language Models (LLMs) in handling lengthy online comments and their underdeveloped efficacy in clustering."}
{"hash_id": 8780099841619580745, "entities": ["position bias", "large language models", "debiasing methods"], "background": "1. Large language models exhibit position bias, which leads to poor generation performance and vulnerability to adversarial attacks, and there is a lack of effective methods for debiasing position bias without external knowledge or annotated non-biased samples. 2. Existing debiasing methods for LLMs are primarily focused on social biases and are not transferable to address position bias, which is a significant issue affecting the generalization of LLMs."}
{"hash_id": 6001108867462838770, "entities": ["ontology hierarchy", "hyperrelational embeddings", "semantic relatedness", "dominance issue"], "background": "1. The need to incorporate ontology hierarchy into hyper-relational knowledge graph embeddings to improve the semantic relatedness of entity representations. 2. The requirement to address the dominance issue of hyper-relational facts over ontology in existing embedding methods, which leads to suboptimal performance."}
{"hash_id": 239897572958949387, "entities": ["ideological leanings detection", "label semantics", "explanatory descriptions", "multiple facets analysis"], "background": "1. The need to detect ideological leanings in texts towards multiple facets, rather than a single generic facet, to better understand public opinion and detect potential extremism. 2. The importance of incorporating label semantics and explanatory descriptions of ideologies to enhance the model's generalization ability and provide more meaningful ideological categorizations."}
{"hash_id": 3298920264408155081, "entities": ["characterlevel parsing", "word boundaries", "intraword structures", "dependency parsing"], "background": "1. The need to transition from word-level to character-level parsing in Chinese to overcome the challenge of absent word boundaries, which can improve parsing accuracy and reduce dependency on preprocessing segmentation. 2. The requirement for a more linguistically informed approach that can represent and predict multifaceted intra-word structures, enhancing the plausibility and practicality of character-level dependency parsing."}
{"hash_id": 8395902403641930881, "entities": ["prototypebased zsre", "encoding gap", "side information", "prototype embeddings"], "background": "1. Existing prototype-based ZSRE methods overlook abundant side information of relations and suffer from a significant encoding gap between prototypes and sentences, which hinders performance. 2. The quality of prototype embeddings is often compromised by noise in the description information, and there is a need to enhance the robustness and accuracy of these embeddings."}
{"hash_id": 7081098271071805269, "entities": ["alignment tax phenomenon", "data biases", "instructionfollowing data"], "background": "1. The need to overcome the alignment tax phenomenon, where the performance of large language models on knowledge and reasoning benchmarks deteriorates with increased instruction-following data. 2. The hypothesis that data biases are a primary cause of the alignment tax, rather than low-quality samples or knowledge forgetting."}
{"hash_id": 2853850516569536392, "entities": ["knowledge mismatch", "domainspecific applications", "information compliance", "structured format", "natural language format"], "background": "1. To overcome the knowledge mismatch between public knowledge graphs and the specific domain needs of tasks, which often leads to incomplete or inadequate information for domain-specific applications. 2. To improve information compliance, addressing the disparity between the structured format of knowledge graphs and the free-flowing natural language format, which can result in confusion and hallucinations in model outputs."}
{"hash_id": 1080399527493342679, "entities": ["critical errors", "granular level", "sentencelevel error detection"], "background": "1. The need to identify and correct critical errors at a granular level to prevent catastrophic meaning distortions that can have adverse personal or societal impacts. 2. The insufficiency of existing methods that focus on sentence-level error detection, which do not provide precise localization of errors within the sentence."}
{"hash_id": 815340281615857184, "entities": ["large language models", "lowrank adaptation", "pruning method"], "background": "1. The need to reduce the vast model scale and computational costs associated with deploying large language models (LLMs) fine-tuned using low-rank adaptation (LoRA). 2. The requirement for a pruning method that is compatible with LoRA and does not rely on pre-trained weights' gradients, which can lead to significant memory overhead."}
{"hash_id": 5685852845159579952, "entities": ["reduce deployment training costs", "accelerate token generation", "speculative decoding methods", "fixed drafting step"], "background": "1. The need to reduce deployment and training costs as well as significantly accelerate token generation speed in large language models. 2. The limitation of existing Speculative Decoding methods in optimizing the trade-off between the quality and speed of draft token generation and the use of a fixed drafting step."}
{"hash_id": 7740245931409893901, "entities": ["multireference instances", "chinese gec training", "systematic investigation"], "background": "1. The high proportion of multi-reference instances in Chinese GEC training data is not effectively utilized, which could potentially improve the performance of GEC models. 2. Existing methods for handling multi-reference instances in GEC training lack a systematic investigation and comparison, leading to suboptimal use of valuable training data."}
{"hash_id": 7452908944716590003, "entities": ["agent capabilities", "open llms", "generalizable agent abilities"], "background": "1. The need to improve the agent capabilities of open LLMs, which are currently inferior to commercial LLMs in complex, real-world scenarios. 2. The lack of research focus on enhancing the generalizable agent abilities of LLMs while maintaining their original language processing capabilities."}
{"hash_id": 910542075585436541, "entities": ["aspectbased sentiment analysis", "structural integrity", "pretrained language models", "structured finetuning", "opinion tree generation"], "background": "1. The need to improve the structural integrity of generated sentiment elements in aspect-based sentiment analysis, as existing approaches often overlook this aspect, leading to the need for additional post-processing. 2. The desire to leverage the power of pre-trained language models for structured fine-tuning to enhance the accuracy of opinion tree generation and reduce error propagation in traditional methods."}
{"hash_id": 5103993119468580097, "entities": ["dynamic topic models", "repetitive topics", "topic evolution", "markov chains", "topic diversity"], "background": "1. To overcome the limitations of existing dynamic topic models that suffer from repetitive topics and unassociated topics, which hinder the accurate tracking of topic evolution and negatively impact downstream applications. 2. To break the traditional approach of chaining topics via Markov chains, which leads to a lack of topic diversity and the inclusion of irrelevant words in topics."}
{"hash_id": 2167660717889275501, "entities": ["nonenglish datasets", "chinese llm safety", "challenging dataset", "safety assessment"], "background": "1. The lack of non-English datasets for evaluating the safety of LLMs, particularly in Chinese, which is important given the regional and cultural differences that can affect LLM responses. 2. The need for a more challenging dataset that can match the evolving speed of LLMs and provide a better assessment of their safety mechanisms across different models."}
{"hash_id": 5794172538381880753, "entities": ["improve explainability readability", "stock movement predictions", "large language models", "financial timeseries forecasting"], "background": "1. The need to improve the explainability and readability of stock movement predictions by moving beyond traditional keyphrase-based and sentiment-based methods. 2. The desire to leverage the capabilities of Large Language Models (LLMs) to extract factors that are more directly related to stock market dynamics, enhancing the predictive power of financial time-series forecasting."}
{"hash_id": 2666086981652616007, "entities": ["autonomous gui agents", "inference cost reduction", "action prediction capabilities", "error propagation reduction"], "background": "1. The need to improve the efficiency and reduce the inference cost of autonomous GUI agents by eliminating the dependency on external parsing tools and application-specific APIs. 2. The goal to enhance the action prediction capabilities of agents and reduce error propagation in order to achieve more accurate and reliable task automation in diverse environments."}
{"hash_id": 5310017640242409025, "entities": ["hidden dimension compression", "structured pruning"], "background": "1. The need to effectively compress the hidden dimension in pre-trained language models to reduce model size and improve efficiency without significant loss in accuracy. 2. The underexplored potential of structured pruning for reducing the hidden dimension compared to other compression techniques like distillation."}
{"hash_id": 3039044841596273869, "entities": ["dialogue summarization data", "transfer learning", "perturbationbased methods", "semantic diversity", "privacy concerns"], "background": "1. The significant lack of dialogue summarization data due to the complexities in collecting, organizing, and publicizing dialogue data, which is further compounded by privacy concerns and the need for manual annotations. 2. The limitations of existing approaches such as transfer learning and perturbation-based methods, which do not fully capture the characteristics of dialogue data or lack semantic diversity and can potentially harm performance."}
{"hash_id": 3219707322151845492, "entities": ["compositionality assessment", "emergent communication", "finegrained characterization"], "background": "1. The need for a more direct and interpretable method to assess the compositionality of emergent communication in artificial agents, as current evaluation procedures do not expose this trait effectively. 2. The desire to provide a fine-grained characterization of emergent communication protocols that can reveal their strengths and weaknesses, which is not achieved by existing coarse and opaque evaluation measures."}
{"hash_id": 9105087257779423019, "entities": ["document version changes", "collaborative writing", "change logs", "thematic edits"], "background": "1. The lack of efficient tools to provide a comprehensive overview of changes between document versions, which hinders understanding and review in collaborative writing environments. 2. The need to present change logs in a way that allows for swift human consumption and easy navigation through thematic edits, improving the collaborative reviewing experience."}
{"hash_id": 7084603329790447833, "entities": ["gender bias", "nlp models", "stance detection"], "background": "1. The perpetuation of harmful stereotypes and discrimination through gender bias in NLP models, which has been overlooked in stance detection tasks. 2. The need to understand and mitigate the sources of gender bias in stance detection to ensure fair and unbiased decision-making."}
{"hash_id": 3091843217403031904, "entities": ["likelihood bias", "llms", "evaluation metrics", "natural language generation"], "background": "1. The reliance on LLMs as evaluation metrics for natural language generation tasks could be compromised by likelihood bias, which affects the accuracy and fairness of the evaluation process. 2. Existing evaluation methods like BLEU and ROUGE have limitations, and LLMs have shown potential to\u8d85\u8d8a them, but their biases need to be addressed to fully leverage their capabilities."}
{"hash_id": 1631192722683102231, "entities": ["musical capabilities", "large language models", "gender disparities", "music scholarship"], "background": "1. The need to assess and improve the musical capabilities of large language models, which are an essential aspect of human culture and have been largely overlooked in existing benchmarks. 2. The recognition of gender disparities in historical music literature, with a goal to promote inclusivity and advancement in the field of music scholarship."}
{"hash_id": 5026652336149728099, "entities": ["gpu memory usage", "llm inference", "kv cache compression"], "background": "1. The immense GPU memory usage during LLM inference, particularly in the KV cache, which can be several times the size of the model, hinders the scalability of LLMs for real-time applications. 2. Existing methods for KV cache compression only reduce the already computed cache and neglect the significant memory consumption during the pre-computation phase, especially for longer prompts and larger models."}
{"hash_id": 2508881937501010917, "entities": ["ethical alignment vlms", "harmful multimodal inputs", "red teaming benchmark", "vlm robustness evaluation"], "background": "1. The need to ensure that VLMs are ethically aligned and can handle potentially harmful or misleading multimodal inputs, given their expanding capabilities and use in various applications. 2. The lack of a comprehensive and systematic red teaming benchmark to evaluate the robustness and vulnerabilities of current VLMs in terms of faithfulness, safety, privacy, and fairness."}
{"hash_id": 6355895816865683006, "entities": ["semantic consistency", "computational costs", "internal mechanisms", "llms", "black box"], "background": "1. The need to enhance semantic consistency in LLMs without incurring high computational costs associated with data-driven finetuning methods. 2. The desire to gain deeper insights into the internal mechanisms of LLMs and treat them as more than a \"black box\" to understand and address the causes of semantic inconsistency."}
{"hash_id": 6396133198921922787, "entities": ["crossdomain environments", "task planners", "disentangle learned behaviors"], "background": "1. The need to adapt pretrained language model-based task planners to execute skills in cross-domain environments, where skills are intricately entangled with domain-specific knowledge. 2. The requirement for a framework that can disentangle learned behaviors from their original domain contexts and apply them effectively in new domains."}
{"hash_id": 6970280581451382572, "entities": ["rlhf stability", "multiresponse scenarios", "preference alignment"], "background": "1. The complexity and sensitivity to hyperparameters in implementing RLHF, which makes achieving stable performance and scalability challenging. 2. The limited exploration of multi-response scenarios in preference alignment, overlooking the potential richness within the candidate pool."}
{"hash_id": 6632044669658769008, "entities": ["neuralsymbolic reasoning", "large language models", "kgqa tasks", "incontext learning", "performance gaps"], "background": "1. To enhance the neural-symbolic reasoning capabilities of language agents powered by Large Language Models (LLMs) in KGQA tasks, which is crucial for real-life applications. 2. To overcome the limitations of In-Context Learning (ICL) agents, such as performance gaps compared to classical methods and practical concerns related to privacy, costs, and model flexibility."}
{"hash_id": 1040282601186365290, "entities": ["reduce inference times", "large language models", "cloudedge collaboration"], "background": "1. The need to reduce inference times and resource demands of Large Language Models (LLMs) without requiring fine-tuning, which is challenging for average users. 2. The desire to integrate acceleration and cost reduction techniques in a cohesive manner for cloud-edge collaboration."}
{"hash_id": 5487031322852915509, "entities": ["grounded language models", "compositional generalization", "factual hallucinations", "implicit encoding"], "background": "1. To investigate and enhance the ability of grounded language models to exhibit compositional generalization, which is central to human reasoning and a long-standing challenge in machine learning. 2. To address the issue of factual hallucinations in model outputs caused by the implicit encoding of knowledge in a large parameter space during pre-training."}
{"hash_id": 4412856565638364278, "entities": ["zeroshot performance", "large language models", "generative ner", "negative instances"], "background": "1. The need to enhance the zero-shot performance of Large Language Models (LLMs) in Generative Named Entity Recognition (NER) tasks, as their performance lags behind supervised training state-of-the-art methods due to limited NER data. 2. The recognition that negative instances, which are crucial in traditional classification models, have not been fully explored for their potential in improving generative models."}
{"hash_id": 4822529168265674257, "entities": ["language model inaccuracies", "lifelong knowledge updates", "performance degradation", "knowledge editing methods"], "background": "1. The need to rectify inaccuracies in large language models without the high cost of retraining, especially for lifelong knowledge updates. 2. The observed performance degradation in current knowledge editing methods when applied to lifelong editing scenarios, characterized by toxicity buildup and flash."}
{"hash_id": 3053992228896967900, "entities": ["neuralbased absa models", "annotation biases", "debiasing methods", "causal inference", "aspect and review biases"], "background": "1. Neural-based ABSA models are vulnerable to annotation biases, leading to poor robustness against adversarial data transformations, which motivates the need for debiasing methods. 2. Existing debiasing methods based on single-variable causal inference are inadequate for ABSA with two input variables, necessitating a more sophisticated approach that can address the complexities of both aspect and review biases."}
{"hash_id": 5959669234320530416, "entities": ["dataset consumption", "lora finetuning", "uncertaintybased active learning"], "background": "1. The need to reduce the consumption of large annotated datasets during the fine-tuning of large language models. 2. The poor performance observed when combining parameter-efficient fine-tuning methods like LoRA with traditional uncertainty-based active learning."}
{"hash_id": 4838269440128980992, "entities": ["llms success mechanisms", "machine translation evaluation", "neural metrics", "bleu insufficiency"], "background": "1. The need to understand the mechanisms behind LLMs' success in machine translation evaluation to improve their performance and reliability. 2. The recognition that traditional metrics like BLEU are not sufficient for high-quality translations and that neural metrics based on LLMs can provide a better correlation with human judgments but lack interpretability and robustness."}
{"hash_id": 4435877457733721550, "entities": ["hallucinations reduction", "closed book qa", "factchecking mechanisms"], "background": "1. The need to reduce the generation of plausible yet incorrect factual information (hallucinations) in large language models. 2. The desire to improve the accuracy of responses in tasks such as closed book QA and longform text generation by incorporating fact-checking and self-critique mechanisms."}
{"hash_id": 3571453125295890018, "entities": ["ai agents bargaining abilities", "realworld negotiation scenarios", "bargaining performance gap"], "background": "1. The need to quantitatively assess and improve AI agents' bargaining abilities, particularly as buyers, in order to enhance their performance in real-world negotiation scenarios such as e-commerce transactions. 2. The requirement to bridge the gap in bargaining performance between AI agents acting as sellers versus buyers, as the task of playing a buyer is significantly more challenging."}
{"hash_id": 1736274256186207251, "entities": ["videolanguage understanding", "ai systems", "comprehensive survey", "model architecture", "data perspectives"], "background": "1. The need to create artificial intelligence systems that can understand and process video and language in a human-like manner, to better mimic human senses and improve communication and perception. 2. The lack of a comprehensive survey that addresses all aspects of video-language understanding, including model architecture, training, and data perspectives, which hinders progress in the field."}
{"hash_id": 3864328593980615624, "entities": ["nonalphabetic languages", "input scenarios", "user preferences", "input methods evolution", "accuracy improvement"], "background": "1. The need to handle complex and diverse input scenarios in non-alphabetic languages like Chinese, which are not adequately addressed by current input methods. 2. The requirement for input methods to evolve and adapt to user preferences and feedback to improve accuracy and efficiency."}
{"hash_id": 3932179627536658180, "entities": [], "background": "1. The need to improve the performance of large language models by supplementing them with necessary knowledge, especially in complex scenarios where they lack domain-specific or latest information. 2. The desire to overcome the bottleneck of retriever performance in existing Retrieval-Augmented Generation (RAG) approaches and to explore a more efficient and effective method for knowledge integration."}
{"hash_id": 4506974229478877435, "entities": ["code generation accuracy", "code solution reranking", "functional similarities"], "background": "1. The need to improve the accuracy and reliability of code generation by effectively reranking the numerous solutions generated by CodeLLMs. 2. The failure of existing methods to fully capture the intricate functional similarities and interactions between clusters of code solutions, leading to suboptimal selection of the best code solution."}
{"hash_id": 8189967773752566142, "entities": ["preference optimization", "manual annotation", "distribution gap", "large language models"], "background": "1. The need to reduce manual annotation efforts and improve the efficiency of preference optimization in large language models. 2. The requirement to overcome the distribution gap that emerges between model-generated samples and human-annotated preference data during iterative updates of the LLM."}
{"hash_id": 852869508391279672, "entities": ["generation instability", "aspect sentiment quad prediction", "gridbased methods"], "background": "1. To resolve the persistent challenge of generation instability that affects the performance of aspect sentiment quad prediction. 2. To improve the effectiveness of grid-based methods in ASQP, which have been underexplored in current studies."}
{"hash_id": 3860648688803270066, "entities": ["catastrophic forgetting", "neural networks", "spoken language understanding", "classincremental learning", "learned representations"], "background": "1. The need to overcome catastrophic forgetting in neural networks when learning new tasks continuously, especially in spoken language understanding where new categories and intents emerge over time. 2. The requirement for a more effective approach to preserve learned representations and adapt to new data without compromising the performance on previously learned tasks in a class-incremental learning setup."}
{"hash_id": 1933191025559581822, "entities": ["improve generalizability ai", "realworld lowresource scenarios", "human reasoning"], "background": "1. The need to improve the generalizability of AI technologies to handle data dynamics in real-world low-resource scenarios, where both textual and structural information is scarce. 2. The inspiration drawn from human reasoning, which successfully deduces connections based on limited relation semantics without requiring expert knowledge or prior learning."}
{"hash_id": 2871020346862823589, "entities": ["unsupervised parsing method", "syntactic units", "frequency information", "predicateargument structures"], "background": "1. The need for an unsupervised parsing method that can effectively identify word sequences corresponding to syntactic units (constituents) by utilizing frequency information, which is not available in previous parsing methods. 2. The discovery of a statistical difference between the frequencies of participant-denoting and event-denoting constituents in sentences with equivalent Predicate-Argument Structures (PAS), which provides a foundation for future labeled unsupervised parsing research."}
{"hash_id": 3172647911354323648, "entities": ["debiasing methods", "explicit bias words", "implicit bias words", "fairness of language models"], "background": "1. Existing debiasing methods mainly focus on explicit bias words and ignore the presence of implicit bias words in training data, which can indirectly lead to unfair decisions by pre-trained language models. 2. There is a need for a more comprehensive data-centric approach that improves the quality of training data by addressing both explicit and implicit biases to enhance the fairness of language models."}
{"hash_id": 8161553448768034603, "entities": ["crossdocre tasks", "domain knowledge", "interpretability", "textual explanations"], "background": "1. The first motivation is to improve the performance of CrossDocRE tasks by incorporating domain knowledge, which is crucial for understanding relations between entities but is often ignored in existing models. 2. The second motivation is to enhance the interpretability of the relation extraction process by providing textual explanations for the predicted relationships."}
{"hash_id": 4268241888127049195, "entities": ["improving dialogue naturalness", "sales interactions", "controllable dialogue model", "understanding user intents"], "background": "1. The need to improve the naturalness and coherence of dialogues in salesperson-customer interactions, particularly when transitioning from casual chit-chat to task-oriented dialogue. 2. The requirement for a more controllable and explainable model that can understand and respond to user intents that emerge during the conversation, rather than assuming an explicit goal from the start."}
{"hash_id": 3236127423772018585, "entities": ["knowledge graphs", "large language models", "promptbased methods", "structured information", "knowledge conflict"], "background": "1. To improve the expertise and accuracy of large language models by integrating knowledge graphs, which contain structured factual knowledge and domain expertise. 2. To overcome the limitations of existing prompt-based methods, such as the loss of structured information, knowledge conflict, and over-reliance on super LLMs."}
{"hash_id": 6570223676402240270, "entities": ["enhance reasoning capabilities", "complex reasoning tasks", "greedy decoding strategy", "chainofthought prompting"], "background": "1. The need to enhance the reasoning capabilities of language models beyond their current limitations, especially in complex reasoning tasks. 2. The requirement to overcome the issues of repetitiveness and local optimality that arise from the naive greedy decoding strategy used in chain-of-thought prompting."}
{"hash_id": 4822923531725345344, "entities": ["mathematical reasoning", "large language models", "financial documents"], "background": "1. The need to understand and improve the mathematical reasoning capabilities of Large Language Models (LLMs) in the context of financial documents, which is crucial for real-world applications such as financial analysis and decision-making. 2. The desire to bridge the gap between LLMs' performance in natural language understanding and their ability to handle complex mathematical reasoning tasks that involve a hybrid of structured tables and unstructured text."}
{"hash_id": 6580859888672199409, "entities": ["large language models", "incontext learning", "sentiment analysis", "human learning processes", "feedback adjustment"], "background": "1. The limitations of Large Language Models (LLMs) in differentiating subtly similar sentiments when using in-context learning (ICL), leading to incorrect sentiment predictions. 2. The potential to improve LLMs' sentiment analysis performance by\u6a21\u4eff human learning processes, where feedback is used to adjust and refine understanding."}
{"hash_id": 6977193341862955603, "entities": ["knowledge conflicts", "decoding methods", "language models", "contextual knowledge"], "background": "1. Large language models struggle with incorporating external contextual knowledge that may conflict with their internal parametric knowledge, leading to inaccurate or inconsistent outputs. 2. Existing decoding methods that aim to resolve knowledge conflicts often deteriorate performance in the absence of conflicts, and they do not discern between conflicting and non-conflicting contexts."}
{"hash_id": 4913220045105028478, "entities": ["large language models", "safety measures", "safety evaluation scenarios"], "background": "1. The need to ensure robust safety measures in the rapidly evolving landscape of Large Language Models (LLMs) due to their powerful capabilities and potential safety threats. 2. The absence of a comprehensive benchmark that covers a wide range of safety evaluation scenarios, including attacks and defenses, for LLMs."}
{"hash_id": 4207863764877310506, "entities": ["representation learning", "expert annotations", "nuanced details", "radiology reports", "clinical information"], "background": "1. The scarcity of expert annotations for text and images in specialized fields like medicine hinders the advancement of representation learning. 2. Existing encoders and label extraction techniques struggle with capturing nuanced details and factual information from free-text radiology reports, leading to incomplete or inaccurate clinical information."}
{"hash_id": 3346898062522254525, "entities": ["computational demands reduction", "lowdata adaptability", "incontext learning dynamics", "promptbased finetuning"], "background": "1. The need to reduce the computational demands of fine-tuning Large Language Models (LLMs) while enhancing their adaptability and performance in low-data scenarios. 2. The recognition of the importance of information flow dynamics in In-Context Learning (ICL) and the potential to leverage this understanding for more efficient and effective prompt-based fine-tuning."}
{"hash_id": 4563298634856652889, "entities": ["abstractive summarization", "longform documents", "movie screenplay summarization"], "background": "1. The need to advance research in abstractive summarization by providing a dataset that includes long-form documents with dispersed important information, such as movie screenplays. 2. The recognition that existing language models struggle with processing long input contexts, and there is a lack of adequate datasets to specifically address this issue in the context of movie screenplay summarization."}
{"hash_id": 1129434116486293837, "entities": ["intelligent virtual assistants", "mixed reality environments", "ai agents integration", "multimodal assembly assistance"], "background": "1. The need to enhance the efficiency of industrial assembly tasks and reduce cognitive workload for trainees through the use of intelligent virtual assistants in mixed reality environments. 2. The gap in existing research that addresses the integration of AI agents into fine-grained, multimodal assembly assistance, particularly in grounding assistant services and understanding users' situated multimodal contexts."}
{"hash_id": 6631637047989605114, "entities": ["performance disparities", "south asian englishes", "language technology challenges"], "background": "1. To understand the impact of performance disparities in NLP systems on the user experience of speakers of South Asian Englishes, a dialect with a large global speaker base. 2. To identify and address the unique challenges faced by South Asian English speakers when interacting with language technology, which could lead to improved support for their dialects."}
{"hash_id": 2330706360244845470, "entities": ["transformers surfacelevel performance", "reasoning tasks", "memorization patterns", "specific mechanisms", "multistep reasoning tasks"], "background": "1. To gain insights into the internal workings of transformers beyond their surface-level performance on reasoning tasks, in order to determine whether they truly reason or simply memorize patterns. 2. To identify and understand the specific mechanisms that transformers employ to solve multi-step reasoning tasks, which can inform the development of more effective and interpretable models."}
{"hash_id": 8544454253274572862, "entities": ["attention weights", "modality gap", "semantic matching", "multielement correlations"], "background": "1. Existing methods heavily rely on automatically learned attention weights, which may over-concentrate on partial correlations and not consider global correlations. 2. There is a need to bridge the modality gap and enable fine-grained semantic matching by better leveraging multi-element correlations within entity and mention contexts."}
{"hash_id": 6092070385714088311, "entities": [], "background": "1. To bridge the gap between empirical observations of neural language models' capabilities and theoretical understanding of these capabilities, particularly in relation to RNNs. 2. To investigate the inductive biases of RNNs and determine whether their efficiency in representing languages is solely linked to hierarchical structures."}
{"hash_id": 4553189785354435058, "entities": ["data selection methods", "instruction tuning", "visionlanguage models"], "background": "1. Existing data selection methods for instruction tuning are either unreliable or time-consuming and can lead to over-fitting on evaluation datasets. 2. Vision-language models (VLMs) benefit from training with the most challenging instructions, which are not effectively identified by current approaches."}
{"hash_id": 6616494706577919486, "entities": ["context length expansion", "transformer models", "positional embeddings"], "background": "1. The need to improve the context length expansion of transformer models, particularly for long context tasks where the model's ability to extrapolate beyond the trained length is limited. 2. The challenge of finding efficient and balanced extrapolation methods for positional embeddings that can maintain model performance without requiring complex, artificially designed functions."}
{"hash_id": 2931419668422378652, "entities": ["visually impaired applications", "precise answers", "calibration methods", "vqa incontext learning", "large multimodal models"], "background": "1. The need for precise answers, especially in applications assisting the visually impaired, which requires models to be well-calibrated and able to quantify their uncertainty. 2. The lack of a comprehensive study on calibration methods and metrics for VQA with in-context learning Large Multi-Modal models (LMMs)."}
{"hash_id": 722713351146643770, "entities": ["data scarcity", "direct speechtosql", "robustness generalization", "diverse speech samples"], "background": "1. To overcome the challenge of data scarcity in training direct speech-to-SQL parsing models, which is crucial for languages without written forms and to avoid error compounding in cascaded systems. 2. To enhance the robustness and generalization of speech-to-SQL systems to diverse out-of-domain speech samples, particularly when dealing with personalized speech and varying accents."}
{"hash_id": 7950899240201678659, "entities": ["computational overhead", "llm training", "long context efficiency"], "background": "1. The high computational overhead and resource expenditure required to train LLMs with long context windows from scratch or through existing extension methods. 2. The need to improve the efficiency of LLM training for long contexts without compromising numerical precision or model performance."}
{"hash_id": 2113072142699534316, "entities": ["multihop reasoning", "llm efficiency", "structured environments", "faithfulness", "hallucinations avoidance"], "background": "1. The need to improve the efficiency of LLMs in multi-hop reasoning over structured environments, as existing methods require excessive interactions and are prone to error propagation. 2. The requirement for ensuring faithfulness in LLM reasoning, where the model's output is grounded on the structured environment to avoid hallucinations and improve accuracy."}
{"hash_id": 2680116402634025407, "entities": ["integration of logical rules", "knowledge graph embedding", "reasoning capabilities", "entityrelation embeddings"], "background": "1. The need to integrate logical rules with knowledge graph embedding methods to enhance reasoning capabilities and address the brittleness of logical rules. 2. The desire to improve the performance of knowledge graph reasoning by enriching and regularizing entity/relation embeddings with prior logical rule information."}
{"hash_id": 3454545598932755037, "entities": ["linguistic complexities control", "user preferences", "length bias mitigation", "llm alignment techniques"], "background": "1. The need to precisely control various linguistic complexities of large language model outputs to cater to different user preferences and reading complexities. 2. The requirement to mitigate the length bias observed in existing LLM alignment techniques that favor verbose responses, without compromising response quality."}
{"hash_id": 42568124252293239, "entities": ["finegrained attribute intensity", "text generation", "evaluation frameworks"], "background": "1. The need for fine-grained control over attribute intensity in text generation to cater to diverse scenarios and user preferences. 2. The lack of effective evaluation frameworks and training-free methods to achieve smooth control over the attribute intensity in LLM-generated text."}
{"hash_id": 5601013587572340609, "entities": ["comprehensive llm evaluation", "complex realworld scenarios", "tool utilization", "simplified benchmarks", "synthetic queries"], "background": "1. The necessity to comprehensively evaluate LLMs in complex real-world scenarios where tool utilization involves planning, creation, and usage, as current benchmarks fail to capture this complexity. 2. The need to move beyond simplified benchmarks that rely on pre-defined toolsets and synthetic queries, which do not accurately reflect the intricacies and demands of real-world tasks."}
{"hash_id": 1649161874681431913, "entities": ["hallucinations evaluation", "grounded generation", "hidden states analysis"], "background": "1. The need for a more accurate and efficient method to evaluate hallucinations in grounded generation tasks, as current approaches are framed as textual entailment problems and may not fully capture the model's internal knowledge of its own hallucinations. 2. The exploration of using the language model's own hidden states to detect hallucinations, given that these states may contain rich meta-information about the model's generation process."}
{"hash_id": 3315125559093386414, "entities": ["ensemble language models", "ranking processes", "error propagation", "separation of models"], "background": "1. To overcome the time-intensive nature of ranking processes in ensemble language models, which can be inefficient due to the need for pairwise comparisons and additional resource costs. 2. To mitigate the issue of error propagation that arises from the separation of ranking and generation models, which can lead to poor candidates affecting the final generated output."}
{"hash_id": 5331784763214238903, "entities": ["multimodal large language models", "speech recognition tasks", "auxiliary information", "shallow fusion", "neural biasing"], "background": "1. To enhance the performance of multi-modal large language models in speech recognition tasks by incorporating auxiliary information, which traditional ASR models do not fully leverage. 2. To overcome the limitations of existing approaches like shallow fusion and neural biasing, which either lack generality or are limited in the amount of contextual information they can process."}
{"hash_id": 9211837475007533555, "entities": ["efficient task handling", "mixed realworld tasks", "loras integration", "domainspecific adaptations", "scalable approach"], "background": "1. The need to efficiently handle diverse and mixed real-world tasks without the computational expense of finetuning large language models (LLMs) on each specific domain. 2. The requirement for a dynamic and scalable approach to integrate and update LoRAs as the pool of available domain-specific adaptations continuously expands."}
{"hash_id": 710040789596957202, "entities": ["computational costs", "financial barriers", "distillation methods", "knowledge transfer", "smaller models"], "background": "1. The high computational costs and financial barriers of deploying Large Language Models (LLMs) make them impractical for many institutions, especially in resource-constrained environments. 2. Existing distillation methods do not effectively ensure the sufficient transfer of knowledge from LLMs to smaller models, leading to potential inefficiencies in performance and cost."}
{"hash_id": 7824319957212592302, "entities": ["multilingual language models", "global majority", "nonenglish speakers", "intended use", "multilingual capabilities"], "background": "1. The need to ensure that large language models benefit a global majority of non-English speakers, as current models are often intended for use in just English or a few high-resource languages. 2. The observation that people use large language models in many different languages, often beyond their intended use, which necessitates research into the multilingual capabilities of these models."}
{"hash_id": 1274819445658479967, "entities": ["contextual word embeddings", "semantic information", "semantic proxies"], "background": "1. The need to clarify the nature of information captured by contextual word embeddings, particularly whether they primarily encode semantic information or other factors. 2. The importance of determining the reliability of these embeddings as semantic proxies for various research and industry applications, such as semantic shift detection."}
{"hash_id": 8986681851531275348, "entities": [], "background": "1. The need to understand and mitigate the new privacy risks introduced by the integration of retrieval data in RAG systems, which could potentially leak sensitive information from the retrieval database. 2. The goal to explore the impact of RAG on the memorization behavior of LLMs and determine if it can reduce the tendency of LLMs to leak private information from their training data."}
{"hash_id": 3347440546590718388, "entities": ["authentic naturalistic data", "empathy modeling", "longitudinal dataset"], "background": "1. The need for more authentic and naturalistic data to better model and understand empathy in real-world settings. 2. The requirement for a dataset that captures the longitudinal nature of empathy, considering how it develops and is influenced by multiple interactions over time."}
{"hash_id": 1975897737538082757, "entities": ["fully hyperbolic approach", "hierarchical data representation", "numerical stability", "hyperbolic space calculations"], "background": "1. The need to improve the representation of hierarchical data in knowledge graphs by using a fully hyperbolic approach, which overcomes the limitations of existing methods that rely on tangent space approximations. 2. The desire to enhance the numerical stability and accuracy of calculations in hyperbolic space, which is crucial for effectively capturing diverse relational patterns observed in knowledge graphs."}
{"hash_id": 6178400284801737836, "entities": [], "background": "1. The need to improve the granularity of features learned in UI representations, which is crucial for tasks like automating user commands and evaluating accessibility. 2. The absence of a publicly available dataset and a standardized benchmark for UI representation learning, which hinders the development and comparison of new methods."}
{"hash_id": 2616477615893598606, "entities": ["prompt variations", "llm responses", "labeling data", "biased labeling"], "background": "1. The reliability and consistency of LLMs' responses to prompt variations is crucial for their effectiveness in labeling data, yet this aspect is not well understood. 2. The potential for small prompt changes to significantly alter LLM predictions could lead to inaccurate or biased labeling, affecting the integrity of research and applications that rely on these models."}
{"hash_id": 3308407163066079873, "entities": ["harnessing internal neurons", "text classification", "costefficient models", "interpretable models"], "background": "1. The need to harness the rich information contained in internal neurons of LLMs, which is currently untapped in text classification tasks, to potentially improve performance. 2. The requirement for more cost-efficient and interpretable models that can avoid the computational expense of fine-tuning LLMs for specific tasks."}
{"hash_id": 7306587248637118555, "entities": ["word embeddings interpretability", "cooccurrence matrices", "linguistic regularities"], "background": "1. The need for a systematic approach to interpret the dimensions of word embeddings, as the interpretability of their semantic and syntactic features is crucial for natural language processing tasks. 2. The recognition that the latent structure within co-occurrence matrices reflects linguistic regularities and can be used to uncover interpretable categories, which is essential for understanding the underlying linguistic patterns in language models."}
{"hash_id": 6392165999355878526, "entities": ["bridging radiology literacy gap", "parallel corpus creation", "automatic evaluation limitations"], "background": "1. The need to bridge the gap between complex radiology reports and the health literacy of patients to improve patient outcomes and involvement in their care. 2. The challenge of creating a widely acceptable parallel corpus for radiology report simplification and the limitations of automatic evaluation methods."}
{"hash_id": 8659300262208079174, "entities": ["generated questions", "experts requirements", "unified standard", "question generation"], "background": "1. The need for generated questions in education to meet experts' requirements for content control and assessed abilities, beyond just difficulty control. 2. The lack of a unified standard for defining and measuring difficulty in question generation, and the requirement for a more controllable and diverse generation process."}
{"hash_id": 6170970223627949356, "entities": ["enhance factual reasoning", "incorporate external knowledge", "retrievalaugmented generation limitations"], "background": "1. The need to enhance the factual reasoning capabilities of large language models by incorporating external knowledge without incurring high training costs. 2. The requirement to overcome the limitations of current retrieval-augmented generation methods, which can be impeded by the retrieval of irrelevant knowledge texts and the challenge of integrating this knowledge with the model's existing knowledge."}
{"hash_id": 2487615072713029754, "entities": ["hallucinations in llms", "text embedding techniques", "qa performance bottleneck", "ir system combination"], "background": "1. To overcome the issue of hallucinations and outdated information in Large Language Models (LLMs) and the underdeveloped text embedding techniques in contemporary Information Retrieval (IR) systems, which collectively pose a performance bottleneck for question answering (QA) tasks. 2. To enhance the effectiveness of combining multiple IR systems, which has shown promise but lacks optimization and adaptation specifically for QA retrieval settings."}
{"hash_id": 265644936649834266, "entities": ["question decomposition", "multihop reasoning", "human intuition"], "background": "1. The limitations of existing question decomposition methods that fail to account for dependencies between subquestions, particularly in multi-hop reasoning scenarios. 2. The need to align the question decomposition process more closely with human intuition and problem-solving strategies."}
{"hash_id": 2491625153348811777, "entities": ["improve absa performance", "highquality examples", "retrieval method limitations", "surface similarity"], "background": "1. The need to improve the performance of ABSA by using high-quality examples for instruction tuning, as the quality of in-context examples significantly affects the model's output. 2. The requirement to overcome the limitations of existing retrieval methods that are based on surface similarity and are independent of the LM's generative objective, leading to additional computational costs and training difficulties."}
{"hash_id": 4907755367314802331, "entities": ["catastrophic forgetting", "language coverage", "multilingual sentiment classification"], "background": "1. The high cost and potential for catastrophic forgetting when retraining LLMs for improved language coverage and sentiment classification accuracy. 2. The need to enhance multilingual sentiment classification without computationally intensive processes, given the limitations of LLMs in certain languages or domains."}
{"hash_id": 3292249444769228121, "entities": ["interpretability", "relational concepts", "knowledge recall", "fact recall processes"], "background": "1. The need to enhance the interpretability of knowledge recall mechanisms within large language models by understanding how relational concepts are represented. 2. The desire to extract and reuse relational representations to improve the flexibility and controllability of fact recall processes in LLMs."}
{"hash_id": 8489711523342501194, "entities": ["performance variability", "retrievalfree lms", "ralms inconsistency", "knowledge sources variability"], "background": "1. The need to consistently improve performance over retrieval-free LMs at the example level, as RALMs do not always outperform them and exhibit variability among different retrievers. 2. The requirement to understand and mitigate the sources of inconsistency in RALMs, particularly the innate differences in knowledge sources and the unpredictable degeneration of the reader model."}
{"hash_id": 1939078230306315929, "entities": ["resourceintensive dl", "sentiment analysis", "lightweight framework"], "background": "1. To mitigate the issues of extensive computing resources, knowledge forgetting, and the lack of adaptability and human subjectivity in traditional deep learning approaches for sentiment analysis. 2. To propose a more lightweight and efficient sentiment analysis framework that can handle cross-domain and cross-lingual tasks without the need for large-scale pre-training or extensive annotation efforts."}
{"hash_id": 5089334525001325905, "entities": ["trustworthiness evolution", "large language models", "pretraining enhancement"], "background": "1. The need to understand the evolution of trustworthiness in large language models during pre-training, which is a phase often overlooked in favor of fully pre-trained models. 2. The potential to harness the pre-training period to actively enhance the trustworthiness of LLMs, thereby improving their reliability, privacy, toxicity, fairness, and robustness."}
{"hash_id": 6911960797426249021, "entities": ["selfevaluation method", "llms", "traditional evaluation methods"], "background": "1. The need for a self-evaluation method for LLMs that does not require an additional evaluation model or dependency on external, proprietary models like GPT-4. 2. The requirement to accurately gauge the performance of LLMs as they evolve rapidly, exceeding the capabilities of traditional evaluation methods like BLEU which focus on superficial text differences."}
{"hash_id": 2138693518507719320, "entities": ["semantic consistency", "adversarial examples", "evaluation metric"], "background": "1. The need to ensure that generated adversarial examples maintain semantic consistency with the original text, as current methods often fail to do so, leading to invalid examples. 2. The requirement for a more reliable evaluation metric for the validity of adversarial examples, given that current metrics and constraints are not sufficient to prevent the generation of invalid examples."}
{"hash_id": 5425016982163797862, "entities": ["language encoder", "crossmodal models", "crossmodal task performance"], "background": "1. The limited investigation and improvement in the language encoder component, which is crucial for encoding natural language descriptions into vector representations in cross-modal models. 2. The need to analyze and enhance the quality of the language encoder to improve cross-modal task performance, especially as models scale up in pretraining data and modalities."}
{"hash_id": 2989125125930842102, "entities": ["collaborative abilities", "large language models", "collaboration outcomes"], "background": "1. The need to assess and improve the collaborative abilities of large language models (LLMs) when working with humans or other LLMs on equal footing, particularly in coordinating diverse goals and communicating effectively. 2. The gap in existing studies that evaluate the collaborative outcomes of LLMs in terms of specific collaboration abilities, such as coordinating diverse goals or seeking assistance, rather than just the overall performance of LLMs."}
{"hash_id": 1387426407287418700, "entities": ["large language models", "gpu memory", "context compression", "keysvalues caches"], "background": "1. The immense size and parameter count of large language models require significant GPU memory, which increases with the length of input text, leading to inefficiency in memory usage and computation during inference. 2. Existing methods for compressing context information, such as relying on label words or observed attention patterns, are not universal and do not provide a systematic approach to reduce keys/values caches without compromising accuracy."}
{"hash_id": 7102683562307196296, "entities": ["multilevel progressive capabilities", "mvqa models", "doctors thought process"], "background": "1. Existing MVQA models ignore multi-level progressive capabilities due to unspecific data and plain architecture, limiting their effectiveness in real-world scenarios. 2. The need to simulate a doctor's thought process by recognizing, detailing, diagnosing, knowing, and reasoning to answer complex medical questions."}
{"hash_id": 1458920995852036235, "entities": ["bias mitigation", "verbosity exploitation", "reinforcement learning", "dpo efficiency"], "background": "1. The need to mitigate the bias towards verbosity in human feedback, which can lead to length exploitation in Reinforcement Learning from Human Feedback (RLHF) and Direct Preference Optimization (DPO) settings. 2. The requirement for a more efficient and resource-effective solution that retains the performance benefits of DPO without the length-related drawbacks."}
{"hash_id": 8857396356675994523, "entities": ["finegrained multimodal entity knowledge", "mllms", "coarsegrained knowledge"], "background": "1. The current benchmarks predominantly focus on coarse-grained knowledge, leaving the intricacies of fine-grained multimodal entity knowledge unexplored, which is essential for the practical deployment of MLLMs in real-world scenarios. 2. There is an urgent need for novel approaches to address the significant challenges faced by state-of-the-art methods in tackling fine-grained knowledge editing in MLLMs."}
{"hash_id": 3452642021416482600, "entities": ["memes", "model interpretability", "semantic roles", "content moderation"], "background": "1. The need to accurately interpret and assess the potentially harmful content in memes, which are a prevalent and influential medium of communication, especially on social media. 2. The requirement for enhanced model interpretability and a user-friendly method for probing the semantic roles of entities in memes to aid in content moderation and deeper understanding."}
{"hash_id": 5312614373177178780, "entities": ["hallucinations reduction", "misinformation prevention", "language model optimization", "human citation mechanisms"], "background": "1. The need to reduce hallucinations and misinformation in content generated by large language models, which limits their real-world applicability. 2. The requirement to optimize the generation process not just for the original answer but also for the attribution process, mimicking human citation mechanisms."}
{"hash_id": 1227402316226259485, "entities": ["hangeul invention principles", "korean language models", "subcharacter level", "subwordbased approach"], "background": "1. Existing pre-trained language models for Korean have overlooked the unique invention principles and structure of Hangeul, which include an additional \"subcharacter\" level that is crucial for accurate linguistic representation. 2. The subword-based approach used in English language models is not suitable for Korean due to its distinct \"word-subword-character-subcharacter\" structure, which can overlook important linguistic information beyond characters."}
{"hash_id": 5178575029945553164, "entities": ["training efficiency", "human language acquisition", "syntactic performance", "inference efficiency costs"], "background": "1. The need to improve training efficiency in language models to be more similar to human language acquisition. 2. The desire to maintain or enhance syntactic performance without the inference efficiency costs associated with explicit syntactic structure generation."}
{"hash_id": 7092117678001142943, "entities": ["bypass llm security defenses", "implicit malicious clues", "jailbreak attacks", "llm alignment mechanisms"], "background": "1. The need to bypass the increasing security defenses of LLMs, which can easily recognize and defend against explicit malicious intent expressed in current jailbreak attacks. 2. The observation that providing implicit clues rather than explicit malicious prompts can lead to more effective jailbreak attacks that are harder to detect by LLM alignment mechanisms."}
{"hash_id": 861262826115011755, "entities": ["privacy security challenges", "real clinical notes", "clinical language model", "healthcare organizations"], "background": "1. The need to overcome privacy and security challenges that restrict the accessibility and use of real clinical notes for training large language models. 2. The demand for a clinical language model that can operate autonomously within healthcare organizations and be tailored to their specific needs without compromising patient privacy."}
{"hash_id": 483703772678706857, "entities": ["interpretable ai models", "black box models", "extractive rationalizing models", "user acceptance", "model transparency"], "background": "1. The need for more interpretable AI models, especially for complex and opaque black box models, to enhance trust and address potential biases. 2. The desire to improve the plausibility and faithfulness of explanations provided by extractive rationalizing models, which is crucial for user acceptance and model transparency."}
{"hash_id": 6331110321029668179, "entities": ["crosslingual vqa", "annotated datasets", "translation artifacts"], "background": "1. The limited availability of annotated datasets in various languages hinders the development of reliable cross-lingual visual question answering (VQA) systems. 2. The presence of translation artifacts, which are characteristics unique to machine-translated texts, significantly affects the performance of VQA models when using translate-test approaches."}
{"hash_id": 9185925595375875583, "entities": ["entity types", "argument comprehension", "semantic associations", "semantic integrity", "onesided understanding"], "background": "1. The need to explicitly explore the role of entity types in EAE to improve argument comprehension and accuracy. 2. The requirement to address the three main issues faced by existing methods: weak semantic associations, compromised semantic integrity, and one-sided semantic understanding."}
{"hash_id": 9181445856931460177, "entities": ["reduce memory overhead", "increase efficiency", "pacbayes regularizer", "generalization performance"], "background": "1. The need to reduce memory overhead and increase efficiency in fine-tuning large language models, especially as model size increases. 2. The desire to improve generalization performance, particularly in scenarios with limited training data, by incorporating a PAC-Bayes regularizer."}
{"hash_id": 7721010700649285918, "entities": ["textconditioned image retrieval", "smallscale datasets", "attribute semantics", "semantic correlations"], "background": "1. The existing methods in text-conditioned image retrieval struggle with small-scale datasets due to the labor-intensive process of data collection and the complexity of attribute semantics in modification texts. 2. Models tend to learn a generalized representation of the query, missing the specific semantic correlations between image and text attributes, leading to overfitting and poor performance on unseen queries."}
{"hash_id": 2407191681886878593, "entities": ["example selection paradigm", "internal relationships", "incontext learning", "sequential information", "contextuality relevance"], "background": "1. The existing \"select then organize\" paradigm for example selection neglects the internal relationships between examples, leading to inconsistencies in training and inference and suboptimal performance in in-context learning. 2. There is a need for a method that can capture the inter-relationships and sequential information among examples to enrich the contextuality and relevance of prompts for large language models."}
{"hash_id": 2868289005402150891, "entities": ["singleitem assessment", "llm capabilities", "comprehensive evaluation"], "background": "1. The need to discern whether LLMs genuinely possess capabilities or are simply memorizing/guessing answers due to the limitations of single-item assessment paradigms. 2. The requirement for a more robust and comprehensive evaluation that resists data contamination and reduces interference from potential biases in LLM assessments."}
{"hash_id": 3808099493600713733, "entities": ["privacy seesaw phenomenon", "privacy protection", "balanced approach"], "background": "1. The need to mitigate the Privacy Seesaw phenomenon, where protecting certain private information can lead to the exposure of other private data in large language models. 2. The requirement for a more effective and balanced approach to privacy protection that maintains model performance without compromising on privacy."}
{"hash_id": 3573239485185324380, "entities": ["highquality summaries", "summarization methods", "interpretability", "multidocument summarization"], "background": "1. The lack of in-depth analysis of the nature and components of high-quality human-written multi-document summaries, which hinders the improvement of summarization methods. 2. The need for clear interpretability and understanding of how information from multiple documents contributes to a summary to enhance the performance of multi-document summarization systems."}
{"hash_id": 7415566855527074952, "entities": ["backdoor triggers", "wordspace purification", "clean accuracy", "model performance"], "background": "1. The need to counteract diverse backdoor triggers, including those in the feature space, which existing word-space purification methods are unable to address effectively. 2. The requirement to maintain high clean accuracy while removing backdoor triggers, as current methods significantly impair the model's performance on clean data."}
{"hash_id": 3230795522201945905, "entities": ["textbased understanding", "chemical reactions", "moleculerelevant tasks", "opensource dataset", "predicting experimental procedures"], "background": "1. The need to improve text-based understanding of chemical reactions and single molecules for molecule-relevant tasks, which is currently limited in existing works. 2. The absence of an open-source dataset and effective methods for predicting experimental procedures, a crucial task for automating chemical synthesis."}
{"hash_id": 4063174337649979498, "entities": ["improving answer accuracy", "medvqa challenges", "language models integration", "medical knowledge integration"], "background": "1. The need to improve the accuracy and relevance of answers to open-ended questions in Med-VQA, which are currently challenged by the complexity and depth of medical knowledge required. 2. The gap in integrating advanced language models with medical images and external medical knowledge to enhance question answering capabilities in the medical field."}
{"hash_id": 7354288840776098844, "entities": ["ordinal classification tasks", "ordinal nature", "sentiment analysis", "pretrained language models", "nuanced classification"], "background": "1. The need to improve the performance of ordinal classification tasks by accounting for the ordinal nature of labels, which is crucial for applications like sentiment analysis and rating prediction. 2. The potential to leverage the rich semantic information from pretrained language models to implicitly handle ordinality, which could lead to more nuanced and accurate classification."}
{"hash_id": 4658301593250949355, "entities": ["educational survey articles", "automation", "large language models"], "background": "1. The high cost and significant expert input required to create and update educational survey articles can be reduced through automation, potentially making such materials more accessible and up-to-date. 2. The effectiveness and limitations of Large Language Models (LLMs) in the domain-specific task of generating scientific surveys have not been thoroughly investigated, despite their success in general NLP tasks."}
{"hash_id": 5622562661677742366, "entities": ["performance degradation", "model collapse", "large language models", "benchmarking constraints"], "background": "1. The need to prevent significant performance degradation (model collapse) in large language models after editing, which can render them nearly unusable. 2. The impracticality of thoroughly benchmarking large language models after each edit due to the time and resource constraints."}
{"hash_id": 1807337226905279887, "entities": ["updated language models", "knowledge attenuation", "sequential updating"], "background": "1. The need to maintain updated and customized information within language models without the necessity of resource-intensive fine-tuning. 2. The desire to overcome the issue of knowledge attenuation that occurs when sequentially updating language models, which leads to a decrease in the retention of updated knowledge over time."}
{"hash_id": 7907139918916713556, "entities": ["cascaded methods", "endtoend models", "translation quality", "complex images"], "background": "1. To overcome the limitations of cascaded methods, such as error propagation and difficulties in deployment, while also addressing the modeling burden and pixel sequence prediction challenges faced by existing end-to-end models. 2. To improve translation quality and handle more complex images with multiple lines of text, rotation, and translation, while also maintaining the visual characteristics of the input image."}
{"hash_id": 7357556535390180059, "entities": ["multilingual large language models", "german language", "open government data", "natural language queries"], "background": "1. To bridge the gap in\u8bc4\u4f30 Large Language Models (LLMs) in multilingual settings, especially for languages like German which are important in the context of Switzerland's multilingual society. 2. To improve access to open government data in Switzerland by developing a system that allows citizens to interact with the data through natural language queries, thus enhancing democratic processes by reducing misinformation."}
{"hash_id": 3773079890138134360, "entities": ["differentiate aigenerated content", "watermark robustness", "text quality preservation", "removal attacks"], "background": "1. The need to differentiate between AI-generated content and human-written text to prevent issues of authenticity, trust, and potential copyright violations. 2. The challenge of balancing watermark robustness with the preservation of text quality in the face of various removal attacks."}
{"hash_id": 8554198597671508635, "entities": ["key feature forgetting", "crossevent argument confusion", "deae tasks", "large language models", "fewshot settings"], "background": "1. The need to overcome the challenges of key feature forgetting and cross-event argument confusion in D-EAE tasks, which current models struggle with due to interference from event-independent information and difficulty in reasoning about implicit event associations. 2. The potential to harness the emergence capabilities of large language models (LLMs) to improve the extraction of complex event arguments, especially in few-shot settings where limited training data is available."}
{"hash_id": 6305720106838810549, "entities": ["label noise mitigation", "distant supervision", "teacherstudent methods", "calibration issue", "error propagation"], "background": "1. The need to mitigate the issue of label noise that arises from the use of distant supervision in NER, which can lead to poor performance due to incorrectly pseudo-labeled samples. 2. The limitation of existing teacher-student methods in achieving robust performance because of the poor calibration of the teacher network, which results in error propagation."}
{"hash_id": 2216052147196498082, "entities": ["model learning", "classspecific properties", "instancelevel metrics", "classwise hardness", "lowresource scenarios"], "background": "1. The need to understand how class-specific properties affect model learning and generalizability across datasets. 2. The recognition that instance-level metrics are insufficient for measuring class-wise hardness, which is crucial for effective sample selection and task learning in low-resource scenarios."}
{"hash_id": 4940012289315556725, "entities": ["enhance llm robustness", "selection biases", "zeroshot setting"], "background": "1. To enhance the robustness and reliability of Large Language Models (LLMs) in selection problems, which are crucial for various downstream applications. 2. To address the lack of understanding and quantification of selection biases related to option order and token usage in LLMs, particularly in the zero-shot setting."}
{"hash_id": 7468375550282774753, "entities": ["arabic language models", "comprehensive benchmark", "culturally localized content"], "background": "1. The need for a comprehensive benchmark to evaluate the reasoning and knowledge capabilities of large language models in Arabic, given the limited availability of Arabic-specific datasets. 2. The importance of culturally and regionally localized content to ensure that language models can effectively understand and generate contextually appropriate responses in Arabic-speaking contexts."}
{"hash_id": 30731002561956610, "entities": ["extreme multilabel classification", "scalability challenges", "dynamic label integration", "catastrophic forgetting"], "background": "1. The need to effectively handle the scalability and dynamic growth challenges in extreme multi-label classification tasks, where traditional methods struggle with the increasing label set and complex mapping relationships. 2. The requirement to integrate new labels without the need for extensive retraining to avoid catastrophic forgetting and reduce computational resources."}
{"hash_id": 6318951773893844325, "entities": ["large language models", "updated benchmarks", "chinese finance", "evaluation datasets"], "background": "1. The rapid advancements in large language models have created a need for updated benchmarks that can accurately assess their capabilities, particularly in specialized domains like Chinese finance. 2. Existing Chinese financial evaluation datasets are limited in size, diversity, and the range of tasks they cover, which hinders a comprehensive evaluation of LLM performance."}
{"hash_id": 8321172369905950724, "entities": ["reinforcement learning methods", "finegrained supervision", "complex reasoning tasks", "instancelevel rewards", "optimization conflicts"], "background": "1. Existing reinforcement learning methods lack fine-grained supervision, which is essential for complex reasoning tasks, leading to inefficiency and redundant supervision. 2. Instance-level rewards fail to accurately emphasize the important evidence related to correctness and can result in optimization conflicts due to overlapping content in correct and incorrect outputs."}
{"hash_id": 1376191587388195087, "entities": ["semantic change detection", "interpretable methods", "predefined sense inventories"], "background": "1. The need for more interpretable and human-readable semantic change detection methods that can explain the nature of semantic shifts. 2. The limitation of existing methods that rely on pre-defined sense inventories, which may miss important senses, especially in recent text data, and are not available for many languages."}
{"hash_id": 1292247859343779604, "entities": ["instruction alignment", "finetuning methods", "gradual learning"], "background": "1. The need to bridge the gap between a language model's basic next-word prediction ability and its ability to follow complex instructions, which is not effectively addressed by existing one-off fine-tuning methods. 2. The recognition that learning to follow instructions is a gradual process that requires progressive alignment of the model's capabilities with human intent, rather than a single, uniform fine-tuning step."}
{"hash_id": 4899215832784033217, "entities": ["topic relevance automation", "student essays", "large language models", "chinese essays feedback"], "background": "1. The need to automate the assessment of topic relevance in student essays to reduce teachers' workload and enhance students' writing skills. 2. The shortcomings of existing Large Language Models like GPT-4, which often provide incorrect judgments and impractical feedback on topic relevance in Chinese essays."}
{"hash_id": 2726698606033866191, "entities": ["indeterminacy future events", "dynamic knowledge graphs", "deterministic methods", "temporal structural relationships"], "background": "1. Existing TKG reasoning approaches are limited in capturing the indeterminacy of future events, especially for rare or unseen facts, which is critical for accurate predictions in dynamic knowledge graphs. 2. The deterministic methods in current TKG reasoning techniques fail to account for the evolving understanding of temporal and structural relationships, leading to biases towards frequently occurring scenarios and a lack of adaptability to emerging relationships."}
{"hash_id": 4971163897003613274, "entities": ["control accuracy", "generative capabilities", "llm decoding capabilities", "text fluency"], "background": "1. The need to improve control accuracy over specific attributes in generated text without negatively impacting the generative capabilities and text quality of large language models. 2. The recognition that traditional controlled text generation methods using small language models to steer LLM outputs can diminish the decoding capabilities of LLMs and reduce text fluency."}
{"hash_id": 587678216325176759, "entities": ["commonsense reasoning", "language models", "commonsense knowledge graphs"], "background": "1. The need to improve the limitedcommonsense reasoning abilities of language models, especially smaller models which are more vulnerable to spurious contextualization. 2. The requirement for a method to contextualize structured knowledge from Commonsense Knowledge Graphs (CSKGs) effectively, without relying on the limited commonsense capabilities of language models alone."}
{"hash_id": 8107014463382762257, "entities": ["improving large language models", "factual content generation", "computational efficiency"], "background": "1. The need to improve the accuracy and reliability of large language models in generating factual content, especially across different languages. 2. The desire to enhance the portability and computational efficiency of knowledge editing methods without inducing catastrophic forgetting."}
{"hash_id": 2131001658009804639, "entities": ["opensource large language models", "medical domain adaptation", "lightweight models", "consumergrade device performance"], "background": "1. The need for open-source, specialized Large Language Models (LLMs) that can be adapted for the medical domain to ensure privacy and accessibility. 2. The requirement for lightweight models that can perform competitively with larger, proprietary models to facilitate their use on consumer-grade devices."}
{"hash_id": 4305353797433721107, "entities": ["multilingual safety", "large language models", "nonenglish queries"], "background": "1. The need to ensure the safety of large language models (LLMs) across multiple languages, as previous benchmarks only focused on one language, typically English, ignoring the safety concerns in other languages used globally. 2. The requirement to improve the multilingual safety of LLMs, particularly since LLMs are increasingly being deployed in diverse linguistic settings and are producing significantly more unsafe responses for non-English queries."}
{"hash_id": 9052048590465809041, "entities": ["ljp model behaviors", "performance improvement strategies", "humancentered nature", "legal aspects fairness"], "background": "1. The need for a more in-depth understanding of LJP model behaviors, weaknesses, and performance improvement strategies beyond generic metrics like accuracy and F1 scores. 2. Ensuring that LJP models, which have a human-centered nature, are evaluated for crucial legal aspects such as fairness, given their real-world impact on judgments."}
{"hash_id": 8635471210610145286, "entities": ["multidefendant legal judgment", "research datasets", "stateoftheart approaches"], "background": "1. The lack of research and datasets focusing on multi-defendant legal judgment prediction, despite the complexity and significance of such cases in real-world scenarios. 2. The need to improve the performance of existing state-of-the-art (SOTA) approaches when applied to the multi-defendant setting, as they have been primarily designed and tested on single-defendant cases."}
{"hash_id": 9098968350585964912, "entities": ["model editing approach", "standard finetuning", "competitive performance"], "background": "1. The need for a model editing approach that is simple, agnostic to model architecture, and can leverage advances in standard training techniques without additional work. 2. The desire to achieve competitive performance with specialized model editors using standard fine-tuning, which is more accessible and does not require complex model-specific assumptions."}
{"hash_id": 8912732711941674652, "entities": ["logical reasoning datasets", "data annotation", "automated data augmentation"], "background": "1. The scarcity of large-scale logical reasoning datasets hinders the performance of large language models on tasks requiring logical reasoning. 2. Data annotation for logical reasoning tasks is difficult, time-consuming, and costly, necessitating an automated method for data augmentation."}
{"hash_id": 7408629193925490454, "entities": ["social biases", "language models", "news summarization", "experimental setup"], "background": "1. The need to understand and quantify the presence of social biases in the outputs of large language models used for news summarization, given their potential to reinforce harmful biases. 2. The requirement for a controlled experimental setup to accurately attribute biases in summarization to the model itself, rather than the biases inherent in the input documents."}
{"hash_id": 7948274733287762432, "entities": ["error prevention", "safetycritical domains", "llm trustworthiness", "confidence standard"], "background": "1. The need to prevent errors in safety-critical domains where LLMs are used, which could arise from trusting incorrect or nonsensical text generated by LLMs. 2. The absence of a reliable gold standard for confidence that directly reflects response quality, leading to a lack of trustworthiness in LLM outputs."}
{"hash_id": 3195535806415881993, "entities": ["object hallucinations", "lvlms", "mitigation strategies"], "background": "1. The need to mitigate object hallucinations in LVLMs, which are caused by over-reliance on text cues and learned object co-occurrence biases, without requiring complex post-processing or retraining. 2. The lack of specific hallucination-mitigation strategies for LVLMs, despite progress in hallucination evaluation benchmarks and the existence of mitigation techniques for LLMs."}
{"hash_id": 4278511087839889737, "entities": ["enhancing ocr accuracy", "lowresource languages", "postocr correction", "efficient training data"], "background": "1. The need to improve accessibility and preservation of historical and culturally important books by enhancing the accuracy of OCR outputs, particularly for low-resource languages. 2. The requirement to develop a more efficient and less resource-intensive approach to generating training data for post-OCR correction models, in order to reduce dependency on expensive and time-consuming crowdsourcing."}
{"hash_id": 7411641590240333635, "entities": ["finegrained nonstandard addresses", "spatial knowledge", "efficient geographic knowledge update"], "background": "1. The limitations of existing methods in handling fine-grained non-standard addresses and the lack of spatial knowledge in large language models. 2. The need for a more efficient approach to update geographic knowledge due to the frequent changes in address information."}
{"hash_id": 9101270196449041587, "entities": null, "background": null}
{"hash_id": 164277844758453135, "entities": ["high cost alignment methods", "human preferences", "model alignment", "subtlety complexity preferences"], "background": "1. The high cost and limited scalability of current alignment methods that rely on human preferences for improving LLMs. 2. The need for a method that can capture the subtlety and complexity of human preferences beyond simple rankings or scores to enhance model alignment."}
{"hash_id": 7942488854827511116, "entities": ["misuse of llms", "fake news", "detectgpt inefficiency"], "background": "1. The need to prevent the misuse of LLMs, such as the creation of fake news and cheating, which can lead to significant social problems. 2. The inefficiency of existing methods, like DetectGPT, which require a large number of queries to the source LLM, making them impractical for handling large models."}
{"hash_id": 3669611790218606646, "entities": ["personal drug experiences", "harm reduction strategies", "usergenerated content", "substance use disorders"], "background": "1. To bridge the gap in understanding the diverse personal drug experiences of PWUD, which are often marginalized and can provide valuable insights for harm reduction and intervention strategies. 2. To leverage the rich dataset of user-generated content on platforms like Reddit to inform a broader knowledge base on substance use disorders (SUD) and drug use, thereby contributing to more effective support mechanisms for PWUD."}
{"hash_id": 2442758340797246440, "entities": ["heading generation", "summarization quality", "algorithmic constraints"], "background": "1. The lack of a systematic study and formal benchmark for evaluating the quality of heading generation in terms of summarization, neology, and algorithmic aspects. 2. The need for techniques that can balance the multiple constraints of summarization, neologistic creativity, and algorithmic structure in creating effective headings."}
{"hash_id": 4681736728936627939, "entities": ["harmful effects of science communication", "finegrained distortions", "detecting distortions"], "background": "1. The need to understand and prevent the harmful effects of distorted science communication on individuals and society, which can lead to unhealthy behavior changes and decreased trust in scientific institutions. 2. The lack of existing work that focuses on fine-grained distortions in scientific reporting and the challenges of detecting these distortions automatically."}
{"hash_id": 6591577643101427360, "entities": ["bridging humanmachine understanding", "multimodal content", "social media misinformation", "mllm benchmarking"], "background": "1. The need to bridge the gap between human and machine understanding of multimodal content in social media platforms, which is essential for tasks like detecting misinformation and understanding human emotions. 2. The requirement for a comprehensive benchmark to evaluate and highlight the current limitations of MLLMs in handling social media tasks that involve complex social contexts."}
{"hash_id": 9195882784256162075, "entities": ["zeroshot performance", "llms", "weaker llms leverage"], "background": "1. The need to enhance the zero-shot performance of LLMs, particularly when generic trigger phrases are insufficient for complex tasks. 2. The potential to leverage weaker LLMs to improve the performance of stronger ones, which could lead to more ethical and accurate responses."}
{"hash_id": 2892512161356479765, "entities": ["hallucinations in medical responses", "retrievalaugmented generation", "rag implementation in healthcare"], "background": "1. To mitigate the issue of hallucinations and outdated knowledge in medical responses generated by large language models, which can be dangerous in high-stakes domains like healthcare. 2. To provide a comprehensive evaluation and best practices for implementing retrieval-augmented generation (RAG) systems in the medical field, given the lack of clarity on the optimal settings for different components of RAG."}
{"hash_id": 5470895998848165365, "entities": ["generalizing large language models", "musical composition intricacies", "musictext connection"], "background": "1. The need to generalize the capabilities of Large Language Models (LLMs) to the domain of music, which is a fundamental form of human creativity. 2. The recognition that current LLMs do not adequately represent the intricacies of musical composition, such as long-term context dependency and the connection between music notes and text descriptions."}
{"hash_id": 4318253186950729744, "entities": ["multihop temporal reasoning", "multianswer temporal reasoning", "temporal robustness of language models"], "background": "1. The lack of datasets and methods that specifically target multi-hop and multi-answer temporal reasoning, which are common in real-world scenarios. 2. The need to improve the temporal robustness of large language models without frequent and costly updates."}
{"hash_id": 3809850541692869096, "entities": ["prompt template", "incontext learning", "prompt sensitivity"], "background": "1. The prompt template significantly affects in-context learning performance but is often overlooked or inconsistently applied in evaluations, leading to potential misinterpretations of results. 2. There is a need for a practical solution to mitigate the sensitivity of in-context learning methods to the choice of prompt template, as the best template does not consistently transfer across models or setups."}
{"hash_id": 1843360943701620757, "entities": ["factual accuracy", "external knowledge", "binary judgments", "knowledge usage", "indirectly relevant knowledge"], "background": "1. To improve the factual accuracy of LLM-generated outputs by incorporating external knowledge from KGs without the need for extensive computational resources. 2. To address the limitations of existing methods that rely on LLMs to perform binary judgments on knowledge usage, resulting in low flexibility and the potential omission of indirectly relevant knowledge."}
{"hash_id": 7125076062909922883, "entities": ["hallucinations detection", "omissions detection", "wordlevel distinction"], "background": "1. The need to improve the detection of hallucinations and omissions in Machine Translation without relying on the system's internal states or external tools not specifically designed for this task. 2. The requirement for a method that can distinguish between hallucination and omission errors at the word level, which existing approaches do not adequately address."}
{"hash_id": 5329018671897980744, "entities": ["event prediction models", "evolving data", "longterm causal interactions", "historical contexts", "input length constraints"], "background": "1. The need to improve the adaptability of event prediction models to evolving data and new causal relationships without requiring extensive retraining. 2. The requirement to capture long-term causal interactions and historical contexts that are missed by traditional methods due to input length constraints in large language models."}
{"hash_id": 1144380046736578444, "entities": ["large language models", "slot filling tasks", "finegrained content extraction"], "background": "1. The emergence of large language models (LLMs) with human-level reasoning abilities without task-specific fine-tuning, which could potentially revolutionize slot filling tasks. 2. The need to improve the performance of LLMs on fine-grained content extraction like slot filling, particularly when dealing with noisy ASR transcriptions and limited data."}
{"hash_id": 4194823374629695435, "entities": ["early diagnosis alzheimers", "neurodegenerative conditions", "cognitive reserve", "artificial neural networks"], "background": "1. The need for efficient and cost-effective screening tools for early diagnosis of Alzheimer's disease and other neurodegenerative conditions to mitigate the negative consequences of delayed diagnosis. 2. The potential to model and understand the concept of cognitive reserve in artificial neural networks, which could provide insights into how individuals with higher cognitive abilities may be more resilient to cognitive decline."}
{"hash_id": 655649736052514127, "entities": ["lowresource languages", "abstractive text summarization", "hebrew morphological complexity"], "background": "1. The lack of resources and evaluation benchmarks for abstractive text summarization in low-resource languages, particularly morphologically rich languages like Hebrew, which hinders the advancement of large language models in these languages. 2. The need to understand and address the unique challenges posed by Hebrew's morphological complexity in order to improve the performance of generative language tasks."}
{"hash_id": 8582453805322280505, "entities": ["temporal reasoning", "standardized benchmarks", "language models", "humanlevel proficiency"], "background": "1. The lack of standardized benchmarks for evaluating temporal reasoning capabilities in large language models, which hinders consistent and comparable research progress in this area. 2. The significant gap between the temporal reasoning performance of current language models and human-level proficiency, indicating a need for improved models and better evaluation methods."}
{"hash_id": 1105576180413565812, "entities": ["expressing uncertainty", "preventing false answers", "openended questionanswering"], "background": "1. To improve the capability of LLMs to express uncertainty, particularly in scenarios where questions have high uncertainty due to the absence of definitive answers. 2. To prevent LLMs from confidently generating false answers (hallucinations) and to calibrate the certainty of their statements in open-ended question-answering scenarios."}
{"hash_id": 7827348223205438258, "entities": ["defeasibility in causal reasoning", "dynamic causeeffect relationships", "causal strength metrics", "supporters or defeaters"], "background": "1. The need to account for defeasibility in causal reasoning to more accurately reflect the dynamic nature of cause-effect relationships. 2. The requirement for improved causal strength metrics that can capture changes in causal strength due to the presence of supporters or defeaters."}
{"hash_id": 324536335158014857, "entities": ["training data", "nlp models", "promptdriven data generation"], "background": "1. The need for abundant, high-quality training data for building dependable NLP models, which is often not available for many use cases and is labor-intensive to curate manually. 2. The limitations of existing prompt-driven synthetic data generation methods, which tend to produce datasets that lack complexity and diversity."}
{"hash_id": 7704923172093776527, "entities": ["causal language models", "performance disparities", "demonstration orders", "model robustness"], "background": "1. The sensitivity of causal language models to the order of in-context demonstration examples can lead to significant performance disparities, which hinders their robustness and generalizability. 2. Existing studies focus on identifying optimal permutations rather than fundamentally improving the models' robustness against different demonstration orders."}
{"hash_id": 8124714778615150924, "entities": ["empathy personalized responses", "theory of mind", "hidden emotion recognition", "healthcare applications"], "background": "1. The need to improve language models' understanding and explanation of the beliefs of others, which is crucial for tasks requiring empathy and personalized responses. 2. The desire to enhance the ability of language models to engage in theory of mind, particularly in challenging scenarios such as hidden emotion recognition, to make them safer and more effective in various applications like healthcare."}
{"hash_id": 2737405758749168870, "entities": ["improve llms performance", "limited taskspecific data", "dataconstrained domains"], "background": "1. The need to improve the performance of large language models (LLMs) in real-world applications with limited task-specific data, where traditional fine-tuning is challenging. 2. The requirement to reduce dependency on labor-intensive data curation and to make LLM solutions more scalable and performant, especially in data-constrained domains."}
{"hash_id": 7023676898874937714, "entities": ["multidocument summarization", "dedicated datasets", "heuristic alignment methods", "sentencelevel approach"], "background": "1. The decomposed subtasks of multi-document summarization often lack dedicated training and evaluation datasets, hindering progress in improving these individual components. 2. Existing heuristic alignment methods are limited in their sentence-level approach and do not fully exploit the potential of alignments for creating diverse datasets."}
{"hash_id": 167773712241206355, "entities": ["highquality responses", "annotation efforts", "large language models", "active learning", "computational cost"], "background": "1. The annotation efforts required to produce high-quality responses for instructions in large language models are becoming prohibitively expensive, especially as the number of tasks increases. 2. Active learning, while effective in reducing annotation costs, has a high computational cost that is not feasible for the expensive inference and retraining processes of large language models."}
{"hash_id": 3088584250228282860, "entities": ["multimodal information fusion", "noncompositional effects", "modalities contrast", "hateful joyful detection"], "background": "1. The need to improve the understanding of multimodal information fusion, particularly when visual and textual content is not semantically aligned, leading to noncompositional effects like irony or humor. 2. The limitations of existing approaches in effectively recognizing the contrast between image and text modalities, which hinders their ability to detect hateful or joyful information in multimodal content."}
{"hash_id": 787362351589584328, "entities": ["aspectbased sentiment analysis", "unified approach", "ecommerce social media platforms"], "background": "1. The need to improve the performance of Aspect-Based Sentiment Analysis (ABSA) across various subtasks, which is crucial for understanding user-generated content in e-commerce and social media platforms. 2. The desire to simplify the design of ABSA models by using a unified approach that can be extended to any subtask, thereby reducing complexity and enhancing practicality."}
{"hash_id": 6436967032367722263, "entities": ["language model biases", "emotional moral tones", "societal polarization", "affective alignment framework"], "background": "1. To understand and mitigate the potential biases in language models that could lead to unequal representation of emotional and moral tones across different social groups, which can have downstream effects on societal polarization and inequity. 2. To provide a framework for assessing affective alignment that can guide the development of more inclusive and representative AI systems."}
{"hash_id": 8577764871467098014, "entities": ["manual tuning burden", "decoding hyperparameters", "llms autonomy", "selfregulatory capabilities"], "background": "1. The need to reduce the burden of manual tuning of decoding hyperparameters for LLMs, which can be time-consuming and may not always result in optimal performance for each input sample. 2. The desire to enhance the autonomy and self-regulatory capabilities of LLMs, allowing them to adapt their decoding strategies based on the context of the input without external intervention."}
{"hash_id": 1393453240366243664, "entities": ["mathematical verification tasks", "llms", "reasoning chains", "verification process"], "background": "1. The need to improve the reliability of mathematical verification tasks for LLMs, as current methods like Self-Consistency have limitations in accuracy, especially when the number of reasoning chains sampled is already large. 2. The recognition that a significant number of failure cases in existing methods have at least one reasoning chain reaching the correct answer, indicating that a more effective verification process could lead to better overall performance."}
{"hash_id": 3416551567199594798, "entities": ["improving language agent efficiency", "reducing external reliance", "uncertainty measurement", "decisionmaking for agents"], "background": "1. The need to improve the efficiency of language agents by reducing their reliance on external resources and better utilizing the implicit knowledge encoded in Large Language Models (LLMs). 2. The recognition that uncertainty is an important aspect of decision-making for intelligent agents, and that current language agents lack a reliable mechanism for measuring uncertainty."}
{"hash_id": 4390318506897907641, "entities": ["mental health issues", "highpressure lifestyles", "professional psychological support", "affordable care", "counseling access"], "background": "1. The need to address the rise in mental health issues due to the rapid pace of modern life and the prevalence of high-pressure lifestyles, which have led to an increased risk of negative emotions and thought patterns. 2. The challenge faced by individuals in accessing affordable and timely professional psychological support due to the high cost of counseling and long waiting times for appointments."}
{"hash_id": 53996823483494309, "entities": ["large language models", "temporal knowledge graph", "structural temporal integration", "complex question reasoning"], "background": "1. The limited ability of large language models (LLMs) to directly answer temporal knowledge graph questions due to the complexity of temporal constraints and the need for relevant subgraph retrieval. 2. The challenge of integrating structural and temporal information from knowledge graphs into LLMs in a non-shallow way to improve reasoning on complex question types."}
{"hash_id": 3062676252896473960, "entities": ["validity verification", "unanswerable questions", "multistep reasoning"], "background": "1. The need to verify the validity of questions before answering them to ensure reliable performance in real-world applications where users may provide imperfect instructions. 2. The gap in existing datasets and models that fail to robustly handle unanswerable questions and multi-step reasoning over clusters of objects with shared attributes or relations."}
{"hash_id": 4720976490111806147, "entities": ["dubbed speech alignment", "phoneme level learning", "pronunciation habits", "emotional nuances"], "background": "1. The need to improve the alignment of dubbed speech with video in both time and emotion, while maintaining clear phoneme pronunciation and identity stability. 2. The requirement for a more effective approach to learn and adapt the style of speech at the phoneme level, which is crucial for capturing pronunciation habits and emotional nuances."}
{"hash_id": 176745966684675225, "entities": ["computational cost reduction", "transformer architectures", "zerocost proxies", "performance prediction", "across tasks"], "background": "1. The need to reduce the high computational cost associated with evaluating numerous Transformer architectures during the search process. 2. The lack of theoretically grounded zero-cost proxies that can accurately predict the performance of Transformer networks across various tasks, especially beyond specific domains like computer vision or natural language processing."}
{"hash_id": 350326915186334380, "entities": ["enhancing transparency", "medical dialogue systems", "diagnostic reasoning", "clinician preferences"], "background": "1. The need to enhance the transparency and trustworthiness of medical dialogue systems by aligning their responses with clinicians' diagnostic reasoning processes. 2. The desire to improve the accuracy and effectiveness of medical consultations by incorporating a deeper understanding of clinicians' internal thought processes and preferences."}
{"hash_id": 5594199530701176002, "entities": ["finegrained benchmark", "mathematical reasoning", "llms capabilities"], "background": "1. The need for a fine-grained benchmark that can assess LLMs' concept-wise mathematical reasoning, as current benchmarks fail to probe specific failure modes on individual mathematical concepts. 2. The desire to improve the mathematical reasoning capabilities of LLMs in specific areas, particularly for practical applications where certain mathematical concepts are more important than others."}
{"hash_id": 2932925710592786255, "entities": ["difficulty of manual annotation", "large language models", "automatic annotation limitations"], "background": "1. The need to reduce the difficulty and cost of manually annotating high-quality instruction data for large language models. 2. The desire to overcome the limitations of current automatic annotation methods that rely on proprietary LLMs, which can limit the quality of the generated data and raise copyright issues."}
{"hash_id": 4272685078019328563, "entities": ["improve reasoning capabilities", "distilling knowledge", "chainofthought", "distilling stepbystep method"], "background": "1. The need to improve the reasoning capabilities of smaller language models by distilling knowledge from larger ones, while ensuring effective integration of Chain-of-Thought (CoT) knowledge. 2. The desire to address the limitations of the existing Distilling Step-by-Step (DSS) method, which overlooks the intrinsic relationship between the tasks of label prediction and rationale generation."}
{"hash_id": 5223662971814003968, "entities": ["computational storage costs", "finetuning", "parameterefficient methods", "taskspecific knowledge"], "background": "1. The need to reduce the computational and storage costs of fine-tuning large-scale pre-trained language models for each individual task. 2. The requirement to improve the performance of parameter-efficient fine-tuning methods by better exploiting task-specific knowledge and the correlation between source and target tasks."}
{"hash_id": 6607363194780902791, "entities": ["large language models", "incontext learning", "attention heads", "semantic relationships"], "background": "1. The need to enhance the transparency and trustworthiness of large language models by understanding their internal mechanisms, particularly in the context of in-context learning. 2. The desire to uncover the roles of attention heads in encoding semantic relationships, which are crucial for natural language understanding but have been largely unexplored."}
{"hash_id": 5098903048089955856, "entities": ["object hallucinations", "large visionlanguage models", "computational resources", "external models"], "background": "1. The existence of object hallucinations in large vision-language models hinders their broader applications due to the unreliable output they produce. 2. Existing methods to mitigate object hallucinations either require significant computational resources or rely on external models, which may not be efficient or scalable."}
{"hash_id": 6370798104381081936, "entities": ["opendomain question answering", "adaptive retrievalaugmented generation", "retrieval necessity"], "background": "1. The need to enhance the efficiency and relevance of retrieved information in open-domain question answering by dynamically determining the necessity of retrieval. 2. The lack of suitable benchmarks and evaluations for assessing the effectiveness of adaptive retrieval-augmented generation (ARAG) methods."}
{"hash_id": 3377114356804397107, "entities": ["endtoend speech translation", "large language models", "performance efficiency"], "background": "1. To overcome the limitations of current end-to-end speech translation models, which often require extensive training data and complex architectures to achieve strong performance. 2. To effectively harness the potential of large language models (LLMs) to improve speech translation quality and scalability without compromising efficiency."}
{"hash_id": 2345053875826376806, "entities": ["reward models", "multitask disturbance", "human annotations", "consistency", "large language models"], "background": "1. To improve the generalization performance of reward models by mitigating multi-task disturbance and reducing the impact of noisy human annotations. 2. To enhance the consistency of reward models with human preferences and optimize large language models more effectively."}
{"hash_id": 2090480310825014637, "entities": ["crosslingual knowledge transfer", "large language models", "resourcerich languages", "crosslingual supervision", "performance gap"], "background": "1. The need to improve cross-lingual knowledge transfer in large language models to broaden their application beyond resource-rich languages like English. 2. The potential to leverage cross-lingual supervision to bridge the performance gap between English and non-English languages in LLMs."}
{"hash_id": 3042208175288381142, "entities": ["dedicated benchmark", "mixedsourced data", "source bias issue", "neural retrieval models"], "background": "1. The lack of a dedicated benchmark for evaluating IR models in the mixed-sourced data landscape of the LLM era, which includes both human-written and LLM-generated content. 2. The need to understand and mitigate the source bias issue where neural retrieval models preferentially rank LLM-generated content over semantically equivalent human-written content."}
{"hash_id": 1154418363929639469, "entities": ["dialogue systems adaptation", "advanced reasoning transfer", "deployable models efficiency"], "background": "1. The need for dialogue systems to continually adapt to new services without forgetting previously learned tasks, which is crucial for practical applications. 2. The challenge of efficiently transferring advanced reasoning capabilities from large language models (LLMs) to smaller, more deployable models without incurring significant computational costs."}
{"hash_id": 3627925115265060296, "entities": ["aigenerated text", "student plagiarism", "fake news", "ai paraphrasing", "detecting partially paraphrased texts"], "background": "1. The increasing concern over potential misuse of AI-generated text, such as student plagiarism and the spread of fake news, which can be exacerbated by AI paraphrasing. 2. The lack of work focused on detecting partially AI-paraphrased texts, despite the common use of AI paraphrasing in text refinement and diversity."}
{"hash_id": 4759458430702936954, "entities": ["steering llms", "user preferences", "alignment technologies", "unclear boundaries"], "background": "1. The need to effectively steer LLMs to satisfy diverse user preferences and comply with various regulations, especially given the complexity and diversity of human values. 2. The failure of current alignment technologies to address the challenges of unclear boundaries between controlling instructions and regular instructions, leading to model responses being confused or hijacked."}
{"hash_id": 2380479148431024566, "entities": ["computational cost", "overfitting risk", "temporal sparsity", "video correlation"], "background": "1. The high computational cost and risk of overfitting when fully fine-tuning large-scale pre-trained vision-language models for text-video retrieval. 2. The need to capture temporal sparsity and correlation in video data to improve efficiency and effectiveness in text-video retrieval tasks."}
{"hash_id": 2885219648651824579, "entities": ["longtext machine translation", "llm performance", "realworld application", "rigorous benchmark"], "background": "1. The current performance of LLMs in long-text machine translation is not well understood or optimized, which hinders their potential application in real-world scenarios. 2. There is a need for a rigorous benchmark to evaluate the capabilities and limitations of LLMs in handling long documents, which can guide the development of improved models."}
{"hash_id": 4926484474393446513, "entities": ["user engagement", "dialogue coherence", "subsequent topic prediction", "user preferences"], "background": "1. The need to enhance user engagement and dialogue coherence by accurately predicting subsequent topics that align with user preferences and personas. 2. The requirement to overcome the limitations of existing models that indiscriminately integrate side information, leading to the prediction of user-uninteresting and contextually irrelevant topics."}
{"hash_id": 5259500433280261139, "entities": ["incontext learning", "performance fluctuations", "demonstration bias", "semantic ambiguity"], "background": "1. The sensitivity of in-context learning (ICL) to the selection and ordering of demonstrations leads to performance fluctuations and a lack of robustness in LLMs. 2. The need to understand and mitigate the underlying causes of demonstration bias, particularly the semantic ambiguity that affects task comprehension in ICL."}
{"hash_id": 8167769380132529962, "entities": ["training data scarcity", "taskoriented dialog systems", "data augmentation"], "background": "1. The scarcity of training data hinders the effectiveness and reliability of task-oriented dialog systems, especially when training individual modules separately. 2. Data augmentation has shown promise in other NLP tasks but has not been extensively explored or systematically compared in the context of end-to-end task-oriented dialog systems."}
{"hash_id": 5737713289721480243, "entities": ["communication gap", "sign language", "phonological differences", "data scarcity"], "background": "1. The need to bridge the communication gap between sign and non-sign language users, particularly for the deaf and hearing-impaired community. 2. The challenge of handling phonological differences and data scarcity in the context of sign language production."}
{"hash_id": 4112376152468309767, "entities": ["visual dsl representations", "multimodal reasoning", "multistep reasoning"], "background": "1. The need to effectively utilize the strengths of both visual and DSL representations in large vision-language models for complex multimodal reasoning tasks. 2. The requirement to improve multi-step reasoning capabilities and address critical steps within complex problems, which are currently hindered by inconsistencies in reasoning processes between modalities."}
{"hash_id": 8320747611388450770, "entities": ["computational efficiency", "transformer models", "hidden dimensions", "parameter reduction"], "background": "1. The need to reduce the computational and parameter overhead of Transformer models to enhance their efficiency and enable their deployment in resource-constrained environments. 2. The recognition that previous attempts to simplify FFNs have often reduced their hidden dimensions, compromising their expressive power, and the desire to maintain or increase hidden dimensions while still achieving parameter reduction."}
{"hash_id": 6496145447328973363, "entities": ["aspect sentiment quad prediction", "generative methods", "human cognitive processes"], "background": "1. To overcome the limitations of generative methods in aspect sentiment quad prediction, which suffer from imprecise predictions and limited interpretability due to data scarcity and inadequate modeling of the quadruplet composition process. 2. To enhance the model's ability to handle complex reasoning tasks and improve the accuracy of predicting sentiment quads by incorporating a strategy that mirrors human cognitive processes."}
{"hash_id": 3770868672157324506, "entities": ["performance discrepancies", "humanwritten prompts", "automated prompt editing", "iterative feedback loops"], "background": "1. The need to reduce the substantial performance discrepancies caused by the quality of human-written prompts in large language models, which can significantly impact the effectiveness of LLMs in various tasks. 2. The requirement for an automated approach to prompt editing that minimizes human effort and expertise, as the current process of improving prompts through iterative feedback loops is time-consuming and resource-intensive."}
{"hash_id": 8589440015604605119, "entities": ["multilingual incontext learning", "effectiveness of demonstrations", "variability across models and tasks"], "background": "1. The need to understand the role and effectiveness of demonstrations in multilingual in-context learning, which has been under-explored compared to English in-context learning. 2. The desire to determine the variability in the effectiveness of demonstrations across different models, tasks, and languages, and to assess whether current assumptions about demonstrations hold true in a multilingual context."}
{"hash_id": 2336971938657506100, "entities": ["bert architecture", "fewshot scenarios", "large language models", "csc tasks"], "background": "1. Existing CSC approaches based on BERT architecture do not perform well in few-shot scenarios, limiting their practical applications in real-world situations with rapidly changing catchwords and emerging incorrect sentences. 2. Large language models (LLMs) have shown significant potential in semantic analysis and could potentially improve the performance of CSC tasks by serving as a more robust foundation model."}
{"hash_id": 8326493719910991980, "entities": ["incontext learning", "machine translation quality", "icl demonstrations"], "background": "1. To gain a comprehensive understanding of how in-context learning influences machine translation quality, which is crucial for optimizing translation performance and overcoming the current limitations in knowledge about ICL's impact. 2. To investigate the effectiveness of different factors within ICL demonstrations, such as quality, quantity, and spatial proximity, to provide practical guidance for improving machine translation systems."}
{"hash_id": 625472014924254356, "entities": ["evaluation autoregressive models", "multiplechoice questions", "diverse response styles"], "background": "1. The need for more accurate evaluation of autoregressive large language models, especially in the context of multiple-choice questions where first-token probability may not align with the model's actual response. 2. The recognition that current evaluation methods may not adequately capture the diverse response styles and behaviors of language models, particularly those fine-tuned on conversational or safety data."}
{"hash_id": 6173581885699543939, "entities": ["limitation of large language models", "specialized knowledge", "knowledge graphs integration"], "background": "1. The need to overcome the limitations of large language models (LLMs) that struggle with questions requiring specialized knowledge beyond their pre-training content. 2. The desire to harness the full cognitive potential of knowledge graphs (KGs) by integrating them more effectively into the reasoning process alongside LLMs."}
{"hash_id": 6500158601762059423, "entities": ["jailbreak attacks", "llms", "harmful content", "research gap", "defense methodologies"], "background": "1. The need to mitigate the risk of Large Language Models (LLMs) generating harmful content due to jailbreak attacks, which bypass safety measures and produce malicious outputs. 2. The existence of a significant research gap in comprehensive evaluations of attack and defense methodologies against jailbreak attacks on LLMs."}
{"hash_id": 7700264866834095802, "entities": ["medical accuracy", "diagnostic captions", "medical conditions", "medical tags"], "background": "1. To enhance the medical accuracy of diagnostic captions by ensuring key medical conditions are considered during text generation, which can reduce diagnostic errors and improve the efficiency of clinical staff. 2. To leverage the potential of existing medical tags to guide the captioning process, thereby increasing the overall throughput of medical departments and decreasing the cost of medical imaging examinations."}
{"hash_id": 7272193236799691223, "entities": ["catastrophic forgetting", "finetuning", "large language models", "model versatility", "specific tasks"], "background": "1. The need to prevent catastrophic forgetting of previously acquired knowledge while fine-tuning large language models to enhance speciality in specific tasks. 2. The desire to improve model performance across diverse tasks by preserving versatility while targeting updates to specific modules that contribute to speciality."}
{"hash_id": 3841745519690535428, "entities": ["zeroshot relation triplet extraction", "semantics understanding", "labeled data dependency"], "background": "1. The limitations of existing methods in zero-shot relation triplet extraction, which lack sufficient understanding of the semantics of unseen relations, leading to poor generalization to new relation types. 2. The need to reduce dependency on large amounts of labeled data and improve the scalability and applicability of relation triplet extraction methods."}
{"hash_id": 3474899947682755001, "entities": ["computational costs reduction", "peft modules", "foundation model finetuning"], "background": "1. The need to reduce the computational costs and redundancy associated with using large-scale foundation models during fine-tuning for specific tasks. 2. The requirement to minimize the growth in trainable parameters and redundancy in PEFT modules as the size of the foundation model increases."}
{"hash_id": 8670837389967260165, "entities": ["genderfair language", "machine translation", "german translation support"], "background": "1. The need to counteract gender discrimination through the use of gender-fair language in machine translation, aligning with societal movements and the United Nations' sustainable development goals. 2. The lack of support for gender-fair German in existing machine translation systems, which necessitates post-editing or manual translation."}
{"hash_id": 5013018778481732986, "entities": ["humanlike iterative critique", "text summaries", "refinement methods"], "background": "1. The need to mirror human-like iterative critique and refinement processes to enhance the quality of text summaries generated by large language models. 2. The potential to apply refinement methods to diverse text generation tasks and improve overall model performance, including the development of more helpful and harmless models."}
{"hash_id": 9036796443725171173, "entities": ["generative approaches", "large language models", "multimodal entity linking", "visual inputs bridging"], "background": "1. Existing generative approaches struggle with accurately linking visual entity information to the intrinsic knowledge of Large Language Models (LLMs), leading to suboptimal performance in multi-modal entity linking tasks. 2. The need to bridge the gap between visual inputs and the internal knowledge of LLMs to improve the precision of entity linking in diverse multi-modal contexts such as social media."}
{"hash_id": 309255277746331788, "entities": ["dynamic word meanings", "nlp applications", "semantic changes"], "background": "1. The dynamic nature of word meanings over time, which is crucial for various NLP applications and requires time-sensitive predictions. 2. The decline in performance of Large Language Models (LLMs) over time due to their training on static snapshots, which can be improved by recognizing and incorporating semantic changes."}
{"hash_id": 4723756970737315193, "entities": ["performance gap", "nonautoregressive translation", "language dependencies"], "background": "1. The need to bridge the performance gap between non-autoregressive translation and autoregressive translation, especially when using more reliable evaluation metrics. 2. The importance of explicitly modeling language dependencies to improve the naturalness and accuracy of translations."}
{"hash_id": 5061591489753276775, "entities": ["coldstart scenario", "zeroshot anomaly detection", "limited accuracy"], "background": "1. The need for an anomaly detection method that can perform effectively in the cold-start scenario where no labeled data is available initially. 2. The desire to improve upon the limited accuracy of existing zero-shot anomaly detection methods when applied to real-world scenarios with a stream of potentially contaminated observations."}
{"hash_id": 2693301575775406573, "entities": ["large language models capability", "complex relational dynamics", "detective narratives benchmark"], "background": "1. The need to evaluate and improve Large Language Models' (LLMs) capability to understand complex and uncertain relationships in real-life social scenarios, particularly in narrative contexts. 2. The absence of a suitable benchmark dataset that represents the intricacies of relationships found in detective narratives, which are a prime example of complex relational dynamics."}
{"hash_id": 6353242027435008413, "entities": ["massive editing tasks", "incontext knowledge editing", "computational overhead", "inference time efficiency"], "background": "1. The need to extend in-context knowledge editing to handle massive editing tasks, where a large number of facts need to be updated in language models. 2. The desire to reduce computational and memory overhead at inference time caused by the use of lengthy in-context demonstrations."}
{"hash_id": 5261726082518609089, "entities": ["inference latency", "autoregressive decoding", "llms optimization"], "background": "1. The need to mitigate the high inference latency that stems from the token-by-token generation in traditional autoregressive decoding, which hinders the broader application of LLMs. 2. The observation that many tokens can be predicted with less computational overhead and that LLM inference is memory bandwidth-bound, suggesting opportunities for optimization."}
{"hash_id": 7902366578351402433, "entities": ["unitbased hierarchical text classification", "static thresholding", "label imbalance"], "background": "1. The need to improve the performance of unit-based hierarchical text classification models by overcoming the issue of static thresholding, which is computationally intensive and suboptimal. 2. The requirement to address label imbalance, which can lead to undertraining on infrequent labels and overtraining on frequent ones, thus degrading the overall classification performance."}
{"hash_id": 6120833555699637465, "entities": ["longer context handling", "model constraints", "opendomain qa performance"], "background": "1. The need to empower models to handle longer contexts than they currently can due to constraints on model size and computing resources. 2. The challenge of improving performance in open-domain question-answering tasks when the context length exceeds the model's input limit."}
{"hash_id": 4888102176609375601, "entities": ["automatic food risk detection", "resourceefficient llms", "multiclass prediction"], "background": "1. The need to automatically detect and classify food risks in order to estimate their severity and enable faster, more targeted responses to potential health threats. 2. The requirement for resource-efficient use of Large Language Models (LLMs) due to their high resource consumption, particularly in the context of multi-class prediction problems with a high number of classes."}
{"hash_id": 6817886309090103439, "entities": ["computational intensity mitigation", "large language models", "pivot tokens allocation"], "background": "1. The need to mitigate the computational intensity and memory requirements of large language models (LLMs) without significantly compromising their performance. 2. The existence of a previously overlooked type of outliers in LLMs that allocate most attention scores on pivot tokens, which are crucial to the performance of quantized LLMs."}
{"hash_id": 576665003404071858, "entities": ["chinese k education", "llms performance", "educational stages", "chinese culture"], "background": "1. The lack of a dedicated benchmark for evaluating LLMs within the Chinese K-12 education domain, which is crucial for understanding and improving their performance in this context. 2. The need to assess the nuanced strengths and limitations of LLMs across different educational stages and subjects, particularly in relation to Chinese culture and educational content."}
{"hash_id": 5663363107339157538, "entities": ["chart understanding", "visionlanguage models", "chartrelated tasks"], "background": "1. The need for improved generalization in chart understanding by vision-language models, which currently struggle with the unique complexities and relationships found in charts. 2. The requirement for a versatile model that can handle various chart-related tasks without the need for task-specific fine-tuning."}
{"hash_id": 7608663669721675265, "entities": ["knowledgeintensive multihop qa", "small language models", "question decomposition"], "background": "1. To overcome the limited capacity of small language models (SLMs) to memorize the necessary knowledge and perform complex multi-step reasoning required for knowledge-intensive multi-hop question answering. 2. To address the difficulty of distilling integrated subtasks such as question decomposition, knowledge association, and knowledge reasoning into a single SLM."}
{"hash_id": 279842070614494218, "entities": ["alignment approaches", "human supervision signals", "guidance mechanism", "language models"], "background": "1. The limitations of current alignment approaches that struggle with the inconsistency and sparsity of human supervision signals. 2. The need for a more precise and consistent guidance mechanism for language models in complex and open text generation tasks."}
{"hash_id": 9040842915928976609, "entities": ["knnmt efficiency", "interpolation coefficient estimation", "timestep demands handling"], "background": "1. The need to reduce the substantial time overhead caused by kNN retrieval at each timestep in kNN-MT. 2. The failure of existing methods, such as kNN-MT with adaptive retrieval (kNN-MT-AR), to accurately estimate the interpolation coefficient \u03bb and effectively handle the varying demands for kNN retrieval at different timesteps."}
{"hash_id": 799747275053190606, "entities": ["transformerbased models", "computational resource requirements", "scaled dotproduct attention", "parameter efficiency"], "background": "1. The increasing size of Transformer-based models has led to unsustainable computational resource requirements and energy consumption, creating a need for more efficient training methods. 2. The traditional scaled dot-product attention mechanism in Transformers has room for improvement in terms of parameter efficiency and computational complexity."}
{"hash_id": 4964864455904519163, "entities": ["limited training data", "convqa systems", "dialogue systems"], "background": "1. The need to overcome the challenge of limited and expensive training data for ConvQA systems, which hinders their development and real-world application. 2. The opportunity to leverage the vast amount of high-quality, unlabeled documents available in various domains to create more effective dialogue systems."}
{"hash_id": 8028443151862982592, "entities": ["language models", "concept acquisition", "annotation guidelines"], "background": "1. To assess the capability of large language models to acquire new knowledge or concept definitions through prompting, which is essential for their application in knowledge-intensive tasks. 2. To identify the limitations and gaps in concept understanding between open-source and proprietary language models, particularly in adhering to annotation guidelines."}
{"hash_id": 3424409663996986192, "entities": ["attentionbased encoderdecoder", "japanese imes", "traintest mismatch", "decoding approach"], "background": "1. The potential of attention-based encoder-decoder neural networks like Transformer has not been fully realized in Japanese IMEs due to high computational cost and train-test mismatch issues, which lead to suboptimal performance in simultaneous settings. 2. The unique characteristics of the Japanese language, such as a large number of homophones and the monotonic nature of kana-kanji conversion, necessitate a decoding approach that can handle context effectively without compromising latency."}
{"hash_id": 1262169197220018515, "entities": ["emotional commonsense reasoning", "affective aspects", "emotional knowledge representation", "humancomputer interaction"], "background": "1. The need to enhance AI systems with emotional commonsense reasoning abilities to better understand and reason about affective aspects in human interactions. 2. The absence of comprehensive emotional knowledge representation in existing commonsense knowledge graphs, which limits AI systems' capabilities in human-computer interaction and natural language processing tasks."}
{"hash_id": 9158346868951905294, "entities": ["neural machine translation", "model generalization", "training data diversification", "semantic consistency"], "background": "1. The need to improve the generalization of neural machine translation models, which are vulnerable to small perturbations and lack robustness. 2. The requirement to diversify training data to enhance translation quality without compromising semantic consistency between original and augmented data."}
{"hash_id": 1913292631288133396, "entities": ["incontext learning", "problemsolving approach", "surfacelevel semantic similarities"], "background": "1. The need to improve the performance of large language models through in-context learning by selecting more appropriate demonstrations that share a common problem-solving approach with the target question. 2. The desire to overcome the limitations of existing methods that rely on surface-level semantic similarities, which do not always identify the most fitting demonstrations."}
{"hash_id": 5635679979701822078, "entities": ["nonuniform sampling", "nonstable state", "parallel convolution computation"], "background": "1. To overcome the limitations of existing state space models (SSMs) in handling non-uniform sampling, which leads to the Non-Stable State (NSS) problem and hampers their efficiency in long sequence modeling. 2. To improve the computational efficiency of SSMs by enabling parallel convolution computation, which is restricted by the recursive structures of advanced SSMs like S5 and S6 (Mamba)."}
{"hash_id": 1995908636182876942, "entities": ["improving rationale generation", "enhancing trustworthiness", "knowledgeintensive tasks"], "background": "1. The need to improve the generation of rationales for knowledge-intensive tasks without the expensive and difficult process of collecting high-quality human-authored rationales. 2. The requirement to enhance the trustworthiness and reliability of rationales generated by large language models, especially when they are used to explain incorrect model predictions."}
{"hash_id": 771905412740047114, "entities": ["language model adaptability", "error analysis", "llm performance enhancement"], "background": "1. The need to assess the genuine adaptability and robustness of large language models beyond their performance on existing tasks, to avoid test set contamination and understand their true capabilities. 2. The requirement for detailed error analysis to identify specific weaknesses in LLMs and develop targeted strategies for performance enhancement."}
{"hash_id": 5661656817026830492, "entities": ["event argument extraction", "positional bias", "large language models"], "background": "1. The need to cost-effectively extract event arguments from large documents, which is crucial for deeper understanding in various downstream tasks. 2. The aim to overcome the positional bias issue inherent in Large Language Models, which hinders accurate argument boundary identification."}
{"hash_id": 4068903531500927515, "entities": ["democratize llms", "lowresource languages", "multilingual tasks alignment"], "background": "1. The need to democratize LLMs to support a wider range of languages, especially low-resource ones, to align with the diversity of languages spoken globally. 2. The requirement to improve the alignment of LLMs with human preferences for multilingual tasks to enhance their effectiveness in real-world applications."}
{"hash_id": 6528644908240030646, "entities": ["improve latency and throughput", "large language models", "generative ai applications", "enhance gpu utilization", "reduce memory io costs"], "background": "1. The need to improve latency and throughput in hosting large language models, particularly when generating multiple sequences, to better serve real-world generative AI applications. 2. The goal to enhance GPU utilization and reduce memory I/O costs during auto-regressive decoding processes to make more efficient use of available compute resources."}
{"hash_id": 5707016207068732373, "entities": ["input order", "reading comprehension", "input emphasis methods"], "background": "1. The first motivation is to challenge established practices in natural language processing that have not been properly evaluated, particularly in the context of reading comprehension where the order of inputs can significantly affect model performance. 2. The second motivation is to explore and standardize the use of input emphasis methods to improve the ability of language models to follow instructions and answer questions, especially when they lack specific parametric knowledge."}
{"hash_id": 793471756881626117, "entities": ["language models", "reacquire pruned concepts", "model editing techniques"], "background": "1. The need to understand how large language models can reacquire pruned concepts, which is crucial for enhancing model safety and preventing the reemergence of undesirable concepts. 2. The importance of developing more robust model editing techniques that can permanently remove unwanted concepts without compromising model performance or integrity."}
{"hash_id": 6251240151830443880, "entities": ["promptguided large language models", "task embedding methods", "multimodel scenarios"], "background": "1. The emergence of prompt-guided Large Language Models (LLMs) operating in a gradient-free manner poses challenges for existing task embedding methods, which are not adaptable across diverse models. 2. The current paradigm's reliance on the parameters of a single model restricts the comparison and adaptability of task embeddings, hindering their applicability to multi-model scenarios."}
{"hash_id": 4253269047958349855, "entities": ["virtual assistants", "data annotation", "taskoriented dialogs", "extensive datasets", "practical limitations"], "background": "1. The need to enhance the naturalness and adaptability of virtual assistants across diverse usage scenarios while facing the challenge of slow and costly data annotation for Task-Oriented Dialogs. 2. The requirement to bridge the gap between the demand for extensive datasets and the practical limitations of time and financial resources for data collection."}
{"hash_id": 7221337971195293514, "entities": ["detect machinegenerated text", "document authenticity", "limited contextual information"], "background": "1. The need to detect and localize machine-generated text within documents to prevent misinformation and understand document authenticity. 2. The challenge of identifying short spans of text as machine generated due to limited contextual information."}
{"hash_id": 3361329096773867205, "entities": ["oie benchmark", "noisy benchmark", "benchie benchmark", "methodology limitations"], "background": "1. The need for a more reliable and less noisy OIE benchmark to accurately measure the performance of OIE extractors and draw insightful conclusions. 2. The requirement to correct frequent errors, inconsistencies, and methodology limitations in the existing BenchIE benchmark to ensure fairer rankings of evaluated systems."}
{"hash_id": 1108717519342826082, "entities": ["performance gaps", "reasoning tasks", "translatetraining", "language models"], "background": "1. Large language models show significant performance gaps in reasoning tasks between English and other languages due to the predominant use of English in their training data. 2. The traditional approach of translate-training is expensive and often results in poor-quality translations, especially for complex mathematical reasoning tasks."}
{"hash_id": 8514241197591802175, "entities": ["factconflicting hallucinations", "large language models", "knowledgeintensive tasks"], "background": "1. To mitigate the issue of fact-conflicting hallucinations that occur frequently in large language models during knowledge-intensive tasks. 2. To provide a method that can be applied to a wide range of LLMs, including smaller and open-source models, without requiring complex instructions or access to model weights."}
{"hash_id": 175437774321016189, "entities": ["multiple labels interdependence", "discourse relation recognition", "multiple sense relations"], "background": "1. The need to account for the interdependence of multiple labels in real-world discourse relation contexts, as current approaches treat multiple-labeled instances as separate examples, leading to potential inaccuracies. 2. The desire to improve the performance of discourse relation recognition, particularly for cases where multiple sense relations hold simultaneously, which is not effectively addressed by existing methods."}
{"hash_id": 8154310953025059147, "entities": ["nonexpert programmers", "discriminative benchmark", "code llms performance", "beginning programmers"], "background": "1. The need to better align Code LLMs with the language and descriptions used by non-expert programmers, as current benchmarks are primarily based on expert-written prompts. 2. The requirement for a more discriminative benchmark that can accurately assess the performance of Code LLMs when interacting with beginning programmers, who may have a different approach to describing code problems."}
{"hash_id": 5074337507465988929, "entities": ["lexical substitutes", "language proficiency", "existing benchmarks"], "background": "1. The need to provide English language learners with lexical substitutes that not only fit the context but also improve their language proficiency. 2. The lack of existing benchmarks and models that focus on generating substitutes at or above the proficiency level of the target word."}
{"hash_id": 130177938625464788, "entities": ["tradeoff between quality and diversity", "reranking algorithms", "text generation systems"], "background": "1. The need to improve the trade-off between output quality and diversity in text generation systems, as current methods based on beam search or random sampling have limitations in achieving both. 2. The effectiveness of reranking algorithms is enhanced when candidate outputs are diverse, which is crucial for various applications like image captioning, question generation, and chatbots."}
{"hash_id": 6137483293225657202, "entities": ["inference time reduction", "mbr decoding", "hyperparameter tuning elimination"], "background": "1. The need to reduce the inference time of Minimum Bayes-Risk (MBR) decoding, which is crucial in applications where response time is critical. 2. The desire to eliminate the need for hyperparameter tuning, which is required by existing methods like Confidence-based pruning (CBP) and can be time-consuming and resource-intensive."}
{"hash_id": 1853964365235689670, "entities": ["translation difficulty adjustment", "nmt training bias mitigation", "childrens language level"], "background": "1. The lack of a mechanism in current NMT to adjust the difficulty level of translations to match the language level of users, particularly children, who may struggle with complex words. 2. The need to mitigate the bias in training data for NMT that leads to translations of simple source sentences containing complex words inappropriate for children."}
{"hash_id": 1393005302590151923, "entities": ["improve language models", "complex logical problems", "unidirectional chaining methods", "selectioninference", "lambada ranking strategy"], "background": "1. The need to improve the accuracy and efficiency of large language models in solving complex logical problems, as current unidirectional chaining methods are inadequate. 2. The absence of explicit guidance in unidirectional chaining methods like Selection-Inference and the suboptimal ranking strategy in methods like LAM-BADA, which hinder overall reasoning performance."}
{"hash_id": 1142591969871335741, "entities": ["large language models", "scientific summarization", "communicative intentions"], "background": "1. The need to explore the controllability of large language models for scientific summarization without the need for expensive fine-tuning, given their demonstrated success in zero-shot settings. 2. The requirement to address the variability in communicative intentions and quality criteria in scientific summarization, which is not adequately captured by existing evaluation protocols."}
{"hash_id": 2637248922307121056, "entities": ["knowledge graph completion", "textbased methods", "neighboring triples context"], "background": "1. Existing text-based knowledge graph completion methods ignore the important knowledge context, which contains valuable information for inferring missing triples. 2. Pre-trained language models have a limited input capacity, which poses a challenge when incorporating a large number of neighboring triples for context."}
{"hash_id": 1913515099929476723, "entities": ["continuously evolving ecommerce", "catastrophic forgetting", "domain contamination"], "background": "1. The need to handle the continuously evolving and increasing number of e-commerce product categories and attributes without retraining the model from scratch, ensuring scalability and efficiency. 2. The requirement to prevent catastrophic forgetting and domain contamination in order to maintain the model's performance across all domains throughout its lifetime."}
{"hash_id": 3982750152298767074, "entities": ["novel category gcd performance", "large language models", "data privacy concerns"], "background": "1. The need to improve the performance of GCD on novel categories where current methods lack supervision and struggle to reveal the semantic meanings of discovered clusters. 2. The desire to leverage the extraordinary capabilities of Large Language Models (LLMs) while overcoming their limitations such as data privacy concerns, high inference latency, and costs associated with API usage."}
{"hash_id": 830688600614625909, "entities": ["domainspecific text embeddings", "semantic similarity", "lexical diversity", "unsupervised settings"], "background": "1. The need for domain-specific text embeddings that can capture the unique characteristics of specialized fields, which general models trained on large corpora fail to do effectively. 2. The challenge of training models to maintain a balance between semantic similarity and lexical diversity when dealing with limited data in unsupervised settings."}
{"hash_id": 5721154410025841334, "entities": ["large language models", "multimodal tasks", "comprehensive evaluation system"], "background": "1. The need to assess large language models' capabilities in handling complex, multi-turn, multi-modal tasks within a tool like PowerPoint, which has not been thoroughly investigated. 2. The requirement for a more comprehensive evaluation system that accounts for multiple correct solutions and the final outcome, rather than just comparing generated API sequences."}
{"hash_id": 2751528480908375202, "entities": ["large language models", "confidence calibration", "prompting strategies"], "background": "1. The need for large language models to provide trustworthy outputs by having well-calibrated confidence levels that match their actual performance. 2. The lack of exploration into how different prompting strategies affect the confidence calibration of LLMs, which is crucial for ensuring model reliability and guiding practical use cases."}
{"hash_id": 1432449706742307251, "entities": ["memory consumption", "computation consumption", "large language models", "ultralowbit quantization", "model accuracy"], "background": "1. The need to reduce the memory and computation consumption of large language models (LLMs) to make them more practical for deployment. 2. The challenge of maintaining model accuracy while using ultra-low-bit quantization techniques, which typically result in severe accuracy drops."}
{"hash_id": 6647257423469837855, "entities": ["temporal perception abilities", "video llms", "diverse task formats"], "background": "1. The need to differentiate and evaluate the temporal perception abilities of Video LLMs in various aspects such as speed and direction, which are currently not well-reflected in existing benchmarks. 2. The requirement for a more diverse range of task formats to assess Video LLMs' temporal perception, as the current benchmarks are limited in format and do not test generalization across different types of tasks."}
{"hash_id": 7388191385437648978, "entities": ["annotation issues", "traditional narrative", "theorypractice gap", "annotation pipeline"], "background": "1. To challenge the traditional narrative that frames annotation issues as solely the responsibility of annotators, thus limiting the scope of improving data quality. 2. To address the gap between theory and practice in annotation, where known issues are not effectively linked to their root causes in the annotation pipeline."}
{"hash_id": 2128101384450022729, "entities": ["lowresource languages", "limited training data", "efficient resource utilization"], "background": "1. The need to support numerous low-resource languages, especially those with extremely limited training data, which current large language models struggle to handle. 2. The potential to enable more efficient utilization of limited resources and to contribute to the preservation and education of underrepresented languages."}
{"hash_id": 6164012234971420342, "entities": ["machine translation quality assessment", "segment level performance", "explainable evaluation", "translation errors"], "background": "1. The need to enhance the performance of large language models in machine translation quality assessment at the segment level, where current methods perform poorly. 2. The requirement for a more explainable and reliable evaluation process that mirrors human-like judgment, particularly in identifying major and minor translation errors."}
{"hash_id": 5791588008830123368, "entities": ["chinese language proficiency", "comprehensive benchmark", "humanlevel evaluation"], "background": "1. The need for a comprehensive benchmark that reflects the true capabilities of LVLMs, particularly in the context of Chinese language proficiency. 2. The desire to provide a more human-level evaluation that includes diverse tasks and image types, going beyond the limited scope of existing datasets."}
{"hash_id": 5355684703424890354, "entities": ["nonexperts prompt generation", "texttoimage synthesis", "prompt refinement iterative trial"], "background": "1. The difficulty for non-experts to generate accurate prompts that lead to desired images in Text-to-Image Synthesis (TIS) models. 2. The need to reduce the time and computing resources wasted on iterative trial and error for prompt refinement in TIS models."}
{"hash_id": 281015526523952566, "entities": ["limitation of pcw method", "alternative to pcw", "transformer architecture"], "background": "1. To overcome the limitations of the Parallel Context Windows (PCW) method, which does not guarantee sufficient improvement in handling lengthy documents and may lead to deteriorated reasoning abilities in language models. 2. To provide a simpler and more effective alternative that does not require modifying the transformer architecture and can achieve comparable or better performance than PCW without added computational complexity."}
{"hash_id": 2263316540300315935, "entities": ["improve llm reasoning", "evaluate mathematical word problems", "bridge accuracy gap", "true understanding concepts"], "background": "1. The need to improve the ability of large language models to accurately evaluate the reasoning process behind mathematical word problems, rather than just providing the correct answer. 2. The desire to bridge the gap between the high accuracy of LLMs on direct math problem-solving and their poor performance on evaluating the reasoning process, indicating a lack of true understanding of the underlying mathematical concepts."}
{"hash_id": 8293220798524561737, "entities": ["lora blocks efficiency", "weight updating", "computational overhead reduction"], "background": "1. The need to improve the efficiency of weight updating in LoRA blocks during fine-tuning of large language models to achieve better performance in fewer training steps. 2. The desire to reduce the computational overhead and parameter expense associated with fine-tuning large models, while maintaining or enhancing their performance."}
{"hash_id": 1035652927153938803, "entities": ["social intelligence", "language agents", "complex social interactions", "realistic social tasks", "dynamic social environments"], "background": "1. The lack of standardized, objective metrics to evaluate the social intelligence of language agents in complex social interactions. 2. The need to simulate realistic social tasks to measure an agent's ability to navigate and achieve goals in dynamic social environments."}
{"hash_id": 4685952950021372117, "entities": ["conversational recommender systems", "semantic gap", "historical behaviors"], "background": "1. The need for a large-scale, high-quality dataset that can train conversational recommender systems (CRS) more effectively. 2. The requirement to bridge the semantic gap between dialogue content and users' historical behaviors to improve the relevance and personalization of recommendations."}
{"hash_id": 4158598894766885491, "entities": ["llm bias evaluation", "social inequities", "subtler biases"], "background": "1. The increasing use of LLMs in consequential real-world decisions risks amplifying social inequities and biases if the models are not thoroughly evaluated for bias along various dimensions. 2. Existing studies have primarily focused on the most salient biases like gender and ethnicity, neglecting the impact of subtler biases such as ageism and beauty that also significantly influence human perceptions and decisions."}
{"hash_id": 3083715123824422286, "entities": ["instructiontuned models", "event reasoning", "explicit event modeling"], "background": "1. The need to improve the event reasoning abilities of smaller instruction-tuned models, which currently do not perform as well as large language models on event reasoning tasks. 2. The absence of explicit event modeling and inter-relations in the instruction-tuning data used for training smaller language models, leading to a discrepancy between model interpretations and human understanding of events."}
{"hash_id": 3786382351506230391, "entities": ["traditional sentence compression", "length constraints", "large language models", "zeroshot tasksolving"], "background": "1. Traditional sentence compression models do not effectively consider length constraints due to their limited model capabilities, which necessitates modifying the models to accommodate such constraints. 2. Large Language Models (LLMs) have zero-shot task-solving abilities that could potentially address length control in sentence compression without the need for model modifications, but they have not been fully exploited for this purpose."}
{"hash_id": 836462826464779499, "entities": ["news thumbnail representativeness", "pretrained models", "named entities"], "background": "1. The critical challenge of assessing news thumbnail representativeness, which is important because visual content can have a more persuasive impact than text and misrepresentation can lead to more severe consequences. 2. The existing pretrained vision and language models struggle with the task due to their limited capability to match news actors' visual and textual appearances, particularly because these models were not specifically trained to handle the named entities and proper nouns frequently found in news texts."}
{"hash_id": 3634609013478905988, "entities": ["qabased event extraction", "question quality", "templatebased question generation", "dynamic contextaware approach"], "background": "1. The shift to QA-based event extraction has highlighted the critical role of question quality in the accuracy of the extraction process, yet existing methods struggle to generate effective questions. 2. The limitations of template-based question generation, which often lead to rigid and context-insensitive questions, necessitate a more dynamic and context-aware approach."}
{"hash_id": 5274318837496195386, "entities": ["cost management", "external tools", "budget constraints", "tool usage planning"], "background": "1. The need to manage costs effectively when using external tools in conjunction with large language models, as real-world users often have budget constraints that can limit the utility of these tools if not properly managed. 2. The lack of existing methods that consider comprehensive planning for tool usage under budget constraints, which is crucial for optimizing performance without exceeding financial limits."}
{"hash_id": 9108065291714757151, "entities": ["instructionfollowing capabilities", "multimodal inputs", "complex realworld tasks", "interleaving imagetext inputs", "singleturn datasets"], "background": "1. The need to enhance the instruction-following capabilities of large language models in open-world scenarios with multimodal inputs without relying on extensive human-annotated datasets. 2. The desire to improve the performance of LLMs on complex, real-world tasks that require interleaving of image and text inputs and outputs, which is not adequately captured by existing single-turn or simplified datasets."}
{"hash_id": 865179205693069469, "entities": ["evaluation of critiques", "large language models", "model alignment"], "background": "1. The lack of a systematic method to evaluate the quality of critiques, which are essential for training, evaluating, and refining Large Language Models (LLMs). 2. The need to improve the alignment and refinement of LLMs by providing superior critiques that can lead to better model outputs."}
{"hash_id": 313032875201879657, "entities": [], "background": "1. The need to improve the comprehensive cognition of GUI agents, including exhaustive perception and reliable action response, to better handle complex tasks in realistic scenarios. 2. The requirement to address the limitations of existing methods, such as the dependence on strong (M)LLMs, the alignment of GUI commands with natural language, and the need for sophisticated prompt design."}
{"hash_id": 2994506488807325586, "entities": ["improve structural correctness", "explainable question answering", "reduce search space"], "background": "1. The need to improve the structural correctness and reliability of stepwise proof generation in explainable question answering to ensure that reasoning paths are valid and can be understood by humans. 2. The desire to reduce the large search space and error accumulation that occur in existing stepwise generation methods, which can lead to invalid steps and incorrect reasoning chains."}
{"hash_id": 2072162890709911924, "entities": ["personalized language models", "humanlike interactions", "privacy concerns", "ondevice deployment"], "background": "1. The need to empower large language models with personalized traits to enhance human-like interactions without compromising privacy concerns associated with uploading personal data. 2. The requirement to overcome the limitations of sub-optimal performance from manual prompting and the computational challenges of using large-scale models for on-device deployment."}
{"hash_id": 7879715078147963469, "entities": ["enhance llm performance", "complex tool operation", "entity retrieval optimization", "natural language representations"], "background": "1. The need to enhance the performance of LLMs on complex tasks that require operating multiple tools or entities, such as APIs, where traditional fine-tuning is computationally expensive. 2. The requirement to improve the precision of entity retrieval by optimizing the natural language representations of these entities, which plain descriptions often fail to capture effectively."}
{"hash_id": 593520913336808806, "entities": ["direct speechtospeech translation", "decoding speed", "textless sst approach", "unwritten languages"], "background": "1. The need to improve decoding speed in direct speech-to-speech translation (S2ST) without compromising translation quality, particularly for long speech sequences. 2. The desire to develop a textless S2ST approach that does not require intermediate text supervision and can be applied to unwritten languages."}
{"hash_id": 5515490294606182414, "entities": ["multiimplication problem", "clinical terminology normalization", "automated decomposition method", "semantic ambiguity", "normalization process"], "background": "1. The need to address the multi-implication problem in clinical terminology normalization for Chinese disease diagnoses, which leads to semantic ambiguity and hinders the effectiveness of traditional methods. 2. The requirement for an automated decomposition method that can handle complex mentions with multiple meanings, improving the accuracy of the normalization process."}
{"hash_id": 594015953926607962, "entities": ["large language models", "information extraction", "spurious associations", "relation extraction"], "background": "1. The need to improve the performance of large language models in information extraction tasks, especially in limited-data scenarios where traditional models struggle. 2. The opportunity to exploit the unexpected phenomenon of spurious associations in LLMs to enhance their capabilities in relation extraction, named entity recognition, and event detection."}
{"hash_id": 6654123030997626226, "entities": ["crosslingual chainofthought reasoning", "language selection process", "dynamic weight allocation", "crosslingual reasoning performance"], "background": "1. The need to improve the generalizability of cross-lingual chain-of-thought reasoning by automating the language selection process, reducing reliance on manual specification. 2. The requirement to enhance the performance of cross-lingual reasoning by dynamically allocating appropriate weights to different language reasoning paths."}
{"hash_id": 1308629218743017296, "entities": ["zeroshot translation", "language tag strategies", "offtarget issue", "encoder languagespecific states", "translation accuracy"], "background": "1. The current language tag strategies fail to indicate the desired target language effectively in zero-shot translation, leading to the off-target issue. 2. The placement of the language tag and the conversion of language-specific states in the encoder are critical factors affecting translation accuracy, which are not adequately addressed by existing strategies."}
{"hash_id": 2966474857647399505, "entities": ["advanced quantitative reasoning", "realworld data", "comprehensive benchmarks", "statistical causal reasoning"], "background": "1. The need to assess and improve the ability of Large Language Models in performing advanced quantitative reasoning with real-world data, which is critical for data analysis and decision-making. 2. The absence of comprehensive benchmarks that specifically target statistical and causal reasoning with data, thereby limiting the understanding of models' capabilities and the development of more effective reasoning approaches."}
{"hash_id": 8330390089245466911, "entities": ["llm safety alignment", "malicious exploitation", "finetuning attacks", "ethical safeguards"], "background": "1. The need to expose and mitigate the vulnerabilities in safety alignment of LLMs, which can be exploited by malicious actors, leading to increased harm. 2. The requirement for robust methods to counteract fine-tuning attacks that can reverse the ethical safeguards of open-access LLMs, particularly as these models become more accessible and powerful."}
{"hash_id": 5165832238013807634, "entities": ["vram consumption reduction", "finetuning language models", "dialogue generation enhancement", "parameterefficient knowledge integration"], "background": "1. The need to reduce VRAM consumption during the fine-tuning of large pre-trained language models without compromising their expressive capacity. 2. The desire to enhance dialogue generation models with external knowledge in a parameter-efficient manner to improve performance in knowledge-related conversations."}
{"hash_id": 2210521085359323972, "entities": ["claim verification", "evidence retrieval", "knnbased semantic similarity"], "background": "1. The current focus on claim verification without sufficient attention to the quality of evidence retrieval can lead to suboptimal performance due to task-irrelevant evidence. 2. The limitations of existing retrieval strategies, such as KNN-based semantic similarity, which may not provide the most relevant evidence for claim verification and result in a separated training paradigm."}
{"hash_id": 1803701864323893586, "entities": ["knowledge editing methods", "reasoning questions", "catastrophic forgetting"], "background": "1. Existing knowledge editing methods struggle to utilize edited knowledge for reasoning questions and tend to retain outdated responses, which are detrimental to the accuracy of the model. 2. Retraining or fine-tuning large language models can be computationally expensive and may lead to catastrophic forgetting."}
{"hash_id": 7544056585928354920, "entities": ["social bias detection", "nlp methods", "dialectal variations", "performance disparities"], "background": "1. The need to mitigate unfairness in social bias detection caused by the lack of sensitivity of NLP methods to dialectal variations in language. 2. The recognition that existing models often fail to account for dialect patterns, leading to performance disparities and biases against dialect speakers."}
{"hash_id": 3586141040506988588, "entities": ["privacypreserving text processing", "encoderonly models", "differentially private text rewriting"], "background": "1. The need to preserve privacy in text processing while maintaining the utility of the text, especially as privacy concerns in NLP models rise. 2. The lack of consideration for encoder-only models like BERT in existing differentially private text rewriting methods, and the potential advantages they could offer over autoregressive generative models."}
{"hash_id": 4037780260485958531, "entities": ["simpler video qa approach", "large language models", "textbased qa problem"], "background": "1. The need for a simpler and more efficient approach to video question answering that does not rely on complex architectures or computationally expensive pipelines. 2. The desire to leverage the reasoning capabilities of large language models (LLMs) in the context of video QA by transforming the task into a text-based QA problem."}
{"hash_id": 6277350268708217247, "entities": ["fact verification systems", "explainability", "multihop reasoning", "highquality dataset"], "background": "1. The need for improved explainability in fact verification systems to enhance credibility with human users and potentially improve model performance. 2. The absence of a high-quality dataset that captures the complexity of multi-hop reasoning required for real-world fact verification tasks."}
{"hash_id": 7145870086381452771, "entities": ["performance gap", "agent tuning", "llm challenges"], "background": "1. To bridge the performance gap between open-sourced LLMs and API-based models when acting as agents, which is crucial for real-world application deployment. 2. To address the specific challenges in agent tuning for LLMs, such as the entanglement of format following and reasoning in training data, varied learning speeds of different capabilities, and the issue of hallucinations in model output."}
{"hash_id": 1915585017609320109, "entities": ["large language models", "misinformation", "factchecking method", "models output information"], "background": "1. The increasing reliance on and trust in large language models (LLMs) as primary information sources, which can lead to dangerous spread of misinformation due to the models' tendency to produce factually incorrect \"hallucinations.\" 2. The need for a fact-checking method that does not rely on external knowledge sources, which can be incomplete and computationally expensive, and instead uses information from the model's output itself."}
{"hash_id": 115437686896887306, "entities": ["compute budget reduction", "neural machine translation", "data point contribution"], "background": "1. The need to reduce the compute budget and improve efficiency in neural machine translation without significantly compromising model performance. 2. The recognition that not all data points contribute equally to model training and generalization, and that leveraging early training dynamics can help identify the most valuable data points."}
{"hash_id": 4090072007019120195, "entities": ["zeroshot learning models", "textual descriptions", "decision boundaries", "unfamiliar domains"], "background": "1. The sensitivity of zero-shot learning models to the quality of provided textual descriptions, which can lead to changes in decision boundaries and decreased accuracy. 2. The challenge of choosing proper descriptions for zero-shot models, especially in unfamiliar domains, without incurring the high costs associated with obtaining labeled data."}
{"hash_id": 155404784943157632, "entities": ["temporal spatial inefficiencies", "knnmt", "neighborhood knowledge transfer", "domainspecific lowfrequency words", "selective distillation"], "background": "1. To overcome the temporal and spatial inefficiencies of kNN-MT by transferring neighborhood knowledge from the decoding phase to the training phase, but without arbitrarily restricting the learning of student models. 2. To improve the translation of domain-specific low-frequency words by selectively distilling domain-relevant knowledge from a teacher model."}
{"hash_id": 4605154893885768887, "entities": ["event argument extraction", "processing multiple events", "leveraging correlations", "contextual understanding"], "background": "1. The need to improve efficiency in event argument extraction by processing multiple events within a document simultaneously instead of in isolation. 2. The desire to leverage the correlations among different events in a document to enhance the contextual understanding and accuracy of argument extraction."}
{"hash_id": 1382353008292668682, "entities": ["improve generalization capabilities", "lowresource scenarios", "data augmentation strategies"], "background": "1. The need to improve generalization capabilities of speech-to-text systems in low-resource scenarios. 2. The potential to reduce overfitting by using data augmentation strategies that have been successful in other domains but under-explored in speech-to-text tasks."}
{"hash_id": 5839173397459721107, "entities": ["specialized chemistry knowledge", "large language models", "noisy pseudolabeled data", "downstream tasks drug discovery"], "background": "1. The lack of specialized chemistry knowledge in Large Language Models (LLMs) leads to noisy pseudo-labeled data, compromising the robustness of fine-tuned models. 2. The field requires high-quality datasets to improve the performance of models in downstream tasks related to drug discovery."}
{"hash_id": 4990626473162540509, "entities": ["quote tweet popularity", "social media engagement", "language models", "user engagement factors"], "background": "1. The need to improve the popularity of quote tweets, which can enhance public engagement and discourse on social media platforms. 2. The recognition that existing language models do not effectively learn the factors that contribute to the popularity of text, such as the potential for user engagement measured by likes, replies, and retweets."}
{"hash_id": 2847312012371633225, "entities": ["sample efficiency", "near duplicate subwords", "language model training"], "background": "1. Improving the sample efficiency of language model training by addressing the issue of near duplicate subwords. 2. Understanding the impact of naturally occurring near duplicates on language model performance and finding a balance between generalization and accuracy."}
{"hash_id": 5674640026251381797, "entities": ["semantic underspecification", "pretrained language models", "naturalistic data evaluation"], "background": "1. To investigate and expose the limitations of current pre-trained language models in handling semantic underspecification, which is a common phenomenon in everyday language use. 2. To emphasize the importance of naturalistic data and communicative scenarios when evaluating the language capabilities of pre-trained LMs."}
{"hash_id": 8613912161885798177, "entities": ["visual hallucinations", "multimodal llms", "benchmark evaluation"], "background": "1. Existing studies on visual hallucinations in multi-modal LLMs are limited by the biased understanding due to the limited diversity of VH instances identified in existing image datasets. 2. There is a need for a more comprehensive benchmark to evaluate and improve the performance of multi-modal LLMs in the presence of visual hallucinations."}
{"hash_id": 3700414235299125288, "entities": ["summarization datasets", "language models", "salient information distribution"], "background": "1. The need for summarization datasets that reflect the capabilities of current language models to handle longer documents, as existing datasets do not adequately challenge these models. 2. The requirement for a dataset with a uniform distribution of salient information to prevent models from exploiting biases and to truly test their ability to extract key information from throughout the document."}
{"hash_id": 3409175347708214697, "entities": ["dependency reduction", "ner performance", "lowresource settings"], "background": "1. The need to reduce the dependency on large amounts of manually annotated data for NER, which is costly and time-consuming. 2. The desire to improve the performance of NER in low-resource and class-imbalanced settings, where traditional data augmentation methods are limited."}
{"hash_id": 8073162962180499579, "entities": ["compositionality gap", "llms", "autonomous error rectification"], "background": "1. The need to improve the compositional reasoning capabilities of LLMs, which are crucial for decomposing complex tasks and are currently underperforming, as evidenced by the \"compositionality gap\" in question-answering tasks. 2. The desire to enable LLMs to autonomously rectify their compositional reasoning errors and continuously improve over time without relying on expert-crafted prompting strategies."}
{"hash_id": 3973273975514169172, "entities": ["crosslingual discourse parsing", "parallel corpora scarcity", "rst annotation inconsistencies", "language transfer models"], "background": "1. The need to improve cross-lingual discourse parsing by overcoming the scarcity of parallel corpora and inconsistencies in RST annotation across languages. 2. The potential to enhance the effectiveness of RST parsers for language transfer by developing models that can capture the rhetorical structure of text in both monolingual and bilingual settings."}
{"hash_id": 105006031752564528, "entities": ["costeffective dialogue data", "educational chatbots", "scalable educational tools"], "background": "1. The need for a cost-effective method to generate high-quality dialogue data for educational chatbots due to the scarcity of real teacher-student interaction data. 2. The requirement to improve learning outcomes through scalable educational tools like chatbots, which can be achieved by synthesizing realistic and informative dialogues."}
{"hash_id": 2795088192446536748, "entities": ["metric space generality", "semantic relationships", "fewshot text classification", "class prototypes accuracy"], "background": "1. The need to improve the generality of the learned metric space and model generalization by addressing the oversight of ignoring semantic relationships between prototypes and class clusters. 2. The requirement to enhance the accuracy and distinctiveness of class prototypes in few-shot text classification without relying on complex external knowledge bases."}
{"hash_id": 1840201966102976457, "entities": ["incontext learning", "instance selection", "computational inefficiency", "diversity and representativeness", "large language models"], "background": "1. The need to reduce the time required to select instances for annotation in in-context learning due to the computational inefficiency of existing methods. 2. The requirement to balance diversity and representativeness in instance selection to improve the performance of large language models in new tasks."}
{"hash_id": 1432580829588261192, "entities": ["computational overhead reduction", "edge device deployment", "high compression ratio", "structured pruning methods"], "background": "1. The need to reduce the computational and memory overheads of large language models for local deployment on edge devices, enhancing privacy and independence from network conditions. 2. The challenge of achieving a high compression ratio for scaled-up LLMs without significant performance loss, especially with existing structured pruning methods."}
{"hash_id": 3739353085063439838, "entities": ["subword tokenization limitations", "multilingual translation", "utf encoding", "information density"], "background": "1. The need to overcome the limitations of subword tokenization, such as the inability to adapt to new words and the exacerbation of vocabulary imbalance in multilingual translation. 2. The requirement to enhance the information density of byte sequences in UTF-8 encoding, which is crucial for improving translation quality, especially for languages with characters represented by multiple bytes."}
{"hash_id": 4534370541360731147, "entities": ["language models", "incorrect content reduction", "factual relations reasoning"], "background": "1. The need to reduce the generation of incorrect or nonsensical content by language models and to make them more editable and updatable. 2. The desire to leverage language models' ability to reason about factual relations between pieces of text to improve their own text generation coherence and accuracy."}
{"hash_id": 6298797493848855669, "entities": ["paired speechsinging voice data", "sts models", "selfsupervised learning"], "background": "1. The lack of paired speech-singing voice data hinders the development and scalability of STS models. 2. The highly dynamic nature of singing voice and scarcity of annotated data pose challenges for language models, which can be overcome by leveraging self-supervised learning."}
{"hash_id": 395379502406413972, "entities": ["bias mitigation", "language model steerability", "diverse viewpoints"], "background": "1. To uncover and mitigate biases in large language models when generating text that reflects complex, multifaceted personas, which could perpetuate monolithic and insufficiently nuanced views of demographics. 2. To improve the steerability of language models towards a wider and more diverse range of viewpoints, particularly for incongruous personas where one trait makes other traits less likely."}
{"hash_id": 8741850014243726649, "entities": ["multimodal summarization", "multimodal output", "finegrained entity knowledge"], "background": "1. The rapid increase in multimedia data necessitates advancements in Multimodal Summarization with Multimodal Output (MSMO) to produce summaries that integrate both text and relevant images. 2. Traditional MSMO approaches overlook the essential connections between objects and the entities they represent, leading to a lack of fine-grained entity knowledge integration in the summarization process."}
{"hash_id": 1334314939888846345, "entities": ["language model predictions", "highstakes environments", "universal text perturbations", "inputspecific text perturbations"], "background": "1. The need to ensure the stability and accuracy of language model predictions against a wide range of input variations, especially in high-stakes environments such as healthcare and finance, where the impact of incorrect predictions can be significant. 2. The existence of a more substantial threat posed by universal text perturbations (UTPs) compared to input-specific text perturbations (ISTPs), as UTPs can lead to mispredictions across any input and are more challenging to mitigate."}
{"hash_id": 8118844256680343013, "entities": ["machine translation", "documentlevel context", "discourse phenomena", "contextaware translation methods"], "background": "1. The need to improve the quality of machine translation by incorporating document-level context, which is crucial for translating discourse phenomena correctly. 2. The recognition that existing large-scale datasets have been processed in a way that discards document-level metadata, limiting the potential of context-aware translation methods."}
{"hash_id": 5783736924403697077, "entities": ["metaphor interpretation", "computational metaphor processing", "model performance", "nlp downstream tasks"], "background": "1. The lack of large annotated datasets and effective pre-trained language models (PLMs) specifically for metaphor interpretation hinders progress in the field of computational metaphor processing. 2. The need to improve model performance on metaphor identification and interpretation to better support natural language processing (NLP) downstream tasks."}
{"hash_id": 1476792924200529359, "entities": ["counteract deceptive information", "llm moral alignment", "existing llm research gap"], "background": "1. The need to equip LLMs with the ability to identify and counteract deceptive information to prevent malicious manipulations and align with moral values. 2. The gap in existing LLM research, which assumes the processed information is always honest, ignoring the prevalence of deception and misinformation in real-world scenarios."}
{"hash_id": 5275209971286012974, "entities": ["alignment language models", "preference strength", "finetuning improvement"], "background": "1. The need to better align language models with human preferences without the complexity and cost of training a reward model or using reinforcement learning. 2. The recognition that not all preference pairs are equal and that the strength of preference should be taken into account to improve fine-tuning."}
{"hash_id": 1321315260817067640, "entities": ["talking head translation", "cascading methods", "parallel visual corpus data", "textless languages"], "background": "1. The need to improve the efficiency and accuracy of talking head translation by eliminating the delays and errors caused by cascading methods. 2. The requirement for a method that can handle talking head translation without relying on extensive parallel visual corpus data, especially for languages and dialects that are textless or have limited resources."}
{"hash_id": 3958450789843246341, "entities": ["bias dataset construction", "bias mitigation", "demographic group pair", "fair model performance"], "background": "1. Existing studies on bias dataset construction or bias mitigation methods typically focus on only one demographic group pair, which does not reflect the complexity of real-world bias issues. 2. The need to achieve fairer model performance without sacrificing the pretrained model's capabilities when dealing with multiple demographic groups."}
{"hash_id": 5331214622528523474, "entities": ["adversarial attacks", "safety guardrails", "misuse prevention", "safety alignment"], "background": "1. The need to understand and mitigate potential harm caused by large language models, including their susceptibility to adversarial attacks, which can lead to jailbreaking of safety guardrails. 2. The importance of preventing misuse by malicious actors and ensuring the safety alignment of models that can follow speech instructions and generate relevant text responses."}
{"hash_id": 6479859061624646009, "entities": ["hallucinations in summarization", "interpretability issues", "llmbased evaluation metrics"], "background": "1. The need to detect and correct hallucinations in abstractive summarization, which have become more subtle and challenging with the advancement of large language models. 2. The lack of interpretability and the preference bias observed in recently proposed LLM-based evaluation metrics, which provide only a single score and favor summaries from the same underlying model."}
{"hash_id": 8706762604383666179, "entities": ["large multimodal language models", "parameter finetuning", "downstream tasks efficiency"], "background": "1. The existing challenge of fine-tuning all parameters of large multimodal language models (MLLMs) due to their immense size, which is computationally expensive and requires significant resources. 2. The need to improve the performance and stability of MLLMs in downstream tasks while using fewer trainable parameters, to make fine-tuning more efficient and accessible."}
{"hash_id": 9112504934433115234, "entities": ["language models planning abilities", "opendomain scenarios", "implicit reasoning", "knowledge inference"], "background": "1. The need to evaluate language models' planning abilities in more realistic, open-domain scenarios with linguistic complexity and domain diversity, rather than simplified toy environments. 2. The requirement to assess models' capacity for implicit reasoning and knowledge inference from goals without explicit instructions, which is essential for understanding their true planning capabilities."}
{"hash_id": 6504164705565328406, "entities": ["englishcentric models", "lowresource languages", "unified language model"], "background": "1. The significant gap between well-resourced English-centric models and low-resource languages like Turkish in terms of natural language processing capabilities. 2. The need for a unified language model that can perform both natural language understanding and generation tasks for Turkish, as existing models are either limited in scope or do not perform as well."}
{"hash_id": 3624033266411040706, "entities": ["pretrained language models", "fewshot learning", "prompt selection"], "background": "1. The effectiveness of pretrained language models (PLMs) in few-shot learning is highly dependent on the quality of the provided examples, which is challenging to achieve due to the need for unambiguous instruction and appropriate example selection. 2. Existing methods for prompt selection require additional computation and task-specific training, or they rely solely on embedding space properties without considering other important metrics."}
{"hash_id": 3037653432045310043, "entities": ["incontext learning", "example selection framework", "educational theories"], "background": "1. The need to improve the effectiveness of in-context learning by selecting and combining examples in a way that mirrors human learning processes. 2. The desire to leverage educational theories to guide the development of a more structured and intuitive example selection framework for large language models."}
{"hash_id": 5625729313013676386, "entities": ["selfconsistency language models", "interpretability decisionmaking", "reasoning medical diagnoses"], "background": "1. Enhancing the self-consistency and interpretability of large language models in decision-making. 2. Improving the models' ability to perform reasoning tasks, particularly in fields like medicine where process of elimination is crucial for diagnoses."}
{"hash_id": 6146926793157643403, "entities": ["intent detection models", "lowresource scenarios", "new intents identification"], "background": "1. The need for intent detection models that can promptly identify new intents without requiring extensive fine-tuning on specific intent classes. 2. The challenge of low-resource scenarios where gathering manually annotated examples for new intents is difficult or not feasible."}
{"hash_id": 3247257579647226027, "entities": ["training domainspecific models", "enhancing language models", "opendomain performance"], "background": "1. The high cost and resource requirements of training domain-specific large language models from scratch. 2. The need to enhance the domain-specific capabilities of existing large language models without compromising their open-domain performance."}
{"hash_id": 1891280136725290549, "entities": ["multisource noise", "distantlysupervised joint extraction", "costeffective alternative", "large language models"], "background": "1. The need to effectively handle multi-source noise in distantly-supervised joint extraction, which significantly degrades the performance of traditional models. 2. The desire for a more cost-effective and interpretable alternative to large language models that can be sensitive to prompt design and struggle with complex tasks."}
{"hash_id": 4259267675991194274, "entities": ["factual accuracy", "sensitive applications", "dependency reduction", "simplification process"], "background": "1. The need for reliable factual accuracy in LLM outputs, especially in sensitive applications like medical consultation and legal advice. 2. The desire to reduce dependency on external knowledge bases and cross-referencing algorithms, simplifying the factual detection process."}
{"hash_id": 5875319307182351249, "entities": ["prevent harmful behaviors", "model finetuning", "harmful content acquisition"], "background": "1. The need to prevent large language models from learning harmful behaviors during fine-tuning, as they can inadvertently acquire undesired content from the data. 2. The desire to maintain the model's ability to learn new tasks while avoiding the acquisition of harmful or hallucinatory behaviors."}
{"hash_id": 2776960366049751998, "entities": ["debiasing llms", "scalable method", "costeffective", "downstream tasks"], "background": "1. The need to mitigate biases present in LLMs that can propagate to downstream tasks, hindering their widespread deployment. 2. The requirement for a scalable and cost-effective method that does not necessitate training LLMs from scratch for debiasing."}
{"hash_id": 409942245535775303, "entities": ["large language models", "textual variations", "instruction tuning", "robustness"], "background": "1. Current large language models lack robustness to textual variations and generalizability to unseen instructions, leading to inconsistent outputs and potential trustworthiness issues. 2. Existing instruction tuning methods do not explicitly address models' robustness against variations in instructions, focusing instead on aligning desired outputs for specific instruction-input pairs."}
{"hash_id": 1316830401475802493, "entities": ["dynamic document identifiers", "generative retrieval models", "evolving model parameters"], "background": "1. To overcome the limitations of existing pre-training methods that use pre-defined static document identifiers, which may not align with the evolving model parameters and can hinder retrieval performance. 2. To enhance the discriminative and generalization ability of generative retrieval models by dynamically updating document identifiers during pre-training to better match the model's learned knowledge."}
{"hash_id": 1091370296027077414, "entities": ["pseudolabeled data quality", "target domain unlabeled data", "absa models robustness"], "background": "1. The need to improve the quality control of pseudo-labeled data generated from target domain unlabeled data, which currently leads to inaccuracy and error propagation. 2. The requirement to enhance the diversity of label and text patterns in generated data to boost the robustness and generalization ability of ABSA models."}
{"hash_id": 2253110431608207712, "entities": ["improving llms", "complex reasoning tasks", "multisentence premises", "entailment benchmark"], "background": "1. The need to understand and improve the limitations of Large Language Models (LLMs) in different aspects of language inferences, particularly in complex reasoning tasks. 2. The requirement to develop a more comprehensive evaluation benchmark for entailment verification that includes multi-sentence premises, which are crucial for modern NLP problems like detecting inconsistent model-generated rationales."}
{"hash_id": 1264216531957158848, "entities": ["chartrelated tasks", "realworld applicability", "instruction tuning", "chart comprehension"], "background": "1. The need to enhance the real-world applicability of models in chart-related tasks by overcoming the limitations of task-specific fine-tuning on vision and language tasks. 2. The desire to improve the capability of models to understand and reason over a wide array of chart types and tasks by leveraging instruction tuning, which has shown promise in language models but is underexplored in the context of chart comprehension."}
{"hash_id": 4649857405228712286, "entities": [], "background": "1. To enhance zero-shot translation performance in multilingual NMT by addressing the issue of knowledge entanglement, which hinders knowledge transfer across languages. 2. To improve target language generation by incorporating low-level linguistic features to reduce spurious correlations and off-target issues."}
{"hash_id": 7911146153038758789, "entities": ["weightsharing supernets", "nas", "nlp tasks", "performance gap", "training efficiency"], "background": "1. The need to improve the performance of weight-sharing supernets in NAS for NLP tasks, where there is a significant performance gap between the supernet and training from scratch for the same architecture. 2. The requirement to reduce retraining time and enhance training efficiency in NAS by minimizing the observed performance gap in NLP tasks."}
{"hash_id": 6590890967236316096, "entities": ["training costs", "instructiontuning", "smaller language models", "data curation"], "background": "1. The high training costs and inefficiency of using extensive datasets for instruction-tuning language models. 2. The potential to leverage the learning capabilities of smaller language models to curate training data for larger models, reducing costs and potentially improving performance."}
{"hash_id": 4042015753523438370, "entities": ["code pretrained models", "generalization", "large language models"], "background": "1. The current Code Pre-trained Models (CodePTMs) struggle with generalization and tend to learn superficial features instead of understanding the root causes of code vulnerabilities, leading to poor performance on out-of-distribution data. 2. Large Language Models (LLMs) have shown remarkable reasoning and generalization capabilities, but their direct application to code vulnerability detection without specialized training is challenging."}
{"hash_id": 6017267315675174998, "entities": ["annotated data scarcity", "fewshot ser performance", "application scenarios limitations"], "background": "1. The scarcity of annotated data for training SER models in visually-rich documents, which hinders the performance of existing models, especially in few-shot scenarios. 2. The limitations of existing few-shot SER methods in terms of generality and performance, particularly when applied to different application scenarios."}
{"hash_id": 7147864742277609289, "entities": ["high cost computational resources", "deep neural network architectures", "large language models"], "background": "1. The high cost and computational resources required for evaluating the performance of numerous deep neural network architectures during the architecture search process. 2. The potential of Large Language Models (LLMs) to capture a 'general understanding' of DNN architectures, which could be utilized to create more accurate and efficient performance predictors."}
{"hash_id": 4465885468075690226, "entities": ["multimodal information", "dialogue discourse parsing", "chinese opendomain dataset"], "background": "1. The need to capture multi-modal information in dialogue discourse parsing to align with real-world dialogue scenarios and enhance the understanding of dialogue structure. 2. The absence of a multi-modal open-domain dataset for Chinese dialogue discourse parsing that can facilitate research and practical applications in daily life."}
{"hash_id": 1805091163325949414, "entities": ["specialized pretrained language models", "psychological text analysis", "suicidal tendencies intervention"], "background": "1. The need for specialized pre-trained language models that can effectively analyze psychological text in the Chinese language, particularly on social media where individuals express their emotional states. 2. The importance of proactive identification and timely intervention for individuals displaying suicidal tendencies or facing emotional crises on social media platforms."}
{"hash_id": 644286740941077463, "entities": ["reinforcement learning from human feedback", "diversity of preferences", "efficient language model training"], "background": "1. The limitations of existing reinforcement learning from human feedback (RLHF) methods, which often fail to account for the diversity of human preferences and are unstable and resource-heavy when dealing with multiple, conflicting objectives. 2. The need for a more efficient and stable approach to train language models that can cater to a variety of preferences without requiring extensive fine-tuning for each preference."}
{"hash_id": 7663552513236514976, "entities": ["large language models", "privacy concerns", "apibased llms", "prompt recovery methods"], "background": "1. The need to understand how large language models (LLMs) work and address privacy and copyright concerns by recovering the prompts used to generate outputs. 2. The challenge presented by the increasing opacity of API-based LLMs, which provide limited output information, making traditional prompt recovery methods less effective."}
{"hash_id": 2935069902310710449, "entities": ["improve llm clarification strategies", "domain transferability", "diverse domain characteristics"], "background": "1. The need to improve the effectiveness of LLM-based clarification strategies in unseen domains, as current methods struggle with achieving strong domain transferability. 2. The desire to overcome the limitations of one-size-fits-all strategies that do not adapt well to the diverse characteristics of different domains."}
{"hash_id": 4200783183528549561, "entities": ["generative search engines", "adversarial attacks", "response accuracy"], "background": "1. The need to ensure the accuracy and reliability of responses from generative search engines, which could be manipulated to provide incorrect or harmful information. 2. The urgency to assess the robustness of these engines against adversarial attacks, given their potential to be used in sensitive and complex environments."}
{"hash_id": 8253544055039693208, "entities": ["long prompts", "language models", "prompt engineering", "llm evolution"], "background": "1. The immense search space and complexity of designing effective long prompts for large language models, which often require considerable human effort and are sensitive to minor modifications. 2. The need for automatic prompt engineering techniques to adapt to the rapid evolution of LLMs, as prompts crafted for one version may not be effective for newer versions."}
{"hash_id": 2889454283568390272, "entities": ["improve small models efficiency", "learning cot", "inherent information utilization"], "background": "1. The need to improve the efficiency of small models in learning CoT without relying on data augmentation or altering the model architecture. 2. The recognition that existing methods often overlook the importance of efficiently utilizing the inherent information in the available CoT data."}
{"hash_id": 5910940378786387706, "entities": ["multihop reasoning", "vqa models", "chainofthought prompting"], "background": "1. The need to evaluate and enhance the reasoning capabilities of VQA models, particularly in multi-hop reasoning scenarios, which are often overlooked in existing benchmarks and evaluations. 2. The observed inefficiency of traditional Chain-of-Thought (CoT) prompting in generating effective multi-hop reasoning for VQA tasks, especially for complex scenarios."}
{"hash_id": 8944722717957230238, "entities": ["sensitivity order incontext learning", "standard incontext learning", "suboptimal local optimum influence"], "background": "1. The sensitivity of large language models to the order of in-context learning examples, which can lead to significant variations in performance. 2. The potential for improving standard in-context learning by addressing the suboptimal results caused by the influence of each sample's local optimum on subsequent gradients."}
{"hash_id": 351077634443356802, "entities": [], "background": "1. The need to improve the performance of visualcommonsense reasoning (VCR) tasks by overcoming the limitations of existing models, which struggle with inferring conclusions beyond the image content. 2. The desire to enhance the collaboration between pre-trained vision-and-language models (VLMs) and large language models (LLMs) to utilize their complementary strengths without the need for in-domain fine-tuning."}
{"hash_id": 5217673292384560327, "entities": ["singlestep chainofthought prompting", "attention diffusion", "texttosql tasks", "problemsolving scope"], "background": "1. To overcome the limitations of single-step chain-of-thought prompting in LLMs, which suffer from attention diffusion and inadequate performance in complex tasks like text-to-SQL. 2. To enhance the problem-solving scope and attention of LLMs in text-to-SQL tasks without relying heavily on large labeled datasets, thus reducing annotation costs and improving accessibility."}
{"hash_id": 1616557017341614180, "entities": [], "background": "1. The need to effectively mine and understand the evolving opinions across different modalities of short video news to improve fake news detection accuracy. 2. The requirement to address the limitations of current methods that overlook implicit opinions and the dynamic nature of opinions in multimodal content."}
{"hash_id": 6858694344445023035, "entities": ["data contamination", "blackbox llms", "membership inference attack"], "background": "1. The need to detect and mitigate data contamination in LLMs, which can lead to misleading evaluation results and compromise the effectiveness and security of these models. 2. The requirement for a method that can be applied to black-box LLMs, where traditional membership inference attack (MIA) approaches are not suitable due to limited access to probability distributions."}
{"hash_id": 3074387196302129688, "entities": ["hallucinations mitigation", "lightweight method", "truthful knowledge incorporation"], "background": "1. The need to mitigate hallucinations in LLMs caused by untruthful contexts, which significantly limit the models' broader application and reliability. 2. The requirement for a lightweight and effective method that can selectively incorporate truthful knowledge while protecting LLMs from the negative impact of untruthful information."}
{"hash_id": 6523746875832515155, "entities": ["instruction tuning", "large language models", "dataefficient methods"], "background": "1. The high computing power overhead in instruction tuning for large language models necessitates more data-efficient methods. 2. Existing data-efficient methods are highly dependent on the quality of the original instruction-tuning dataset, which can be noisy, especially when synthesized by LLMs."}
{"hash_id": 3534720104842360420, "entities": ["zeroshot crosslingual transfer", "multilingual nlp tasks", "crosslingual alignment", "encoderdecoder models"], "background": "1. The need to improve zero-shot cross-lingual transfer in multilingual NLP tasks, which relies on effective cross-lingual alignment of encoder representations. 2. The requirement to extend the application of cross-lingual alignment methods beyond encoder models to include encoder-decoder and decoder-only models, particularly as the field shifts focus to generative models."}
{"hash_id": 6077140348689573835, "entities": ["negative transfer", "lifelong learning", "catastrophic forgetting"], "background": "1. The need to mitigate negative transfer that occurs when transferring knowledge between dissimilar tasks in lifelong learning, which can hinder overall learning performance. 2. The requirement for a more efficient and effective method to handle knowledge transfer that avoids catastrophic forgetting and promotes cumulative learning across tasks."}
{"hash_id": 1669991342997586394, "entities": ["domainspecific llms", "tuningfree approaches", "weaktostrong generalization"], "background": "1. The need to enhance the domain-specific capabilities of LLMs without the resource and time-intensive process of fine-tuning, particularly for closed-source commercial LLMs. 2. The desire to achieve weak-to-strong generalization by exploring tuning-free approaches that can match or exceed the performance of specialized state-of-the-art models."}
{"hash_id": 4588629971391160369, "entities": ["kbqa datasets", "lowresource languages", "dataset construction pipelines"], "background": "1. The need to bridge the gap in KBQA datasets for low-resource languages, which is part of a broader issue in NLP concerning underrepresented languages. 2. The inefficiency of existing dataset construction pipelines in terms of human labor and the untapped potential of modern tools like Large Language Models to assist in creating datasets."}
{"hash_id": 33568530034459856, "entities": ["knowledge inclusion", "sql query generation", "large language models"], "background": "1. Existing methods struggle with generating accurate SQL queries when essential knowledge is not explicitly included in the database schema or user question. 2. Large language models (LLMs) have the potential to improve but are hindered by the lack of explicit knowledge, affecting their performance and robustness."}
{"hash_id": 8283689631259000370, "entities": ["improve mbr decoding speed", "computational burden reduction", "neural metrics comet"], "background": "1. The need to improve the decoding speed of MBR decoding, which is quadratic in time complexity and becomes computationally infeasible with a large number of translation hypotheses. 2. The desire to maintain or enhance translation quality while reducing the computational burden, especially when using expensive neural metrics like COMET."}
{"hash_id": 5091602835780377501, "entities": ["manual question creation", "distractor design", "taskagnostic pretraining"], "background": "1. The manual creation of multiple-choice questions is time-consuming and effort-intensive for educators, particularly in designing effective distractors. 2. Existing language models used for distractor generation undergo task-agnostic pretraining, which may not fully align with the specific needs of the distractor generation task."}
{"hash_id": 3940732558840701676, "entities": ["neural ranking models", "semantic perturbations", "content injection attacks", "positional biases", "transformer attention mechanisms"], "background": "1. The need to understand and mitigate the robustness issues of neural ranking models (NRMs) against semantic perturbations, particularly in the context of content injection attacks. 2. The recognition that transformer attention mechanisms can introduce positional biases that can be exploited, leading to attacks that are not limited to a single query or topic."}
{"hash_id": 4137960053322109110, "entities": ["training data quality", "visionlanguage models", "finegrained evaluation metric", "visual concreteness"], "background": "1. The need to improve the quality of training data for vision-language models by filtering out noisy and low-quality samples, especially in resource-constrained settings. 2. The requirement for a fine-grained evaluation metric that can measure the visual concreteness of image captions without an image reference to enhance learning signals in multimodal datasets."}
{"hash_id": 4308419915388903612, "entities": ["realworld data limitations", "humangenerated data biases", "model training evaluation"], "background": "1. The need to overcome the limitations of real-world data, such as high costs, data scarcity, and privacy concerns, which hinder the collection of large amounts of high-quality data. 2. The recognition that human-generated data can be prone to biases and errors, and may not be optimal for training or evaluating models."}
{"hash_id": 7347347276936503417, "entities": ["enhanced text generation", "nonalphabetic languages", "autoregressive inefficiency", "multilingual model performance"], "background": "1. The need to enhance text generation speed in non-alphabetic languages, which are often overly tokenized by English-centric models, leading to inefficiency in the autoregressive generation process. 2. The requirement to maintain the performance of pre-trained multilingual models on specific monolingual tasks while reducing the need for extensive data and computational resources for retraining."}
{"hash_id": 1177854433833229125, "entities": ["semantic frames", "definition sentences", "lowresource languages", "interpretability enhancement"], "background": "1. The need to automate the process of creating definition sentences for semantic frames to reduce the manual effort and time required, especially for low-resource languages and domain-specific frames. 2. The desire to enhance the interpretability of automatically induced semantic frames by providing clear definition sentences, which is crucial for humans to understand the frames' meanings."}
{"hash_id": 5291322726344248567, "entities": ["generative retrieval models", "relevance judgments", "knowledge distillation", "training paradigms"], "background": "1. The need to enhance the performance and relevance judgments of generative retrieval models, which currently suffer from incomplete relevance judgments and binary labels that do not reflect varying levels of relevance. 2. The potential to leverage knowledge distillation to improve generative retrieval, despite the differences in training paradigms between generative and dense retrieval methods."}
{"hash_id": 8509591794926267642, "entities": ["video content", "lowresource languages", "toxic content detection", "legal regulations", "online safety"], "background": "1. The increasing amount of video content, particularly in low-resource code-mixed languages, which poses a significant challenge for detecting toxic content and ensuring online safety. 2. The need to comply with legal regulations that require the swift removal of toxic content from online platforms to avoid fines and create a healthier digital environment."}
{"hash_id": 2873204112817554426, "entities": ["stable evaluations", "reproducible benchmarks", "tool learning", "realworld apis"], "background": "1. The need for stable and reproducible evaluations in tool learning benchmarks to ensure consistent model performance assessments over time. 2. The requirement to balance the stability of benchmarks with the diversity and realism of the APIs and tools used in real-world scenarios."}
{"hash_id": 3515728929970416033, "entities": ["emotional intelligence ei", "llms general intelligence gi", "emotion perception", "catastrophic forgetting"], "background": "1. The need to comprehensively enhance Emotional Intelligence (EI) in LLMs, which is currently limited to emotion perception and neglects emotion cognition and expression. 2. The requirement to prevent catastrophic forgetting of the LLMs' General Intelligence (GI) during the EI enhancement process."}
{"hash_id": 2219708382471332822, "entities": ["large language models", "cultural context alignment", "south korean social values"], "background": "1. The need for Large Language Models (LLMs) to understand and align with the cultural context and basic knowledge of a specific country, like South Korea, for effective deployment. 2. The lack of existing benchmarks to measure the national alignment of LLMs with South Korean social values and common knowledge."}
{"hash_id": 6948666782128848005, "entities": ["improving sentence pair ranking", "nlp task performance", "highlow similarity treatment"], "background": "1. The need to improve the accuracy of ranking sentence pairs according to their similarity, which is crucial for enhancing performance in downstream NLP tasks. 2. The observation that current methods treat the degree of similarity as a continuous spectrum, ignoring the distinct nature of high-similarity (upper-range) and low-similarity (lower-range) samples."}
{"hash_id": 2210337880738939309, "entities": ["dataset sharing restrictions", "privacy concerns", "regulatory limitations", "classifier training", "sensitive user information"], "background": "1. The increasing restrictions on sharing datasets for abusive language detection due to privacy concerns and regulatory limitations. 2. The need to maintain the quality and utility of datasets for training classifiers without exposing sensitive user information."}
{"hash_id": 8835062844094741080, "entities": ["chinese language models", "cultural linguistic context", "llm benchmarks"], "background": "1. The need for a comprehensive benchmark to evaluate the performance of large language models in Chinese, given the lack of suitable tools that account for the language's specific cultural and linguistic context. 2. The recognition that existing benchmarks, such as MMLU, are biased towards Western culture and are not appropriate for assessing the capabilities of LLMs in diverse cultural and linguistic settings like Chinese."}
{"hash_id": 9093824195953795821, "entities": ["nuanced text evaluation", "rich context capture", "transparent evaluation system"], "background": "1. The need for a more nuanced and flexible method to evaluate the quality of VLM-generated text, which traditional metrics fail to provide, particularly in capturing rich context and granular aspects of the output. 2. The requirement for a transparent and accessible evaluation system that can serve as an alternative to expensive human evaluations and closed-source models like GPT-4V."}
{"hash_id": 6193833269078355516, "entities": ["mathematical reasoning evaluation", "error identification", "problemsolving capabilities"], "background": "1. The need for comprehensive evaluations of LLMs' mathematical reasoning capabilities from the examiner's perspective, specifically in error identification and correction, to inspire future model development. 2. The urgency to address the saturation of traditional evaluation tasks and the potential improvement in problem-solving capabilities through accurate error recognition and correction."}
{"hash_id": 3540437933401955420, "entities": ["computational cost reduction", "memory requirements", "explainable pruning algorithm"], "background": "1. The need to reduce the computational cost and memory requirements of large language models without compromising their performance. 2. The absence of a simple, universal, and explainable pruning algorithm that can be applied across various large language models."}
{"hash_id": 1358315120144084081, "entities": ["overconfidence in llms", "specious answers", "retrieval augmentation", "questionanswering efficiency"], "background": "1. To reduce the overconfidence of LLMs that leads to providing specious answers, which is particularly problematic in safety-critical fields like healthcare. 2. To optimize the use of Retrieval Augmentation (RA) by conducting it only when necessary, thus improving efficiency and performance in question-answering tasks."}
{"hash_id": 3096594381584199877, "entities": ["instructionfollowing alignment", "humanpreference alignment", "twostage training", "alignment tax"], "background": "1. The need to mitigate the optimization conflict between instruction-following alignment and human-preference alignment in two-stage training, which can result in suboptimal alignment of large language models. 2. The desire to refine the collaboration between the two alignment tasks to reduce the \"alignment tax\" phenomenon, where models struggle to maintain performance across both objectives."}
{"hash_id": 4266613795968853438, "entities": ["accelerate inference", "large language models", "common token sequences"], "background": "1. The need to accelerate the inference of Large Language Models (LLMs) while maintaining a balance between performance and efficiency of the draft model. 2. The observation that hypotheses produced by the draft model share many common token sequences, indicating a potential for optimizing computation."}
{"hash_id": 9196482105018784990, "entities": ["watermark detection efficiency", "large language models", "text generation quality"], "background": "1. The need to improve the efficiency of watermark detection in large language models to minimize the number of tokens required for detection, especially in short texts and after post-editing. 2. The requirement to maintain high-quality and diverse text generation while embedding watermarks to avoid altering the naturalness of the generated content."}
{"hash_id": 5346152288411347841, "entities": ["safety vulnerabilities", "large language models", "code completion tasks"], "background": "1. The need to identify and address safety vulnerabilities in Large Language Models (LLMs) that may not be evident when tested with traditional natural language inputs, particularly in the context of code completion tasks. 2. The concern that current safety alignment training methods for LLMs do not effectively generalize to domains beyond natural language, such as code, which is an important pre-training domain for modern LLMs."}
{"hash_id": 3975887558359510142, "entities": ["bidirectional reasoning", "artificial general intelligence", "reversal curse"], "background": "1. The need to bridge the gap in bidirectional reasoning capabilities of large language models, which is a significant limitation in the pursuit of artificial general intelligence (AGI). 2. The requirement for a comprehensive solution that goes beyond evaluation observations and provides a foundational understanding of the root cause of the reversal curse."}
{"hash_id": 8189385747086486422, "entities": ["pretrained speech models", "streaming scenarios", "adaptation process"], "background": "1. The need to harness the potential of pre-trained speech models in streaming scenarios where traditional models struggle due to their dependency on complete utterances. 2. The desire to simplify and universalize the adaptation process of pre-trained models for streaming tasks, avoiding complex and task-specific distillation methods."}
{"hash_id": 538146218570014925, "entities": ["trustworthiness of generated answers", "explicit references", "attribution limitations"], "background": "1. Ensuring trustworthiness and reliability of generated answers by providing explicit references to the sources of copied text. 2. Overcoming the limitations of existing attribution methods that require extensive resources and retraining, and do not offer granular attribution."}
{"hash_id": 698118210089528243, "entities": ["chain of thought", "tree of thought", "higherorder tom questions", "social interaction capabilities"], "background": "1. The current reasoning strategies like Chain of Thought (CoT) and Tree of Thought (ToT) are not suitable for ToM reasoning and do not improve the ToM capabilities of LLMs, especially in higher-order ToM questions. 2. There is a lack of effective reasoning strategies for solving higher-order ToM questions, which are crucial for enhancing the social interaction capabilities of LLMs."}
{"hash_id": 5783648768717099458, "entities": ["annotation bias", "nlu models", "bias mitigation techniques"], "background": "1. The reliance of NLU models on annotation bias reduces their generalizability and performance on out-of-distribution datasets. 2. Existing bias mitigation techniques treat the entire model as a black box, missing the opportunity to strategically target and modify components most responsible for mediating bias."}
{"hash_id": 8835682093685222368, "entities": ["nlp model comparison", "realworld performance", "model architecture changes", "linguistic capabilities"], "background": "1. The difficulty in comparing new NLP models to established ones due to the saturation of benchmarks that may not reflect actual performance differences in real-world scenarios. 2. The need to understand how changes in model architecture, such as distillation or size increase, affect the linguistic capabilities of NLP models."}
{"hash_id": 1069767868010068968, "entities": ["adaptive decoding strategy", "chitchat scenarios", "knowledgebased qa", "stochastic decoding algorithms"], "background": "1. The need for an adaptive decoding strategy that can handle both chit-chat and knowledge-based question answering scenarios, which have different requirements for response diversity and accuracy. 2. The limitation of existing stochastic decoding algorithms that cannot simultaneously address the varying decoding space needs for different dialogue scenarios due to their constant randomness."}
{"hash_id": 7764452042708963104, "entities": ["enhance generationbased models", "discourse structure", "argumentation mining", "controlled decoding"], "background": "1. The need to enhance the performance and generalizability of generation-based end-to-end argumentation mining models by incorporating discourse structure information, which has been proven effective for argument mining but is overlooked by current models. 2. The desire to optimize the efficiency and effectiveness of argumentation structure decoding by moving away from traditional autoregressive decoding strategies to a more controlled and parallelizable approach."}
{"hash_id": 1012908316761427147, "entities": ["lowfrequency named entities", "nmt systems", "customer experience", "entity translation", "sentence translation context"], "background": "1. The inadequate translation of low-frequency named entities poses a significant challenge for NMT systems, which is crucial for improving customer experience in commercial translation systems. 2. Existing techniques for named entity translation often decouple the entity translation from the overall sentence translation process, missing the opportunity to enhance translation accuracy within the context of the sentence."}
{"hash_id": 8205543130133859806, "entities": ["dualencoder retrievers", "dark knowledge", "negative sampling", "probability distribution"], "background": "1. The need to improve the performance of dual-encoder retrievers by better utilizing the dark knowledge from cross-encoder teachers, which is not effectively transferred due to the sharpness of soft labels provided by the teacher. 2. The requirement to bridge the gap between highly-relevant and loosely-relevant pairs in negative sampling methods to create a smoother probability distribution that captures more fine-grained information."}
{"hash_id": 2304960109188697689, "entities": ["instruction forgetting", "selfattention mechanism", "conditional sequence generation", "instructionfollowing capability"], "background": "1. The risk of instruction forgetting due to the self-attention mechanism's focus on nearby words in long input sentences, leading to a mismatch between generated responses and user intent. 2. The need to enhance the instruction-following capability of large language models without additional data or annotation costs to improve performance in conditional sequence generation tasks."}
{"hash_id": 3139312636994032145, "entities": ["scaling transformer models", "computational costs", "moe models", "computational inefficiencies"], "background": "1. The need to scale Transformer models without a corresponding increase in computational costs, which is crucial for further advancements in large language models. 2. The observation that existing MoE models exacerbate computational inefficiencies, wasting resources on parameters that are not essential for the input."}
{"hash_id": 4826396375701544884, "entities": ["scaling transformers", "deep architectures", "training instability", "deepnorm limitations"], "background": "1. The need to scale Transformers to extremely deep architectures to enhance model capacity and performance, while addressing the inherent training instability and undertraining issues. 2. The limitation of previous methods like DeepNorm, which stabilize early training but may lead to undertrained models in the long run due to constraints on parameter updates."}
{"hash_id": 7902510818654261123, "entities": ["multistep temporal reasoning", "dynamic knowledge", "tkgqa models"], "background": "1. The lack of datasets and methods that focus on multi-step temporal reasoning in TKGQA, which is essential for understanding dynamic knowledge. 2. The need to improve the performance of existing TKGQA models in handling complex temporal reasoning tasks."}
{"hash_id": 6723490835884439115, "entities": ["improve language models", "inherent uncertainty", "conversation forecasting", "calibration", "opensource models"], "background": "1. The need to improve language models' ability to account for and represent the inherent uncertainty in conversations, akin to how humans manage uncertainty in social interactions. 2. The desire to enhance the calibration of smaller open-source language models to perform on par with much larger pre-trained models in conversation forecasting tasks."}
{"hash_id": 2948741898828975351, "entities": ["training large language models", "crossdomain performance", "model finetuning"], "background": "1. The high cost of training and fine-tuning large language models, which demands extensive computing resources and can lead to inconsistent performance across different domains. 2. The need to enhance model performance in cross-domain or cross-task scenarios without the requirement for further training or additional training data."}
{"hash_id": 7853255733424200999, "entities": ["twostage multitask learning", "additional parameters", "taskspecific knowledge", "transfer learning efficiency"], "background": "1. To reduce the number of additional parameters introduced by two-stage multi-task learning methods, which can be inefficient and lead to increased computational costs. 2. To enhance the reusability of task-specific knowledge and make transfer learning less sensitive to task selection, thereby improving overall task transfer efficiency."}
{"hash_id": 3146934492606225610, "entities": ["enhancing multimodal dialogue", "integrating images", "pretrained vlms", "dialogue context comprehension"], "background": "1. The need to enhance the quality and effectiveness of multimodal responses in dialogue systems by integrating images. 2. The limitation of existing pretrained vision language models (VLMs) in accurately comprehending complete dialogue contexts, leading to suboptimal image retrieval."}
{"hash_id": 6810771634262571160, "entities": ["scientific data visualization", "large language models", "researcher productivity"], "background": "1. The need to automate the time-consuming and labor-intensive process of scientific data visualization to enhance researcher productivity. 2. The untapped potential of Large Language Models (LLMs) for use in scientific data visualization, which could align with the principles of clarity and precision needed in scientific communication."}
{"hash_id": 2171714191710958739, "entities": ["catastrophic forgetting", "embeddings instability", "continual fewshot settings", "training data strategies"], "background": "1. Current methods neglect the instability of embeddings during different task trainings, leading to catastrophic forgetting. 2. There is a need for strategies that can effectively utilize limited training data in continual few-shot settings without compromising the stability of learned embeddings."}
{"hash_id": 4698098440808379671, "entities": ["societal values alignment", "chinese large language models", "moral alignment capacity"], "background": "1. The need to ensure that Chinese large language models align with societal values and norms, particularly as they become more integrated into real-world applications. 2. The lack of comprehensive datasets tailored to evaluate the moral alignment capacity of Chinese LLMs within the context of Chinese culture."}
{"hash_id": 2350357737036161014, "entities": ["explainable ai xai", "trustworthiness assessment", "explanation quality", "uncertainty metrics"], "background": "1. The need to assess the trustworthiness of explanations provided by Explainable AI (XAI) methods, which are crucial for understanding model behavior, especially when small input perturbations can significantly distort these explanations. 2. The absence of a comprehensive investigation into the existing link between uncertainty and explanation quality, despite the potential for uncertainty metrics to indicate model stability issues."}
{"hash_id": 5827980195188276253, "entities": ["decoderonly large language models", "text ranking", "indomain outdomain performance"], "background": "1. The need to leverage the potential of decoder-only large language models (LLMs) for text ranking, which has been relatively unexplored compared to encoder-only and encoder-decoder models. 2. The desire to improve the performance of LLMs in text ranking, particularly in both in-domain and out-domain scenarios, by better adapting them to this task."}
{"hash_id": 7889447409529369844, "entities": ["training data quality", "language models", "human intervention impact"], "background": "1. The need to improve the quality of training data for fine-tuning language models, particularly in languages less resourced than English, to enhance the perceived quality of generated dialogues. 2. The desire to understand the impact of human intervention on machine-generated data and its effect on the performance of dialogical models across different model sizes."}
{"hash_id": 4662514312790805452, "entities": ["query expansion methods", "supervised training", "large language models", "opendomain question answering"], "background": "1. Existing query expansion methods rely on supervised training and struggle with effectiveness across domains and datasets. 2. Large language models have shown promise for query expansion in information retrieval but have not been effectively tailored for the unique requirements of open-domain question answering."}
{"hash_id": 1668021785068206495, "entities": ["sfms relative merits", "downstream slu tasks", "performance benchmark"], "background": "1. The lack of a fine-grained understanding of the relative merits of different SFMs and their usage for downstream SLU tasks. 2. The need for a performance benchmark specifically designed for evaluating SFMs on complex and realistic spoken language understanding tasks."}
{"hash_id": 7942012555049248764, "entities": ["instant feedback", "grammatical error correction", "large language models"], "background": "1. The need to provide instant feedback and personalized learning opportunities for second language learners through improved grammatical error correction. 2. The potential to enhance the performance of existing GEC systems by leveraging the capabilities of large language models and understanding their effectiveness in comparison to supervised models."}
{"hash_id": 2424655023195342176, "entities": ["text simplicity", "explainable method", "benchmarking tool"], "background": "1. The need for an explainable method to assess the overall simplicity of text, which is currently missing and essential for broadening the application of text simplification. 2. The requirement for a benchmarking tool that is independent of target audience and domain, to accurately evaluate simplicity in diverse contexts."}
{"hash_id": 9221335127570356687, "entities": ["limitation of experimental methods", "causal impact", "text features"], "background": "1. The need to overcome the limitations of experimental methods that are constrained to a small number of pre-specified text treatments, which may lack external validity and effectiveness. 2. The desire to discover and understand the causal impact of text features beyond just topics or specific words, which may not always be the primary mechanism of influence on outcomes."}
{"hash_id": 7781719386166959460, "entities": ["training language models", "computational overhead", "scalability", "dataset models"], "background": "1. To improve the efficiency of training language models by reducing the computational overhead and time associated with using corpus-level n-gram statistics. 2. To enable the scalability of these techniques to larger datasets and models without compromising model quality or convergence rate."}
{"hash_id": 7907339695333428364, "entities": ["emotions in conversations", "emotion knowledge graphs", "dynamic emotion transformations"], "background": "1. The need to understand and represent the complex transformations that occur between emotions in real-world conversations, which is crucial for fields like psychology, natural language processing, and neuroscience. 2. The limitation of existing Emotion Knowledge Graphs (EKGs) in focusing on static emotion expressions rather than dynamic emotion transformations and the absence of models that can effectively capture these transformations."}
{"hash_id": 637841506276095506, "entities": ["multilingual capabilities", "large language models", "vocabulary sharing"], "background": "1. The need to enhance the multilingual capabilities of Large Language Models (LLMs) beyond their strong performance in English, particularly when trained predominantly on English data. 2. The desire to understand and leverage the underlying mechanisms, such as vocabulary sharing, that can improve the efficiency and effectiveness of LLMs in multiple languages."}
{"hash_id": 7722402690637669091, "entities": ["nonenglish data", "supervised finetuning", "catastrophic forgetting", "crosslingual generation"], "background": "1. The limited availability and high cost of non-English data for supervised fine-tuning (SFT) hinders the alignment of foundation language models with non-English tasks. 2. The risk of catastrophic forgetting and the superficial alignment achieved through SFT motivate the search for a training-free method to elicit cross-lingual generation without compromising pre-existing knowledge."}
{"hash_id": 4856309435638938737, "entities": ["hierarchical text classification", "peer label interactions", "hierarchical label structure", "classification performance"], "background": "1. Existing methods often fail to capture the interactions between peer labels, leading to confusion and reduced accuracy in hierarchical text classification. 2. The need to fully utilize the hierarchical structure of labels to improve the performance of classification in HTC tasks."}
{"hash_id": 1580744681947815767, "entities": ["zeroshot crosslingual transfer", "multilingual llms", "crosslingual alignment"], "background": "1. The need to explain the mechanisms behind the zero-shot cross-lingual transfer performance of multilingual LLMs, particularly without explicit parallel data supervision. 2. The desire to understand the dynamics of cross-lingual alignment throughout pre-training and how this impacts the emergence of cross-lingual capabilities in LLMs."}
{"hash_id": 7513036148754870003, "entities": ["limited supervision", "clustering methods", "uncontrollable clustering centers"], "background": "1. The scarcity of labeled data in many domains hinders the effectiveness of traditional text clustering algorithms, necessitating a method that can perform well with limited supervision. 2. Existing unsupervised and semi-supervised clustering methods struggle with uncontrollable clustering centers and require prior knowledge of categories, which is not always available."}
{"hash_id": 3131378148364210967, "entities": ["large language models", "limited resources", "quantization strategies", "instructiontuned llms", "perplexitybenchmark performance"], "background": "1. The need to deploy large language models on devices with limited resources, which requires reducing their compute and memory requirements without significant performance loss. 2. The lack of comprehensive evaluation of quantization strategies on instruction-tuned LLMs and the need to understand the relationship between perplexity and benchmark performance in quantized models."}
{"hash_id": 4621026063251929331, "entities": ["translation memories", "lowresource languages", "monolingual data", "backtranslation"], "background": "1. The limitations of existing methods in finding closely matching translation memories, especially in specific domains or for low-resource languages, which hinders NMT's adaptability. 2. The potential to utilize the abundant monolingual data available in most languages without the need for additional training, such as in back-translation."}
{"hash_id": 3482857764440086612, "entities": ["evaluation metrics", "nuanced quality assessment", "audio language models"], "background": "1. The need for evaluation metrics that can provide a nuanced understanding of the quality of audio captions, beyond a single overall score. 2. The desire to have an evaluation approach that can explicitly identify strengths and weaknesses in audio language models (ALMs) to guide improvements."}
{"hash_id": 2401246322140878708, "entities": ["nuanced alignment", "language models", "human preferences", "language feedback"], "background": "1. The need for more nuanced alignment of language models with human preferences beyond scalar rewards to capture specific aspects of commendations and critiques. 2. The potential to improve the scalability and effectiveness of alignment by using language feedback (judgments) that can convey both positive and negative aspects tailored to the model's current capabilities."}
{"hash_id": 5291334487123723710, "entities": ["effective planning strategies", "logical reasoning", "argumentative essay generation", "suitable datasets"], "background": "1. The need for more effective planning strategies that incorporate detailed logical reasoning in argumentative essay generation to improve the quality and persuasiveness of essays. 2. The scarcity of suitable datasets for training and evaluating argumentative essay generation systems, which hinders progress in the field."}
{"hash_id": 3640053149266868782, "entities": ["fallacy recognition", "dataset size", "underrepresented fallacies", "context identification"], "background": "1. The limited availability and size of datasets for fallacy recognition, which hinders the ability of models to generalize and detect rare fallacy types effectively. 2. The need to improve the performance of fallacy recognition models on underrepresented fallacies, especially those that require additional context for accurate identification."}
{"hash_id": 4996155604906085772, "entities": ["incontext learning", "language models", "conceptdependent training data"], "background": "1. The need to understand and improve the quality and robustness of in-context learning in language models, especially since the emergence of this ability is not solely dependent on model scale or multi-task training. 2. The recognition of the importance of concept-dependent training data in enabling in-context learning, as evidenced by theoretical work suggesting that learning to utilize latent concepts can lead to effective in-context learners even in small-scale settings."}
{"hash_id": 8902818408985254150, "entities": ["predictive nlp models", "userlevel features", "purchase behavior", "psychological constructs"], "background": "1. The need to enhance the predictive power of NLP models for user behavior by incorporating user-level features beyond the text itself. 2. The potential to better understand and predict purchase behavior and product recommendations by integrating psychological constructs into NLP."}
{"hash_id": 8620495299026788190, "entities": ["label bias", "nonautoregressive translation", "autoregressive models"], "background": "1. The need to overcome label bias, which is a known issue in generative models like HMMs, especially in the context of non-autoregressive translation where it can lead to suboptimal decoding paths. 2. The goal to improve the performance and efficiency of non-autoregressive translation models to match or exceed the quality of autoregressive models without the drawback of slow inference speed."}
{"hash_id": 2998762842988656082, "entities": ["multimodal content", "stance detection", "visual modality"], "background": "1. The increasing prevalence of multi-modal content (texts with images) in social media posts necessitates the development of methods that can accurately detect stance from these combined modalities. 2. Existing stance detection methods primarily focus on text, neglecting the visual modality that often carries crucial context for understanding users' opinions."}
{"hash_id": 8481887021384432046, "entities": ["hallucinated facts", "truthful directions", "query contexts", "uncertainty capture"], "background": "1. Large language models often generate hallucinated facts, which reduces their reliability in critical applications that require truthful responses. 2. Existing methods that apply truthful directions with a one-size-fits-all intensity fail to generalize across different query contexts and do not adequately capture uncertainty."}
{"hash_id": 8040623782009967977, "entities": ["capitalizing cognitive capabilities", "multimodal tasks", "mmllms survey"], "background": "1. The need to capitalize on the cognitive capabilities of Large Language Models (LLMs) to enhance multi-modal tasks while reducing the high computational costs of training multi-modal models from scratch. 2. The desire to systematize the emerging field of MultiModal Large Language Models (MM-LLMs) by providing a comprehensive survey and taxonomy, which can guide future research and improve the alignment between modalities."}
{"hash_id": 1150530850937708432, "entities": ["large language models", "lesstrained languages", "cultural linguistic diversity"], "background": "1. To address the limitations of large language models in less-trained languages like Chinese, specifically their diminished effectiveness and potential data leakage biases in evaluations. 2. To push towards the development of more culturally informed and linguistically diverse models that can better generalize to new linguistic territories."}
{"hash_id": 3396156249820138004, "entities": ["reward overoptimization", "large language models", "unnatural language generation", "kl regularization", "computational expense"], "background": "1. The need to mitigate reward over-optimization (ROO) in large language models (LLMs) that can lead to unnatural language generation and reduced diversity. 2. The computational expense and limitations of existing approaches, such as KL regularization, which require careful hyperparameter tuning and focus solely on regularizing the language policy without addressing the reward function itself."}
{"hash_id": 4432097692163844271, "entities": ["idiomatic language representation", "noncompositional meanings", "idiomatic expressions", "nlp downstream tasks"], "background": "1. The need to improve the representation of idiomatic language in NLP, which is crucial for enhancing the performance of downstream tasks like machine translation and text simplification. 2. The challenge of capturing the non-compositional meanings of idiomatic expressions, where the traditional compositional approaches fail to adequately represent the idiomaticity of words and phrases."}
{"hash_id": 2075204682730364959, "entities": ["barriers to knowledge propagation", "knowledge editing methods", "reasoning tasks performance"], "background": "1. The need to understand and overcome the barriers that hinder the appropriate propagation of updated knowledge within models, which is crucial for accurate reasoning. 2. The requirement to improve the performance of knowledge editing methods on reasoning tasks, as current approaches struggle with recall and coherence post-editing."}
{"hash_id": 7303221736371824064, "entities": ["stateoftheart nlp models", "longitudinal modelling", "mental health monitoring", "temporal context", "mood changes prediction"], "background": "1. The need to utilize state-of-the-art NLP models like Transformers for longitudinal modelling of text, which is crucial for healthcare tasks such as mental health monitoring. 2. The importance of incorporating temporal context and the timing of events in predicting mood changes, which is not effectively addressed by existing RNN-based models."}
{"hash_id": 6154337878375997379, "entities": ["autonomous agents", "natural language instructions", "environmental feedback", "vln methods", "simulator settings"], "background": "1. The need for autonomous agents to navigate complex, real-world environments using natural language instructions, which requires the ability to self-correct plans based on environmental feedback. 2. The limitations of existing vision-and-language navigation (VLN) methods that operate in less realistic simulator settings and do not effectively incorporate environmental feedback into decision-making."}
{"hash_id": 3664372745828797218, "entities": ["mmir models", "scientific domain", "visual language models", "domainspecific adaptation", "ocr technology"], "background": "1. The lack of comprehensive evaluation of MMIR models within the scientific domain, which has unique characteristics and data distribution compared to generic data. 2. The need to improve the performance of visual language models (VLMs) on scientific MMIR tasks through domain-specific adaptation and the integration of OCR technology."}
{"hash_id": 1545693289700050062, "entities": ["motion perception", "videotext models", "quality datasets"], "background": "1. The need to improve video-text models' ability to perceive and represent motion due to actions, which is essential for a more comprehensive understanding of video content. 2. The absence of quality datasets that explicitly describe motion in videos, which hinders the evaluation and advancement of video-text models in motion understanding."}
{"hash_id": 8919566799979636950, "entities": ["specialized domains", "language models", "tailored datasets"], "background": "1. Large language models often underperform in specialized domains compared to specialized models, especially when domain-specific knowledge is required. 2. The existing one-size-fits-all approach of instruction tuning datasets does not adequately address the needs of specialized domains, which require tailored datasets for effective adaptation."}
{"hash_id": 8129800977949875240, "entities": ["civility in communication", "paraphrasing offensive content", "supervised generative models", "novel settings adaptation"], "background": "1. The need to improve civility in communication environments by paraphrasing offensive content instead of removing it, which can reduce user participation and diversity in online discussions. 2. The challenge of adapting supervised generative models to novel settings due to the requirement of large amounts of labelled training data and the potential retention of toxicity in the paraphrased output."}
{"hash_id": 7220242335855201038, "entities": ["language models", "knowledgeintensive tasks", "epistemological holism"], "background": "1. The increasing application of language models in complex knowledge-intensive tasks, which requires a deeper understanding of their inherent knowledge and reasoning capabilities. 2. The philosophical inquiry into the nature of knowledge and belief systems within language models, and how they align with the principles of epistemological holism."}
{"hash_id": 8900616237873207253, "entities": ["eliminating logical incoherence", "negationrelated hallucinations", "training efficiency"], "background": "1. To improve the reasoning capabilities of language models by eliminating strong hallucinations caused by logical incoherence, specifically those related to negation. 2. To propose a method that does not require training on sparse negative data, making it more practical and efficient for model improvement."}
{"hash_id": 1059348537631808578, "entities": ["evaluation metrics", "natural language processing", "automatic evaluation methods"], "background": "1. The need for reliable and unbiased evaluation metrics in natural language processing to ensure meaningful progress in generated content assessment. 2. The concern over the cost and time constraints of human evaluation, which prompts the development of automatic evaluation methods that must be free from inherent biases."}
{"hash_id": 4840583618954759438, "entities": ["safe llm deployment", "realworld tasks", "dynamic benchmarks"], "background": "1. The need for a better understanding of LLM capabilities on real-world tasks for safe development and deployment. 2. The requirement for dynamic and authentic benchmarks that prevent test data contamination and overfitting, and reflect temporal consistency."}
{"hash_id": 4817055191463780852, "entities": ["misinformation reduction", "llm reliability", "sycophantic behavior mitigation"], "background": "1. The need to reduce the spread of misinformation and improve the reliability of LLMs when users provide them with partial or misleading information. 2. The desire to understand and mitigate the sycophantic behavior of LLMs, which can lead to the confident presentation of inaccurate facts, undermining trustworthiness."}
{"hash_id": 373279961336168821, "entities": ["textual data augmentation", "scalability", "consistency", "fewshot learning tasks"], "background": "1. The need to improve the scalability and consistency of textual data augmentation methods, particularly when using large language models, across various downstream tasks. 2. The desire to enhance the quality of augmented data to improve model performance on few-shot learning tasks."}
{"hash_id": 2951435530036716614, "entities": ["computational cost", "pretrained transformer models", "nlp task", "transferability estimation"], "background": "1. The high computational cost of exhaustively fine-tuning all available pre-trained transformer language models to find the best fit for a specific NLP task. 2. The need for a more accurate and efficient method to estimate the transferability of these models without the need for extensive fine-tuning."}
{"hash_id": 1910359049906562987, "entities": ["specialized event linking methods", "event knowledge bases", "outofkb event mentions", "novel techniques", "event linking models"], "background": "1. The distinct features of events, such as complex structures and information-rich nature, necessitate specialized methods that go beyond the traditional entity linking approaches. 2. The scarcity of event knowledge bases and the limited attention given to identifying and classifying \"out-of-KB\" event mentions require novel techniques to enhance the performance of event linking models."}
{"hash_id": 1041110993452399249, "entities": ["improving language model credibility", "reducing hallucinations", "enhancing efficiency with smaller lms"], "background": "1. The need to improve the credibility and accountability of language models by ensuring their responses are grounded in verifiable sources, reducing the occurrence of errors and hallucinations. 2. The desire to enhance the efficiency of large language models by validating their outputs with smaller LMs that are better at processing relevant information given a query."}
{"hash_id": 7313173482812169785, "entities": ["evaluation inconsistencies", "event extraction methods", "reproducibility studies"], "background": "1. The need to address evaluation inconsistencies and insufficiencies that may not accurately reflect the true performance of event extraction methods. 2. The desire to improve the reproducibility of event extraction studies and facilitate fair comparisons between different approaches."}
{"hash_id": 933139016398621504, "entities": ["performance gap", "opensource models", "iterative refinement"], "background": "1. The need to bridge the performance gap between open-source code generation models and advanced proprietary systems like GPT-4 Code Interpreter. 2. The requirement for open-source models to incorporate execution feedback and human feedback for iterative refinement to improve code quality and alignment with user expectations."}
{"hash_id": 6219486083804849135, "entities": ["indexical bias", "information retrieval", "biased rankings audit"], "background": "1. The lack of reliable metrics and procedures for automatically measuring indexical bias in Information Retrieval systems, which can significantly affect people's opinions and behaviors. 2. The need to audit and address biased rankings in order to ensure fairness, neutrality, and a balance of ideas in the retrieval of information."}
{"hash_id": 475720515689967931, "entities": ["inclusive instruction datasets", "nonenglish language models", "arabic instructiontuning dataset"], "background": "1. The need for more inclusive and culturally diverse instruction datasets to reduce biases in language models and improve their effectiveness in non-English contexts. 2. The lack of a dedicated, culturally relevant Arabic instruction-tuning dataset that captures the linguistic peculiarities and cultural nuances of the Arab region."}
{"hash_id": 4561545824230021213, "entities": ["scarcity annotated datasets", "structured data extraction", "chest xray reports", "imaging modalities"], "background": "1. The scarcity of densely annotated datasets that capture a wide array of clinically relevant information from diverse radiology reports, limiting the effectiveness of structured data extraction methods. 2. The need to extend beyond chest X-ray reports to include a variety of imaging modalities and anatomical regions for more comprehensive clinical insights."}
{"hash_id": 4736368415592375739, "entities": ["systematic task composition", "instruction tuning", "strategic pruning", "representative subset"], "background": "1. The need for a systematic method to determine task compositions for instruction tuning beyond manual tuning or relying on practitioners' intuition. 2. The potential for improved performance by strategically pruning tasks and fine-tuning on a smaller, more representative subset of tasks."}
{"hash_id": 2971550814456382544, "entities": ["improve visionlanguage models", "reduce overcautious abstention", "inferencetime algorithm", "enhance selective systems coverage"], "background": "1. The need to improve the reliability of vision-language models by reducing the over-cautious abstention that leads to a significant loss of correct predictions. 2. The requirement for an inference-time algorithm that can enhance the coverage of selective vision-language systems without compromising on accuracy and without the need for additional training."}
{"hash_id": 1202432979086737197, "entities": ["lowresource language pairs", "machine translation", "northern smifinnish"], "background": "1. The need to improve machine translation for low-resource language pairs, such as Finnish to Northern S\u00e1mi, where conventional methods may not suffice due to limited parallel data. 2. The goal of advancing the state of the art in Northern S\u00e1mi-Finnish machine translation to potentially generalize findings to other less-resourced/minoritized languages."}
{"hash_id": 1624432239630624473, "entities": ["data privacy", "differential privacy", "model compression", "efficient deployment"], "background": "1. The urgent need to maintain data privacy, especially when training large language models on private data, which has led to the requirement for differential privacy. 2. The necessity to compress large language models for efficient deployment on devices with limited resources or in latency-sensitive applications, without significantly compromising model utility."}
{"hash_id": 353958062820402721, "entities": ["language models capabilities", "writing assistants", "knowledgeintensive settings", "detailed instructions", "llmpowered conversational agents"], "background": "1. The lack of understanding of language models' capabilities as writing assistants, particularly in knowledge-intensive settings, where precise and informed revisions are crucial. 2. The need to measure progress and improve language models' ability to follow specific and detailed instructions, given the increasing adoption of LLM-powered conversational agents for real-world tasks."}
{"hash_id": 3372601619335217473, "entities": ["content selection strategies", "salient segments", "tag generation", "reader navigation"], "background": "1. The need to improve content selection strategies to identify salient and relevant segments within lengthy articles, given the overwhelming number of news articles published daily. 2. The underexplored area of tag generation, which is crucial for guiding readers to topics of interest and facilitating navigation across related articles."}
{"hash_id": 8850152210489835278, "entities": [], "background": "1. The lack of a reliable and interpretable metric for evaluating LLMs' compliance with complex instructions, which is crucial for their practical application. 2. The need for a comprehensive benchmark to systematically test and analyze the instruction-following capabilities of LLMs across diverse scenarios."}
{"hash_id": 2184909249928920483, "entities": ["high cost llms", "text reranking", "budgetaware solutions"], "background": "1. The high cost associated with using Large Language Models (LLMs) for text re-ranking due to API charges based on the number of input and output tokens, which becomes especially problematic for high volumes of queries. 2. The lack of existing budget-aware solutions that optimize both cost and performance for text re-ranking using LLMs."}
{"hash_id": 6609374791597767269, "entities": ["advanced numerical processing", "realtime financial analysis", "financial decisionmaking"], "background": "1. The need for advanced numerical processing and reasoning capabilities to handle dense numerical information and domain-specific jargon in financial documents. 2. The requirement for real-time analysis in the rapidly changing financial market while mitigating hallucinations that reduce the usability of large language models in financial decision-making."}
{"hash_id": 5152849077110109388, "entities": ["scarcity of tuning data", "multimodal misalignment", "rlhf paradigm adaptation"], "background": "1. The scarcity of high-quality visual instruction tuning data leads to misalignment and hallucinations in Large Multimodal Models. 2. The need to adapt the successful Reinforcement Learning from Human Feedback (RLHF) paradigm from text-based AI agents to improve multimodal alignment in LMMs."}
{"hash_id": 2856174321327826376, "entities": ["llm defense strategies", "evaluation suite", "overdefensiveness issue"], "background": "1. The lack of a unified and diverse evaluation suite for LLM defense strategies, which hinders accurate and precise assessment of their safety and over-defensiveness. 2. The need to understand and mitigate the issue of over-defensiveness in LLMs, which has been largely overlooked in previous research."}
{"hash_id": 3485707783082450727, "entities": ["presentanchored temporal questions", "reallife scenarios", "temporal questionanswering", "dynamic temporal information"], "background": "1. The need to address the unique challenges of present-anchored temporal questions, which are common in real-life scenarios but have been largely overlooked in existing temporal question-answering research. 2. The requirement for a benchmark that can continuously provide up-to-date answers due to the dynamic nature of present-anchored temporal information."}
{"hash_id": 8254660229447956837, "entities": ["multiagent systems", "comprehensive evaluation", "degree assessments", "experience accumulation", "agent performance"], "background": "1. The need to improve the capability of agents in multi-agent systems beyond self-evaluation and removal of underperforming agents, by incorporating comprehensive evaluation and experience accumulation. 2. The inspiration from corporate organizational practices, particularly the use of 360-degree assessments, to enhance agent performance through reflection and continuous learning."}
{"hash_id": 7228737962059535305, "entities": ["relation extraction", "pncs annotation", "automated systems", "structured format"], "background": "1. The complexity of annotating detailed information on PNCs and the scarcity of available data make conventional document-level relation extraction techniques impractical. 2. The necessity for automated systems to transform unstructured journal data into a structured format to enhance discovery efficiency and reduce manual errors."}
{"hash_id": 948265631246710800, "entities": ["large language models", "chainofthought prompting", "recommender systems", "evaluation frameworks"], "background": "1. The potential of Large Language Models (LLMs) in reasoning tasks, especially through Chain-of-Thought (CoT) prompting, has not been fully explored in the context of recommender systems, which deal with subjective and personalized user preferences. 2. There is a lack of methods to objectively assess the quality of reasoning in LLMs within the realm of recommender systems, necessitating the development of new evaluation frameworks."}
{"hash_id": 4446465735421295025, "entities": ["lowresource african languages", "machine translation", "evaluation benchmarks"], "background": "1. The significant gap in machine translation for low-resource African languages hinders cross-cultural understanding and knowledge exchange, and the proposed methods aim to bridge this divide. 2. The lack of comprehensive evaluation benchmarks and improved metrics for African language translation necessitates the development of new tools to advance the field of NLP for these languages."}
{"hash_id": 4932436641027856374, "entities": ["neural machine translation", "structural generalization", "crosstask evaluation", "semantic parsing"], "background": "1. The need to assess the structural generalization capabilities of neural machine translation models beyond the commonly evaluated lexical generalization. 2. The importance of cross-task evaluations to understand the differences in generalization abilities between semantic parsing and machine translation."}
{"hash_id": 4292625811801018019, "entities": ["mathematical reasoning abilities", "competitionlevel problems", "multistep reasoning", "annotated datasets", "side information"], "background": "1. The need to evaluate and enhance the mathematical reasoning abilities of LLMs, particularly in the context of competition-level problems that require multi-step reasoning and the utilization of additional information. 2. The lack of existing datasets that provide annotated concepts and hints, which are crucial for understanding how LLMs can benefit from such side information in solving complex math problems."}
{"hash_id": 6931109930643394536, "entities": ["large language models", "translation quality", "nmt systems", "hybrid threshold method"], "background": "1. The emergence of Large Language Models (LLMs) presents an opportunity to surpass the translation quality of contemporary NMT systems, especially in generating authentic-sounding translations and handling infrequent words. 2. Existing approaches to combining NMT with LLMs, such as the Hybrid Threshold method, face challenges with alignment to human judgment and efficiency due to increased decoding time."}
{"hash_id": 5040641303268668979, "entities": ["pretrained language models", "speech recognition", "lowdata settings", "decoding process", "domain adaptation"], "background": "1. To leverage the power of pre-trained speech and language models to enhance end-to-end speech recognition performance, especially in low-data settings and to simplify the decoding process. 2. To capitalize on the recent advancements in large language models and their potential for domain adaptation and inference optimization in the context of speech recognition."}
{"hash_id": 6714574828059419537, "entities": ["hallucination detection", "llm applications", "annotation process"], "background": "1. The expensive and quickly outdated annotation process hinders traditional fine-tuning for hallucination detection, especially with the rapid advancement of LLMs across various domains. 2. The need for an accurate, fast, and affordable hallucination detection system is crucial to enhance the safety and trustworthiness of LLM applications."}
{"hash_id": 3252871005002224875, "entities": ["cloudbased inference services", "sensitive data", "smpc methods", "transformer models", "privacypreserving inference"], "background": "1. The need to protect privacy in cloud-based inference services, especially for sensitive data like financial and personal information, which is not adequately addressed by current SMPC methods due to performance degradation. 2. The requirement to maintain the accuracy and efficiency of Transformer models while enabling privacy-preserving inference, as current solutions either sacrifice performance or lead to significant slowdowns."}
{"hash_id": 3148445683127485131, "entities": ["llmintegrated applications", "prompt extraction attacks", "intellectual property rights"], "background": "1. The increasing deployment of LLM-integrated applications makes the theft of proprietary instruction prompts a significant concern for intellectual property rights and raises ethical and privacy issues. 2. There is a lack of comprehensive understanding and systematic benchmarking of the effectiveness and mechanisms of prompt extraction attacks and defenses."}
{"hash_id": 2287548874518381513, "entities": ["history modeling", "conversational search", "manual supervision signals", "shortcut history dependency"], "background": "1. The need to improve history modeling in conversational search to better understand the user's information need within the context of the conversation, especially for long conversations with topic shifts. 2. The challenge of overcoming the limited availability of manual supervision signals and the issue of shortcut history dependency that hampers existing conversational dense retrieval approaches."}
{"hash_id": 6463449012457171808, "entities": ["grounding longtailed entities", "suboptimal search engine performance", "explainability in image selection", "mmkgs quality"], "background": "1. The limitations of current methods in grounding long-tailed entities due to suboptimal performance of search engines and the infrequent appearance of these entities in pre-training datasets, leading to a lack of matching images and poor accuracy in MMKGs. 2. The need for explainability in the image selection process to ensure the quality and reliability of the grounded entities in MMKGs."}
{"hash_id": 6173329266552034973, "entities": ["zeroshot stance detection", "unseen targets", "multiple domains", "costeffective training", "dataefficient approach"], "background": "1. The need to improve the generalization of zero-shot stance detection models to unseen targets across multiple domains, as previous studies mainly focused on single or limited domains. 2. The desire to create a more cost-effective and data-efficient approach for training stance detection models, reducing the reliance on large human-annotated datasets."}
{"hash_id": 8418263306008067071, "entities": ["zeroshot crosslingual performance", "lowresource languages", "synthetic taskspecific data"], "background": "1. The need to improve zero-shot crosslingual performance in low-resource languages where labeled data is not available. 2. The potential to leverage the generation capabilities of large language models to create synthetic task-specific data for training."}
{"hash_id": 8833587253614290791, "entities": null, "background": "1. The need to effectively address the coupled problem of stance detection and rumor verification in social media, which is crucial for mitigating the spread of misinformation. 2. The challenge of training multi-task models on limited and expensive-to-obtain labeled data for stance detection and rumor verification."}
{"hash_id": 607869126495762357, "entities": null, "background": "1. To overcome the limitations of existing methods that fail to capture detailed information in captions, leading to compromised multimodal alignment and potential hallucinations. 2. To improve the compositional reasoning ability of models like CLIP by enhancing the quality and density of the captions used in pre-training."}
{"hash_id": 664746634340335821, "entities": null, "background": "1. The need to improve generalization in zero-shot information retrieval where both test queries and corpora are inaccessible at training time, mimicking real-world deployment scenarios. 2. The desire to bridge the lexical gap between queries and documents that sparse models struggle with, leading to underperformance in dense models compared to simpler sparse models like BM25."}
{"hash_id": 618440565420102868, "entities": null, "background": "1. The need for a general text evaluator based on open-source Large Language Models to compete with the performance of commercial counterparts like GPT-4. 2. The limitation of open-source models like Llama in evaluative tasks, particularly in multi-aspect evaluations, due to a shortage of annotated resources."}
{"hash_id": 6275291825598521610, "entities": ["adversarial robustness", "transformer models", "training data impact", "model robustness influence"], "background": "1. The need to understand how training data impacts the adversarial robustness of transformer models, which is crucial for high-stake domains where model vulnerabilities can lead to significant negative consequences. 2. The existence of a gap in the literature where previous works focused on model-first approaches, neglecting the influence of fine-tuning data on model robustness."}
{"hash_id": 5198562438998059311, "entities": ["semantic gap", "graph reasoning", "model hallucinations"], "background": "1. The need to bridge the semantic gap between graph data and text to improve large language models' ability to perform graph reasoning and generation tasks. 2. The requirement to reduce hallucinations in model outputs, which can be caused by fabricated inputs or lack of pertinent knowledge in graph data."}
{"hash_id": 6182605517085312165, "entities": ["llm task adaptation", "prompt engineering", "adaptability generalization"], "background": "1. To overcome the limitations of existing LLM task adaptation methods, which are sensitive to prompt format and choice of exemplars, and require extensive prompt engineering. 2. To enhance the adaptability and generalization of LLM-powered agents in interactive environments, particularly for complex and unseen tasks."}
{"hash_id": 8296285504251729077, "entities": null, "background": "1. The limitations of previous research in hypothetical induction, which used manually selected observations and focused on commonsense knowledge, making the task less challenging and not reflective of open-domain scenarios. 2. The potential to enhance scientific discovery by leveraging large language models to generate novel hypotheses that are not only valid but also new to humanity."}
{"hash_id": 7637229706854466105, "entities": ["prototype representations inconsistency", "imbalanced semantic distribution", "fewshot tasks"], "background": "1. The inconsistency of prototype representations across different few-shot tasks due to different context sentences for the same relation, even when integrating text labels. 2. The presence of imbalanced distribution in the semantic space caused by different granularities of relations, leading to a lack of comparability and affecting classification results."}
{"hash_id": 7331149224818959374, "entities": ["overcorrection reduction", "chinese spelling correction", "error pattern generalization"], "background": "1. The need to reduce over-correction in Chinese Spelling Correction models, which currently over-fit to the error model and under-fit the language model. 2. The desire to improve the generalization of CSC models to error patterns outside the standard distribution without requiring additional task-related pre-training."}
{"hash_id": 5723635889645240957, "entities": ["multimodal language models", "comprehensive review", "visual grounding", "critical areas"], "background": "1. The rapid development of large language models that can handle multiple modalities requires a comprehensive review to understand their capabilities and limitations. 2. Existing surveys have overlooked critical areas such as visual grounding, image generation, and editing, which are important for advancing multimodal language models."}
{"hash_id": 2083816333505671586, "entities": ["codecomment aligned data", "code llms", "model capabilities", "efficient approach"], "background": "1. The scarcity of code-comment aligned data in pre-training corpora hinders the performance of code LLMs, and the high correlation between natural language comments and code suggests that enhancing this alignment could improve model capabilities. 2. The cost and feasibility of generating large amounts of code-comment aligned data using existing methods are prohibitive, necessitating a more efficient and effective approach."}
{"hash_id": 6218700202274577916, "entities": ["domain adaptation", "nonautoregressive translation", "catastrophic forgetting"], "background": "1. The need to improve domain adaptation in Non-Autoregressive Translation (NAT) models, which have been under-explored compared to their Autoregressive (AT) counterparts, despite showing promise in terms of efficiency. 2. The desire to enhance translation performance in specific domains without the computational expense and potential for catastrophic forgetting that can come with fine-tuning large language models."}
{"hash_id": 6603491457511413934, "entities": ["reversal mathematical reasoning", "reversal curse", "general reasoning capacities", "traditional lefttoright training objectives"], "background": "1. The need to improve the reversal mathematical reasoning ability of large language models, which currently struggle with the \"reversal curse\" when applied to complex mathematical tasks. 2. The requirement to enhance the general reasoning capacities of LLMs by addressing the insufficient modeling of the relationship between reasoning steps caused by traditional left-to-right training objectives."}
{"hash_id": 807582159807861221, "entities": ["logical semantic relations", "global topological context", "neural symbolic reasoning"], "background": "1. The need to capture the logical semantic between relations and the global topological context information, which current KGC approaches fail to effectively address due to their reliance on simple linear updates and aggregation of only local neighborhood information. 2. The requirement for a more comprehensive approach that can infer missing knowledge in knowledge graphs with better interpretability and accuracy, by integrating both neural reasoning and symbolic reasoning."}
{"hash_id": 7154293048467230385, "entities": ["large language models", "knowledge updating", "realtime settings"], "background": "1. Large language models often \"hallucinate\" factually incorrect information due to outdated knowledge, which reduces their trustworthiness, especially in real-time settings where accurate information is crucial. 2. Existing methods for updating knowledge in LLMs, such as additional training or in-context learning, are not easily scalable for real-time updates."}
{"hash_id": 5623625722455167792, "entities": ["aligning large language models", "safety", "instructiontuned llms"], "background": "1. The current approaches for aligning large language models with safety require substantial training efforts, which are costly and inefficient. 2. There is a need for more efficient and flexible methods to boost the safety of instruction-tuned LLMs without compromising their general-purpose abilities."}
{"hash_id": 3443368467483376476, "entities": ["concept reasoning", "large language models", "dataset absence"], "background": "1. The need to enhance the concept reasoning capabilities of large language models without relying on stored factual knowledge (modeledge leakage) or direct extraction from the input context (context leakage). 2. The absence of a dedicated dataset and optimized reasoning method to evaluate and improve large language models' concept reasoning performance in various downstream tasks."}
{"hash_id": 101978723817975703, "entities": ["label sparsity", "short text", "word cooccurrence patterns"], "background": "1. The limited length of short texts results in incomplete labels for likelihood maximization, leading to biased training signals and suboptimal topic modeling. 2. Existing methods have not effectively addressed the label sparsity issue, which is distinct from the data sparsity problem, and fail to capture the word co-occurrence patterns specific to a domain."}
{"hash_id": 528187828848671996, "entities": ["user feedback acceptance", "refuting instructions", "instructionfollowing benchmarks"], "background": "1. The need to assess and improve large language models' ability to positively accept and consistently adhere to user feedback, particularly in the form of refuting instructions, which is crucial for enhancing user interaction and customization in various applications. 2. The gap in existing benchmarks that evaluate instruction-following capabilities of LLMs, specifically in the context of refuting instructions that challenge the model's initial responses."}
{"hash_id": 7093128704934287143, "entities": ["knowledge graph completion", "complex logical query answering", "training time overhead"], "background": "1. Existing knowledge graph completion (KGC) models are not well-calibrated for complex logical query answering (CLQA), which leads to inaccuracies in the outcomes and limits their performance on this task. 2. The current methods for CLQA require extensive training on numerous complex queries, resulting in substantial training time overhead and limited generalization to new query structures."}
{"hash_id": 9218998287280096471, "entities": ["improve translation faithfulness", "decoderonly llms", "source context alignment"], "background": "1. The need to improve the faithfulness of translations generated by large language models, which often generate hallucinatory responses due to a lack of explicit alignment with source contexts. 2. The requirement to mitigate the bias towards previously generated tokens over corresponding source tokens in decoder-only LLMs, which leads to insufficient focus on the source context during translation."}
{"hash_id": 2916290764122258994, "entities": ["block diagram summarization", "existing approach limitations", "hallucination problem", "ocr errors"], "background": "1. The scarcity of literature and methods specifically focused on block diagram summarization, which is essential for document understanding and efficiency. 2. The need to overcome the limitations of existing approaches, such as the hallucination problem, OCR errors, and the lack of a large dataset, particularly for complex block diagrams."}
{"hash_id": 557059626886441851, "entities": ["contextdependent textsql", "diverse sql operations", "schema dependencies", "long dialogues", "dynamic schema changes"], "background": "1. To improve the realism and effectiveness of context-dependent Text2SQL datasets by incorporating diverse SQL operations and schema dependencies that reflect complex, real-world database interactions. 2. To enhance the performance of models in handling long dialogues and capturing the dynamic changes in database schemas during interactive dialogues."}
{"hash_id": 3050771203403092171, "entities": ["leveraging demonstration knowledge", "machine translation", "promptingbased ltms", "tuningbased ltms"], "background": "1. To improve the adaptability and translation depth of large language models in machine translation by better leveraging demonstration knowledge. 2. To address the limitations of both prompting-based and tuning-based LTMs by combining their advantages and mitigating the negative impact of noisy demonstrations."}
{"hash_id": 8861357094743240343, "entities": ["pretrained language models", "outofdomain data", "domain adaptation methods"], "background": "1. The need to improve the performance of pre-trained language models on out-of-domain data, where they currently struggle due to differences in term distributions. 2. The goal to enhance existing domain adaptation methods by incorporating more accurate and representative domain-specific information."}
{"hash_id": 1872826451234930999, "entities": ["selfcorrection capabilities", "reasoning tasks", "mistake finding", "output correction"], "background": "1. The need to improve the self-correction capabilities of LLMs, particularly in reasoning tasks where they currently struggle to identify and fix errors without external feedback. 2. The recognition that separating the process of mistake finding from output correction can lead to a better understanding of each component's challenges and potential improvements."}
{"hash_id": 8576673916177109557, "entities": ["data contamination", "large language models", "texttosql", "zeroshot settings"], "background": "1. To investigate the impact of data contamination on the accuracy and reliability of Large Language Models in Text-to-SQL translation tasks. 2. To provide a more rigorous evaluation of LLMs' capabilities in zero-shot settings by using both familiar and completely new datasets."}
{"hash_id": 6291916954846677402, "entities": ["misinformation in data visualizations", "labeling claims", "debunking explanations"], "background": "1. The need to combat the prevalent issue of misinformation spread through data visualizations like charts, which is a less studied area compared to misleading chart design. 2. The importance of not just labeling claims as true or false, but also providing explanations to convince readers and debunk misinformation effectively."}
{"hash_id": 1145717784462845256, "entities": ["zeroshot entity linking", "conversational settings", "domainspecific knowledge bases"], "background": "1. The current evaluation approaches for zero-shot entity linking do not capture the real-world complexities and challenges of conversational settings, leading to suboptimal performance in practical applications. 2. There is a need for EL models that can adapt to new, domain-specific knowledge bases without requiring extensive training data, given the scarcity of entity-annotated conversational datasets and the dynamic nature of knowledge bases."}
{"hash_id": 3247828054935648053, "entities": ["professional consulting competence", "chinese psychological counseling", "authentic counseling datasets", "multiturn dialogues evaluation", "benchmark quality"], "background": "1. The need to enhance the professional consulting competence of large language models in Chinese psychological counseling by utilizing authentic counseling datasets. 2. The requirement for an effective benchmark to evaluate the quality of multi-turn dialogues in psychological counseling."}
{"hash_id": 4117177460016652844, "entities": ["implicit hate speech", "nuanced stereotypes", "contextual reasoning"], "background": "1. The current methods for generating explanations for implicit hate speech do not effectively capture the nuanced stereotypes and require a better understanding of contextual reasoning and societal norms. 2. The use of knowledge graph tuples in existing models is not always task-agnostic and may not account for the multi-hop/indirect nature of hate, leading to suboptimal performance."}
{"hash_id": 1331277738092991341, "entities": ["scalable explanations", "large language models", "explanation methods limitations"], "background": "1. The need for scalable and faithful explanations to understand the internal mechanisms of large language models, especially when dealing with long documents, to address concerns related to safety, security, and truthfulness. 2. The limitations of existing explanation methods in terms of computational efficiency and their ability to scale to larger models and longer inputs."}
{"hash_id": 5929077675628609528, "entities": ["improving large language models", "addressing suboptimality", "unbalanced data distribution", "preventing overfitting", "rare document learning"], "background": "1. The need to improve the efficiency and generalization of Large Language Models by addressing the sub-optimality of random sampling which ignores the unbalanced nature of training data distribution. 2. The requirement to prevent overfitting on common samples while ensuring that rare documents are adequately learned by the model."}
{"hash_id": 3233625843228248600, "entities": ["facilitating communication", "deafhearing communities", "sign language models", "parallel videotext corpora"], "background": "1. The need to facilitate communication between the deaf and hearing communities without relying on expensive and time-consuming manual annotations of sign language. 2. The challenge of effectively training sign language translation and generation models due to the scarcity of high-quality parallel video-text corpora."}
{"hash_id": 6219305999843679033, "entities": ["insufficiency annotated data", "nonenglish languages", "annotation complexity", "laborintensive task"], "background": "1. The insufficiency of annotated data for training semantic parsers, particularly in non-English and low-resource languages, which hinders the scalability of semantic formalisms to multiple languages. 2. The complexity and cost of manual annotation processes, which make existing semantic parsing formalisms difficult to apply broadly due to the labor-intensive nature of the annotation task."}
{"hash_id": 4411488309686900326, "entities": ["computational storage challenges", "attention memory swapping", "decoderbased llms"], "background": "1. The large scale of language models introduces significant computational and storage challenges, limiting their wider applicability and increasing costs related to attention and memory swapping. 2. Existing methods to address these challenges either require additional hardware or lead to a drop in performance, and they do not effectively handle dynamic context shifts in decoder-based LLMs."}
{"hash_id": 428808378009404491, "entities": ["reduce hallucinations", "enhance verifiability", "simplify evidence location"], "background": "1. The need to improve the reliability and trustworthiness of large language models by reducing hallucinations and enhancing verifiability through better citation practices. 2. The desire to simplify the process for users to locate specific supporting evidence by moving beyond the current practice of citing document identifiers."}
{"hash_id": 3756860124434787215, "entities": ["single architecture", "entity linking", "relation extraction", "computational demands"], "background": "1. The need for a single architecture that can simultaneously improve performance, inference speed, and flexibility in both Entity Linking and Relation Extraction tasks. 2. The goal to make high-quality Information Extraction accessible to academic groups with limited resources by reducing the computational demands and training costs."}
{"hash_id": 3124546114326283090, "entities": ["semantic depth", "intent descriptors", "conversational intent discovery", "large language models", "context length limitations"], "background": "1. The need to improve the semantic depth and precision of intent descriptors in Conversational Intent Discovery to avoid overfitting to known intents and enhance the detection of new intents. 2. The requirement for a method that can effectively leverage the strengths of Large Language Models (LLMs) for intent discovery without being hindered by their context length limitations and the potential for generating unpredictable intent labels."}
{"hash_id": 6380614760682164401, "entities": ["factuality metrics", "interpretability", "longform document summaries"], "background": "1. The lack of interpretability in current factuality metrics, which do not specify which parts of the summary are factual or hallucinated. 2. The need for a metric that can evaluate summaries of long-form documents, as current metrics are mainly designed for short documents like news articles."}
{"hash_id": 7370025334509438298, "entities": ["computational costs reduction", "selfconsistency strategy", "reasoning paths", "qualitydiversity tradeoff"], "background": "1. The need to reduce the high computational costs and low efficiency of the self-consistency strategy when generating reasoning paths for complex reasoning tasks. 2. The requirement to maintain or improve the quality of reasoning while reducing the number of samples needed, thus achieving a better trade-off between quality and diversity."}
{"hash_id": 5830179382012850519, "entities": ["decoderonly llms", "sequence labeling tasks", "causal mask"], "background": "1. The need to improve the performance of decoder-only LLMs on sequence labeling tasks, which are crucial for information extraction, to match or exceed the performance of encoder-only models. 2. The observation that the causal mask, which restricts bidirectional information flow, may be responsible for the subpar performance of LLMs on these tasks."}
{"hash_id": 4515497559081299876, "entities": ["languagespecific datasets", "multilingual language models", "languagespecific knowledge"], "background": "1. The current lack of language-specific datasets for evaluating the natural language understanding capabilities of multilingual language models, particularly in capturing language-specific knowledge and common sense. 2. The high costs and limited availability of annotators for manually creating multilingual datasets, which often result in translated datasets that do not accurately reflect language-specific nuances."}
{"hash_id": 197178401304755713, "entities": ["effective pretraining strategies", "linearized constituency trees", "syntactically controlled paraphrase generation", "tradeoffs in scpg"], "background": "1. The lack of effective pre-training strategies that enable models to understand and generate intrinsically non-sequential structures like Linearized Constituency Trees (LCTs) in syntactically controlled paraphrase generation. 2. The need to overcome the trade-offs and shortcomings in data obtaining, task formulation, and pretraining strategies that are currently tacitly accepted in the field of SCPG."}
{"hash_id": 4667767441643012298, "entities": ["reduce model reliance", "improve zeroshot icl efficiency", "associated costs"], "background": "1. The need to reduce the model's reliance on external information and the associated costs of constructing demonstration pools and incorporating external databases. 2. The requirement to improve the efficiency and reliability of zero-shot ICL, as current methods are either time-consuming or generate content of uncertain quality."}
{"hash_id": 6419010975337418949, "entities": ["data scarcity", "endtoend speech translation", "modality gap", "zeroshot setting"], "background": "1. The need to overcome the challenge of data scarcity in end-to-end speech translation by using only ASR and MT data without requiring paired ST data. 2. The requirement to bridge the modality gap between speech and text, which hinders the performance of ST systems, especially in a zero-shot setting where no target language data is available."}
{"hash_id": 6954938878701090821, "entities": ["mathematical reasoning", "large language models", "numerals and units", "problem complexity"], "background": "1. The need to improve the performance of Large Language Models (LLMs) in mathematical reasoning tasks that involve numerals and units of measurement, which are crucial in real-life scenarios. 2. The recognition that minor changes in numbers or units can significantly affect problem complexity and LLM performance, which has not been adequately addressed in current evaluations of these models."}
{"hash_id": 7638470124692059621, "entities": ["large language models", "factually accurate health information", "presuppositions in queries"], "background": "1. The increasing integration of large language models into search offerings necessitates ensuring they provide factually accurate health information, which can be critical in high-stakes scenarios. 2. The potential for presuppositions in user queries to lead to reinforcement of false health claims, highlighting the need for models that can challenge incorrect premises."}
{"hash_id": 727573569926048969, "entities": ["word sense disambiguation", "plain text", "lexical semantics integration"], "background": "1. The difficulty of applying WSD to plain text due to its strong assumptions about identified spans and provided sense candidates. 2. The need to integrate lexical semantics into downstream applications more effectively."}
{"hash_id": 3281469260736175177, "entities": ["neural network memorization", "computer vision", "natural language processing", "generalisation hypothesis", "language classification tasks"], "background": "1. To investigate the inconsistent findings in the literature regarding the localization of memorization in neural networks, particularly between computer vision and natural language processing domains. 2. To provide a more nuanced understanding of the \"generalisation first, memorisation second\" hypothesis in the context of natural language classification tasks."}
{"hash_id": 3187806484163598668, "entities": ["temporal knowledge graph reasoning", "sparsity of data", "multirelational inferences"], "background": "1. The current benchmarks and models for temporal knowledge graph reasoning are limited by the sparsity of the data and the reliance on simple single-relational reasoning, which does not capture the complexity of real-world temporal relationships. 2. There is a need for more complex and diverse reasoning patterns that can handle multi-hop and multi-relational inferences in temporal knowledge graphs."}
{"hash_id": 6609207059205535715, "entities": ["realtime hallucination detection", "llm effectiveness", "unsupervised approach", "computational efficiency"], "background": "1. The need for real-time detection of hallucinations in LLMs to improve their effectiveness and robustness in practical applications, as current post-processing methods are computationally intensive and have high latency. 2. The requirement for an unsupervised approach that does not rely on expensive and time-consuming manual annotations, given the rapid development of LLM techniques."}
{"hash_id": 984494837622592763, "entities": ["unified sentiment model", "sentiment subtasks", "large language model"], "background": "1. The need to integrate interrelated sentiment knowledge among various sentiment subtasks into a unified model to enhance overall sentiment understanding. 2. The absence of an open-source large language model with robust sentiment analysis capabilities that can perform well across multiple sentiment subtasks."}
{"hash_id": 7453361050418343435, "entities": ["adversarial attacks", "nlp models", "attackagnostic defense"], "background": "1. The need to enhance the robustness of NLP models against adversarial attacks, particularly in black-box settings where the attacker has limited access to the model. 2. The requirement for a lightweight and attack-agnostic defense mechanism that does not significantly increase computational overhead during training or inference."}
{"hash_id": 4886811317893709888, "entities": ["academic integrity", "text originality", "plms", "innate originality enhancement"], "background": "1. The need to ensure academic integrity and originality in text generated by PLMs, especially in domains like academic writing and storytelling. 2. The absence of research focusing on directly enhancing the innate originality of content generated by PLMs, rather than just detecting plagiarism."}
{"hash_id": 4785075571401014059, "entities": ["data scarcity", "multimodal emotion recognition", "unsupervised domain adaptation", "domain alignment", "heterogeneous data"], "background": "1. The need to address data scarcity in multimodal emotion recognition by leveraging unsupervised domain adaptation techniques. 2. The requirement for a balanced domain alignment across different modalities to prevent under-training of certain modalities and to optimize the utilization of heterogeneous data."}
{"hash_id": 4345583552652582666, "entities": ["crosslingual summarization", "hallucinated content", "automatic methods", "faithfulness issues"], "background": "1. The need to improve the quality of training and evaluation data in cross-lingual summarization by reducing the presence of hallucinated content in reference summaries. 2. The lack of automatic methods specifically designed for cross-lingual settings to assess and address faithfulness issues in summarization datasets."}
{"hash_id": 4666616265215461419, "entities": ["unify information extraction", "modalities inefficiency", "treat modalities equally"], "background": "1. The need to unify various information extraction tasks across different modalities to avoid inefficiency and resource wastage caused by studying each modality in isolation. 2. The requirement to treat all modalities equally and recognize fine-grained information from them, rather than prioritizing text-centric outputs."}
{"hash_id": 4596868100516588447, "entities": ["domain bias", "imagedialogue datasets", "multimodal large language models", "visualtextual alignment", "complex human instructions"], "background": "1. The need to overcome domain bias in existing image-dialogue datasets that can limit the generative capabilities of multimodal Large Language Models. 2. The desire to enhance the alignment of visual and textual modalities in LLMs to better comprehend complex human instructions."}
{"hash_id": 5648447979348176899, "entities": ["semantic evolution", "generative llms", "contextual word representations"], "background": "1. The need to understand the semantic evolution within generative large language models (LLMs) like GPT, which differ from discriminative models like BERT in their architecture and training objective. 2. The potential to improve the extraction of contextual word representations from generative LLMs, which are known for their robustness and are foundational models in various applications."}
{"hash_id": 5736625708919318285, "entities": ["escalating expenses", "retraining ml models", "large language models", "trainingfree manner"], "background": "1. The escalating expenses and resource demands associated with re-training and fine-tuning large ML models. 2. The potential of Large Language Models (LLMs) to improve ML performance in a cost-effective and training-free manner."}
{"hash_id": 4204722548249611043, "entities": ["debate evaluation automation", "large language models", "argument organization"], "background": "1. The need to automate the debate evaluation process to improve debate quality in various scenarios and to advance the development of debate automatons. 2. The challenge of using Large Language Models (LLMs) for judging long, multi-turn debates, which requires a deep understanding of argument organization, refutation, and a systematic evaluation across multiple dimensions."}
{"hash_id": 225245076182241130, "entities": ["language model alignment", "human preferences", "blackbox llms", "whitebox models"], "background": "1. The need to align language models with human preferences without the drawbacks of complex, unstable, and resource-intensive reinforcement learning methods like PPO. 2. The potential to leverage the alignment capabilities of existing well-aligned black-box LLMs to improve the alignment of parameter-visible white-box models."}
{"hash_id": 4300290863089320544, "entities": ["multimodal document understanding", "operational bottleneck", "document intelligence services", "enterprise datasets"], "background": "1. The current multimodal document understanding models are not effectively applied to real-world enterprise datasets due to constraints and limitations, leading to a significant operational bottleneck and reliance on error-prone manual processes. 2. There is a growing demand for Document Intelligence services in enterprise settings, which requires improving the performance and reliability of multimodal document understanding models."}
{"hash_id": 1756631349205920630, "entities": ["user satisfaction estimators", "robustness improvement", "dataset balancing"], "background": "1. The need to improve the robustness of user satisfaction estimators in TOD systems when faced with an increase in dissatisfaction labels, which is currently unexplored due to the highly skewed nature of existing datasets. 2. The requirement for a more cost-effective and efficient method to balance datasets with more dissatisfactory dialogue samples, as traditional data collection and annotation are both time-consuming and expensive."}
{"hash_id": 394376848954985997, "entities": ["answering difficulty", "retrieved evidence", "question complexity", "qa system performance"], "background": "1. The need to estimate the answering difficulty of questions based on the availability and completeness of retrieved evidence. 2. The absence of a comprehensive metric that captures the complexity of questions in terms of required information spread across multiple documents, which affects QA system performance."}
{"hash_id": 7909000056033268011, "entities": ["longitudinal mental health", "social media posts", "clinical assessments", "timeseries language data", "extractive summarization methods"], "background": "1. The need to capture and summarize longitudinal changes in individuals' mental health through their social media posts, which is crucial for clinical assessments but not currently accessible to clinicians. 2. The limitations of existing methods in handling the arbitrary lengths of time-series language data and the impracticality of purely extractive summarization methods for social media timelines."}
{"hash_id": 1255043442611243572, "entities": ["generative tasks", "pixelbased language models", "symbolic input representations", "outofvocabulary words"], "background": "1. The need to extend pixel-based language models to perform generative tasks, which current models are unable to do. 2. The desire to reduce the dependency on symbolic input representations and the issues associated with finite vocabularies, such as out-of-vocabulary words and susceptibility to orthographic attacks."}
{"hash_id": 7122488541616546238, "entities": ["large language models", "computational costs", "training efficiency", "inherent sparsity"], "background": "1. The high computational costs and time requirements of additional training for large language models, such as continual pre-training and supervised fine-tuning. 2. The potential to improve the efficiency of training by leveraging the inherent sparsity observed in activated neurons during forward iterations."}
{"hash_id": 7577938614648122218, "entities": ["robustness enhancement", "chainofthought reasoning", "malicious prompt injection attacks"], "background": "1. The need to enhance the robustness of Chain-of-Thought (CoT) reasoning in large language models against preemptive answers that can significantly impair their reasoning capabilities. 2. The requirement to protect against potential malicious prompt injection attacks that could deliberately manipulate the model's output by providing preemptive answers."}
{"hash_id": 177843204460841162, "entities": ["opensource llms", "roleplaying optimization", "interaction experiences", "closedsource llms"], "background": "1. The lack of optimization for role-playing in existing open-source LLMs, which are predominantly trained on general domains, leading to a need for enhanced interaction experiences and familiar companionship. 2. The constraints imposed by the closed-source nature of state-of-the-art LLMs, such as high API costs, unavailability of fine-tuning, and limited context window size, which necessitate alternative solutions."}
{"hash_id": 4598383152701166747, "entities": ["embodied agents", "diverse environments", "internalized world knowledge", "embodied tasks", "communication action strategies"], "background": "1. The need to assess the capabilities of LLMs as embodied agents without relying on simulation engines and in diverse, dynamic environments. 2. The desire to enhance the generalizability of LLMs across different embodied tasks by allowing them to develop \"internalized world knowledge\" and customize communication and action strategies."}
{"hash_id": 2925671774247762858, "entities": [], "background": "1. To enhance the robustness of spoken language understanding (SLU) models against errors propagated from automatic speech recognition (ASR) systems. 2. To improve the utilization of both clean manual transcripts and error-prone ASR transcripts for better SLU performance."}
{"hash_id": 3374680968338764302, "entities": ["instructiontuning", "multitask transfer", "catastrophic forgetting"], "background": "1. The need to understand the mechanisms behind the success of instruction-tuning in order to improve a model's ability to generalize to unseen tasks via multi-task transfer. 2. The desire to mitigate the negative impact of instruction-tuning on transfer learning and in-context learning, specifically addressing the issue of catastrophic forgetting."}
{"hash_id": 527091092983750283, "entities": ["incontext learning performance", "fewshot learning", "example orders"], "background": "1. The sensitivity of in-context learning performance to the order of examples, which significantly impacts few-shot learning capabilities, especially when in-domain data is scarce. 2. The need for an approach that does not rely on additional data for assessing example orders, given the practical challenges of acquiring annotated data in certain domains."}
{"hash_id": 6069325475001538413, "entities": ["multimodal comprehension", "theoreticallygrounded framework", "higherlevel cognitive skills"], "background": "1. The need for a theoretically-grounded framework to evaluate multi-modal comprehension beyond simple memorization and low-level reasoning tasks. 2. The requirement to probe and improve the consistency and reliability of multi-modal models, especially when dealing with higher-level cognitive skills."}
{"hash_id": 3032121394291555833, "entities": ["automatic claim attribution", "standardized benchmarks", "attribution evaluation methods"], "background": "1. The need for efficient and effective methodologies to automatically assess the attribution of claims in generated responses, as human evaluation is costly and time-consuming. 2. The lack of standardized benchmarks for comparing different attribution evaluation methods, leading to a gap in understanding the challenges and progress in this field."}
{"hash_id": 7548958697768376329, "entities": ["text attribute control", "autoregressive model errors", "diffusion model quality"], "background": "1. The need to control the attributes of generated text, such as sentiment or toxicity, for various applications and audiences without degrading the fluency of the language model. 2. The desire to avoid the decoding errors that can cascade in auto-regressive models when using existing guidance methods, while also overcoming the higher perplexity and lower generation quality of diffusion models."}
{"hash_id": 1948985865659814244, "entities": ["excessive memorization", "model edits portability", "editing approach stability"], "background": "1. To overcome the excessive memorization, error propagation, and knowledge conflict issues that hinder the portability of model edits in one-hop or multi-hop content. 2. To improve upon the existing model editing methods that either focus on direct parameter modification or rely on external knowledge repositories, by introducing a more effective and stable editing approach."}
{"hash_id": 3644401161755900553, "entities": ["bridging preference labels", "tokenlevel human preference", "autoregressive generation", "discrete reward systems"], "background": "1. The need to bridge the gap between sequence-level preference labels and the autoregressive generation of tokens in language models. 2. The desire to account for varying degrees of human preference at the token level, which is not captured by existing discrete reward systems."}
{"hash_id": 4379422983489201252, "entities": ["improve large language models", "mitigate attention bias", "intrinsic information preference"], "background": "1. The need to improve the ability of large language models to effectively utilize and capture relevant information from the middle of long input contexts, which is crucial for tasks like search and summarization. 2. The desire to understand and mitigate the intrinsic attention bias in LLMs that leads to a preference for information at the beginning and end of inputs, regardless of relevance."}
{"hash_id": 2331555585501334753, "entities": ["temporal misalignment", "language model", "model performance", "internal temporal knowledge"], "background": "1. The significant impact of temporal misalignment between language model pretraining and deployment on model performance. 2. The need to update and align models' internal temporal knowledge without solely relying on incorporating new knowledge post-pretraining."}
{"hash_id": 5889373856674287994, "entities": ["multilingual toxicity mitigation", "foundational benchmark", "diverse languages toxicity evaluation"], "background": "1. The need to extend toxicity mitigation efforts to multilingual settings as language models are increasingly used in languages other than English, which monolingual approaches cannot address effectively. 2. The requirement to establish a foundational benchmark and framework for evaluating and improving toxicity mitigation across diverse languages due to the lack of existing research in this area."}
{"hash_id": 3654828341973617260, "entities": ["nlp systems vulnerability", "backdoor attacks", "democratization of models", "defense mechanism"], "background": "1. The increasing vulnerability of NLP systems to backdoor attacks due to the democratization of pre-trained language models through open-source initiatives. 2. The need for a practical and resource-efficient defense mechanism that does not require retraining or access to the model's training source or the nature of the attack."}
{"hash_id": 2023158376367019451, "entities": ["text accessibility", "reading difficulties", "aligned simplification datasets", "portuguese language"], "background": "1. The need to increase accessibility to complex texts for individuals with reading difficulties, cognitive disabilities, or non-native speakers, particularly in Portuguese, which is spoken by a large population. 2. The scarcity of aligned simplification datasets in Portuguese, which necessitates the development of methods that do not rely on large amounts of supervised training data."}
{"hash_id": 5098487722363335881, "entities": ["zeroshot translation", "mmt models", "resourceconsuming", "englishcentric"], "background": "1. The extensive data mining and bridging efforts required to improve zero-shot translation in MMT are resource-consuming and not practical for scaling up to real-world applications. 2. The potential of single-language-centric MMT models, particularly English-centric ones, is often overlooked and underutilized for zero-shot translation."}
{"hash_id": 1471106096784549753, "entities": ["prediction errors", "neural grammar induction", "compact grammars", "parsing tasks"], "background": "1. To overcome the issue of prediction errors, high variance, and the necessity for extensive grammars that result from structural optimization ambiguity and simplicity bias in unsupervised neural grammar induction. 2. To improve the learning of more compact, accurate, and consistent explicit grammars for better interpretability and performance in parsing tasks."}
{"hash_id": 5441106595056232800, "entities": ["gap in questionanswering research", "database querying", "multihop questioning", "financial reports data"], "background": "1. The need to bridge the gap in existing question-answering research, which often overlooks the essential steps of database querying and reasoning in real-world scenarios. 2. The requirement for a dataset that accurately reflects the complexities of multi-hop and sequential questioning, which are common in discussions involving financial reports and other tabular data."}
{"hash_id": 8666765906112556539, "entities": ["fact correction in language models", "model editing techniques", "multiple edits performance"], "background": "1. The need to correct and update incorrectly learned facts in large language models without the cost and inefficiency of retraining the entire model. 2. The requirement for model editing techniques that can handle multiple edits over time, maintaining performance and avoiding interference between facts."}
{"hash_id": 3647770510856073811, "entities": ["form document understanding", "diverse visual cues", "traditional document understanding models", "form structures", "document versions"], "background": "1. The complexity of form document understanding due to the involvement of two distinct authors (form designers and users) and the integration of diverse visual cues. 2. The limitations of traditional document understanding models in accounting for the diverse carriers of document versions and their associated noises, which exacerbate challenges in understanding form structures and components."}
{"hash_id": 6578703959773314963, "entities": ["personabased conversational datasets", "ai chatbot interactions", "large language models automation"], "background": "1. The need for large, dynamic, and high-quality persona-based conversational datasets to improve the engagement and realism of interactions between AI chatbots and users. 2. The desire to reduce the labor-intensive process of creating and updating such datasets by automating the generation and refinement of conversations using Large Language Models."}
{"hash_id": 7124219250858467767, "entities": ["visionlanguage models", "task diversity", "annotation error reduction", "gpt instruction tuning"], "background": "1. The need to improve the generalizability of vision-language models by increasing task diversity in pretraining and visual instruction tuning. 2. The requirement to reduce annotation error and bias in GPT-4 synthesized instruction tuning data to prevent issues like hallucination and catastrophic forgetting."}
{"hash_id": 7008104791034456076, "entities": ["knowledge editing techniques", "language models", "factuality accuracy generalizability", "humanlike belief revision"], "background": "1. The need to ensure that knowledge editing techniques for language models can consistently update related facts and improve factuality, accuracy, and generalizability. 2. The goal to bridge the gap between human-like belief revision, which involves coherent changes across many beliefs, and current model editing methods that often fail to generalize consistently."}
{"hash_id": 2376413715187762121, "entities": ["language understanding", "physical world interaction", "embodied approach", "language acquisition", "human experience"], "background": "1. The need to bridge the gap between language understanding and the physical world, as current AI models are limited to the \"Internet world scope\" and lack interaction with the environment. 2. The potential to enhance language acquisition and understanding by incorporating an embodied approach, which mirrors the human experience of actively engaging with the environment to learn language."}
{"hash_id": 1638840635476912679, "entities": ["coreference resolution models", "multiple datasets", "evaluation factors"], "background": "1. The need to draw meaningful conclusions about the generalization capabilities of coreference resolution models when evaluated across multiple datasets. 2. The requirement to disentangle the factors affecting evaluation results, such as differences in the definition and operationalization of coreference, to better understand what is actually being measured."}
{"hash_id": 1282309957322701129, "entities": ["goaldriven actions", "complex narratives", "claim verification", "nuanced design"], "background": "1. The need to improve the understanding and reasoning of goal-driven actions in complex narratives, which is essential for tasks like claim verification and question answering. 2. The challenge of capturing the nuanced design and intent behind actions, especially when narratives contain slight variations that lead to different inferences, and the limitations of current large language models in this regard."}
{"hash_id": 8000788310309893346, "entities": ["onetomany problem", "opendomain dialogues", "domainspecific scenarios"], "background": "1. The limitations of existing evaluation metrics in handling the one-to-many problem in open-domain dialogues, where multiple reasonable responses are possible for a single context. 2. The need to improve the performance of evaluation metrics in domain-specific scenarios, considering the biases present in large language models."}
{"hash_id": 1531631673145044694, "entities": ["language models", "architectural features", "goodenough performance", "simplified structures"], "background": "1. To investigate how language models can achieve human-like efficiency in processing language, despite using heuristics, by identifying key architectural features that contribute to this \"good-enough\" performance. 2. To challenge the notion that reliance on heuristics is a flaw by showing that, like humans, models can still perform well with simplified structures."}
{"hash_id": 4053899568909441196, "entities": ["chinese spelling correction", "highquality corpora", "noisy data augmentation"], "background": "1. The lack of large-scale high-quality corpora for Chinese Spelling Correction (CSC) due to the labor-intensive process of labeling errors in real-life writing or typing scenarios. 2. The need to reduce over-correction and false positive predictions caused by noisy data introduced by common data augmentation methods."}
{"hash_id": 586190272994392441, "entities": ["multimodal models", "compositional reasoning", "semantic counterfactuals"], "background": "1. The current multimodal models like CLIP and LLaVA show near-chance performance in physically grounded compositional reasoning tasks, indicating a significant gap in their ability to understand positions and count objects. 2. There is a lack of utilization of advanced text and image generation models to create accurate and challenging semantic counterfactuals, which are crucial for enhancing compositional reasoning capabilities."}
{"hash_id": 3352856463487688809, "entities": ["privacy concerns", "infrastructure costs", "unified approach", "synthetic clinical text", "clinical nlp tasks"], "background": "1. The need to address privacy concerns and reduce infrastructure costs associated with using large language models (LLMs) directly on sensitive clinical text data. 2. The requirement for a unified approach to adapt LLMs for generating high-quality, diverse synthetic clinical text data that aligns with real-world datasets for various clinical NLP tasks."}
{"hash_id": 901826101512408617, "entities": ["expressive speechtospeech translation", "realworld translation scenarios", "speech expressivity"], "background": "1. Existing expressive speech-to-speech translation systems are vulnerable to noise, which is common in real-world translation scenarios. 2. It is crucial to preserve the expressivity of speech, such as style, emotion, or tone, during translation to achieve natural conversation."}
{"hash_id": 6621756285810059195, "entities": ["bridging legal assistance gap", "llm legal consultations", "userllm interactions", "legal knowledge deficiency"], "background": "1. To bridge the gap in legal assistance affordability and access, especially for individuals with modest means, by enhancing the capabilities of LLMs in providing accurate and useful legal consultations. 2. To improve the effectiveness of user-LLM interactions by addressing the lack of legal knowledge and query formulation skills of users without a legal background."}
{"hash_id": 79544118808637278, "entities": ["fake news detection", "explainability", "generalizability", "controllability"], "background": "1. The need for a trustworthy fake news detection system that can provide transparent reasoning processes, improve generalization across diverse data, and allow for human intervention. 2. The current lack of a comprehensive solution that simultaneously addresses explainability, generalizability, and controllability in the context of fake news detection."}
{"hash_id": 807319519828964004, "entities": ["information verification", "language model transparency", "contentsource connection", "model output reliability"], "background": "1. Enhancing the transparency and trustworthiness of large language models by allowing users to verify the accuracy of information at a finer granularity. 2. Improving the efficiency of users in assessing the connection between generated content and its supporting sources, thereby increasing the reliability of model outputs."}
{"hash_id": 6075312728577424118, "entities": ["personalized ai experiences", "tailored daily routines", "multiagent editing architectures"], "background": "1. The need for personalized and tailored experiences in AI-assisted daily routines, such as customizing how-to procedures to fit users' specific requirements. 2. The potential to improve the effectiveness of Large Language Models (LLMs) in procedure customization by exploring multi-agent editing architectures."}
{"hash_id": 5481651330940057344, "entities": ["transformer architecture", "causal language modeling", "metalearning principles"], "background": "1. The need to explain the inner workings and capabilities of the Transformer architecture, particularly in the context of causal language modeling, which is crucial for advancing AI in natural language processing and other fields. 2. The potential to enhance the understanding and performance of large language models by leveraging meta-learning principles to uncover the optimization processes within Transformers."}
{"hash_id": 7200320681727590669, "entities": ["large language models", "resourceconstrained environments", "knowledge distillation techniques"], "background": "1. The need to make large language models (LLMs) applicable in resource-constrained environments by reducing their size and computational requirements without significant performance loss. 2. The challenge of traditional knowledge distillation techniques not being effective for LLMs due to the lack of access to teacher model outputs, large capacity gaps between teacher and student, and the issue of mis-calibration."}
{"hash_id": 5762197328165075541, "entities": ["selfcorrection abilities", "small language models", "reasoning performance", "highstakes domains"], "background": "1. The need to improve self-correction abilities in small, open-source language models without relying on large, proprietary models or human annotation. 2. The desire to enhance the reasoning performance of small language models by enabling them to self-correct errors, which is critical for high-stakes domains and scientific understanding."}
{"hash_id": 779521394470318203, "entities": ["mitigating biases", "large language models", "marginalized populations", "safety safeguards", "qualityofservice harms"], "background": "1. The need to identify and mitigate biases in large language models that can lead to harm, particularly for marginalized populations, as these models are increasingly used in high-stakes applications. 2. The recognition that current safety safeguards may not be effective in truly eliminating biases and could instead mask them, leading to quality-of-service harms."}
{"hash_id": 3494623701802658549, "entities": ["decisionmaking llms", "chinese societal applications", "reallife api interactions", "comprehensive benchmark", "evaluation framework"], "background": "1. The need to assess the decision-making capabilities of LLMs as agents in real-world scenarios, particularly in the context of Chinese language and societal applications, to understand their performance and limitations. 2. The absence of a comprehensive benchmark and evaluation framework that accurately simulates real-life API interactions, which hinders the progress of LLMs toward acting as reliable agents in complex tasks."}
{"hash_id": 5960500811930563465, "entities": ["generative models", "tokenization issues", "partial tokens", "subword scenarios"], "background": "1. Generative models frequently produce incorrect or nonsensical outputs when dealing with partial tokens due to tokenization issues, which can significantly impact their performance in various applications. 2. There is a need for a method that can maintain or improve the performance of generative models on complete contexts while also effectively handling partial inputs or subword scenarios."}
{"hash_id": 1390110124336612752, "entities": ["human annotation benchmarks", "multilingual summarization", "lowresource languages"], "background": "1. The scarcity of human annotation benchmarks for multilingual summarization meta-evaluation hinders the evaluation of automatic metrics in non-English languages. 2. The need to avoid the difficulty and resource-intensiveness of collecting human annotations for each language, especially for low-resource languages."}
{"hash_id": 6448993996386040358, "entities": ["universal speech emotion model", "semantic information", "computational costs", "ssl models"], "background": "1. The need for a universal speech emotion representation model that can capture rich semantic information for various emotional tasks, surpassing the limited performance of traditional speech features. 2. The requirement to reduce computational costs and avoid data-specific or model-constrained solutions associated with fine-tuning existing self-supervised learning (SSL) models for emotion recognition."}
{"hash_id": 442466745464582204, "entities": ["computational efficiency", "large language models", "lowrank adaptation", "model accuracy", "quantization"], "background": "1. The need to improve computational efficiency and reduce memory demands when deploying large language models (LLMs) for specific applications with limited resources. 2. The challenge of maintaining model accuracy while combining quantization with Low-Rank Adaptation (LoRA) techniques, especially in aggressive low-bit scenarios."}
{"hash_id": 5085158114741001962, "entities": ["information transmission", "user clusters", "realworld impact", "information spread prediction"], "background": "1. The need to understand how information is transmitted among communities of users on social networks to gauge the impact on real-world events. 2. The challenge of identifying and characterizing user clusters, which hinders the prediction of information spread."}
{"hash_id": 3952044042529011284, "entities": ["textbased reinforcement learning", "interactive fiction games", "skill transfer", "game generation approach"], "background": "1. The need to improve the generalization capabilities of text-based reinforcement learning agents in interactive fiction games, as current environments do not effectively allow agents to transfer learned skills to new situations. 2. The requirement for an efficient approach to generate a large number of text-based games to train agents without relying on time-consuming manual processes."}
{"hash_id": 6350442700672160589, "entities": ["codellms performance", "code completion", "user expectations alignment"], "background": "1. The need to improve the performance of large language models of code (CodeLLMs) in completing partial code with potential bugs, as previous work has shown shortcomings in this area. 2. The desire to align the generated code more closely with user expectations by preserving the original structure and naming conventions of the partial code."}
{"hash_id": 5236753608365352891, "entities": ["incontext learning", "large visual language models", "visual reasoning tasks", "crossmodal interactions", "representation spaces disparities"], "background": "1. The need to enhance the In-Context Learning (ICL) capabilities of Large Visual Language Models (LVLMs) to perform better in visual reasoning tasks. 2. The desire to overcome the specific challenges faced by LVLMs, such as the difficulty in cross-modal interactions and the disparities in representation spaces between visual features and language embeddings."}
{"hash_id": 9188502381834604938, "entities": ["language bias", "parallel data bias", "machine translation", "llm specialty", "continual learning"], "background": "1. To mitigate the language bias of LLMs and the parallel data bias of STMs in machine translation. 2. To enhance the speciality of LLMs without sacrificing generality and to facilitate continual learning without the high cost of LLM tuning."}
{"hash_id": 637218093630268547, "entities": ["zeroshot retrieval", "large language models", "retrieval quality"], "background": "1. The need to improve the performance of zero-shot retrieval, which is crucial for information-seeking tasks, without relying on labor-intensive annotations. 2. The desire to leverage the power of large language models (LLMs) to enhance retrieval quality while avoiding the issue of spurious and out-of-domain answers caused by ambiguous queries."}
{"hash_id": 4019133692650057844, "entities": ["manual factchecking", "automatic factchecking", "news outlet profiling", "fake news detection"], "background": "1. The manual and automatic fact-checking of every suspicious claim or article is unfeasible due to the vast amount of content and the time required to verify the information. 2. Profiling entire news outlets allows for quick identification of potential \"fake news\" based on the reliability of their source, which can be used for distant supervision in \"fake news\" detection tasks."}
{"hash_id": 9117329441019882711, "entities": ["embedding compression", "semantic quality", "lowresource settings", "nlp efficiency", "dimensionality reduction"], "background": "1. The need to compress word and sentence embeddings while maintaining their semantic quality for use in low-resource settings. 2. The potential to enhance the efficiency of NLP applications by reducing the dimensionality of embeddings without compromising performance."}
{"hash_id": 8962620470972948721, "entities": ["jailbreaking attacks", "model robustness", "benign prompts", "adversarial manipulation"], "background": "1. The need to defend large language models against jailbreaking attacks, where harmful prompts are concealed through adversarial manipulation, making existing models vulnerable. 2. The requirement for a method that enhances model robustness without compromising generation quality for benign prompts and without the need for additional training."}
{"hash_id": 1464207694977457842, "entities": ["contradictory responses", "datadriven contradiction suppression", "training data scarcity"], "background": "1. The lack of a large-scale collection of RGM-generated contradictory responses hinders the understanding of their characteristics and the development of effective data-driven contradiction suppression methods. 2. Existing data-driven approaches are limited by the scarcity of training data and the discrepancy between the types of contradictions generated by models and those used for training."}
{"hash_id": 4838740192195924262, "entities": ["large language models", "critiquerefine framework", "text simplification", "indian languages", "crosslingual transfer"], "background": "1. The high inference cost of using large language models in the critique-refine framework for text simplification. 2. The lack of existing work on text simplification for Indian languages and the potential for cross-lingual transfer."}
{"hash_id": 5111538803268925234, "entities": ["retrievalaugmented models", "inference latency", "scalability", "knowledge sources"], "background": "1. The need to improve the efficiency of retrieval-augmented models by reducing inference latency, which is currently hindered by the long context length of retrieved knowledge. 2. The desire to enhance the scalability of these models by broadening the range of knowledge sources they can utilize, moving beyond the limitations of a single type of knowledge source."}
{"hash_id": 6950701163565274397, "entities": ["social bias quantification", "llms bias evaluation", "multidimensional bias"], "background": "1. The need for a more direct and comprehensive quantification of social bias in LLMs, considering the composite of social perceptions from diverse perspectives among identities. 2. The recognition that existing methods for evaluating bias in LLMs do not fully capture the multi-dimensional aspects of social bias, often relying on indirect assessments or fixed stereotypes."}
{"hash_id": 4617201542921116313, "entities": ["highorder historical information", "temporal knowledge graph forecasting", "large language models"], "background": "1. To improve the semantic comprehension abilities of graph-based models in Temporal Knowledge Graph forecasting by incorporating high-order historical information. 2. To enhance the reasoning performance of Large Language Models under heavy historical information loads and to combine their strengths with graph-based models."}
{"hash_id": 1147838389785219425, "entities": ["inclusive environments", "diverse groups", "userdefined stance", "controllability in llms"], "background": "1. The need to create more inclusive environments by allowing LLMs to speak for diverse, especially minority, groups and generate statements supporting their perspectives. 2. The lack of sufficient controllability in existing LLMs to consistently generate statements aligned with a user-defined stance, often leading to inconsistent, neutral, or biased content."}
{"hash_id": 4830453133037040431, "entities": ["contrastive learning", "hard negative samples", "implicit hate speech detection", "identity term bias"], "background": "1. Existing contrastive learning approaches do not effectively learn from hard negative samples, leading to limited improvements over traditional cross-entropy loss-based learning for implicit hate speech detection. 2. The need to address the challenges of implicit hate speech detection, such as identity term bias, which causes false positives and makes it difficult to distinguish between innocent and hateful content."}
{"hash_id": 2940912398721404710, "entities": ["instructiontuning data quality", "student model compatibility", "selective integration mechanism"], "background": "1. The need to improve the quality of instruction-tuning data for LLMs, especially in terms of compatibility with the student model being fine-tuned. 2. The absence of a mechanism for the student model to selectively integrate improvements in instruction and response data, accounting for the inherent randomness and potential degradation in generative models' output."}
{"hash_id": 7988682279073996438, "entities": ["personalization of dialogues", "persona profiles", "response diversity", "current approaches limitations"], "background": "1. The need to improve the personalization of dialogues with persona profiles in large language models without sacrificing response diversity. 2. The challenge of overcoming the limitations of current approaches, such as textual prompting and direct fine-tuning, which often result in responses that are either not aligned with ground truths or too repetitive and generic."}
{"hash_id": 5267754469739576087, "entities": ["large language models", "safety robustness", "ethical integrity", "unpredictable behaviors"], "background": "1. The need to ensure the safety and robustness of large language models (LLMs) as they advance and become more integrated into various applications. 2. The requirement to balance the accuracy of LLMs with their ethical integrity, especially when making modifications that could lead to unpredictable and potentially unsafe behaviors."}
{"hash_id": 8653040871387798821, "entities": ["visionlanguage models", "contextual cues", "image retrieval", "ircd task"], "background": "1. Current vision-language models (VLMs) struggle with aligning key contextual cues in both images and complex linguistic descriptions, leading to poor performance in image retrieval from linguistically complex descriptions. 2. There is a need for a method that can capture long-range dependencies among candidate images and perform fine-grained alignment to address the challenges of the IRCD task."}
{"hash_id": 917592747112965302, "entities": ["multiagent collaboration", "dynamic environments", "scalable solution"], "background": "1. The need to improve multi-agent collaboration in dynamic environments with complex dependencies, such as spatial, causal, and temporal constraints. 2. The requirement for a scalable and generalizable solution that can handle diverse tasks and adapt to changing conditions, which existing models do not fully address."}
{"hash_id": 3923257382337753353, "entities": ["attentiondriven compositors", "imagetext correlations", "retrieval performance", "computational efficiency"], "background": "1. The limitations of traditional attention-driven compositors in capturing correlations between image and text attributes, leading to suboptimal retrieval performance. 2. The need for improved computational efficiency and scalability in text-conditioned image retrieval systems."}
{"hash_id": 1760521070938140993, "entities": ["tokenlevel context", "utterancelevel discourse", "encoding interactions"], "background": "1. The need to improve the understanding of token-level context by incorporating interactions among different utterances in the encoding stage. 2. The recognition that discourse information is naturally organized at the utterance level, and learning it solely at the token level is insufficient for optimal results."}
{"hash_id": 2632850793816537604, "entities": ["decoderonly models", "encoderonly models", "semantic understanding tasks"], "background": "1. The prevalent assumption that decoder-only large language models always perform better than encoder-only models, especially in understanding word meaning, needs empirical validation. 2. Previous research by Zhu et al. (2024) indicating that decoder-only language models struggle with understanding nuanced contextual features, which motivates a direct comparison in semantic understanding tasks."}
{"hash_id": 797515119550755663, "entities": ["long text management", "structural connections", "hierarchical memory", "inference capability"], "background": "1. Existing approaches in managing long text in LLMs fail to consider the structural connections between text fragments, leading to limited capability in handling texts with intensive inter-relations. 2. The need to improve the inference capability of Hierarchical Memory based LLMs, especially in scenarios like understanding coherent stories or code repositories, where there are strong associations across fragments of long text."}
{"hash_id": 8971505733805853158, "entities": ["docre models", "entity name variations", "robustness enhancement"], "background": "1. Existing DocRE models do not perform well when entity names vary, which hinders their generalization to new or unseen entity names. 2. There is a need to enhance the robustness and understanding/reasoning capabilities of DocRE models to handle realistic scenarios with entity name variations."}
{"hash_id": 8336046340859417900, "entities": ["social media interactions", "emotion analysis", "responsive emotions"], "background": "1. The lack of datasets and models that can effectively capture and analyze the implicit and complex responsive relationships in social media interactions, which are crucial for understanding human emotions and online behavior. 2. The need to advance the field of emotion analysis by providing a resource that explicitly annotates responsive emotions, their causes, and relationships in a social media context."}
{"hash_id": 184678825789054425, "entities": ["texttosql models", "ehr databases", "interactivity", "compositionality", "efficiency"], "background": "1. The need to enhance the practical application of text-to-SQL models for EHR databases by incorporating interactivity, compositionality, and efficiency. 2. The gap in existing text-to-SQL datasets that fail to capture the complexity and seqential nature of real-world medical queries in EHRs."}
{"hash_id": 5459707326027122897, "entities": ["user engagement", "conversational agents", "engaging dialogues"], "background": "1. The need to increase users' willingness to engage in ongoing conversations with conversational agents, as current dialogues often lack the depth and engagement required for long-term user interest. 2. The absence of a dataset that teaches agents to generate more engaging dialogues while incorporating detailed personas and diverse language styles."}
{"hash_id": 8637855126398977430, "entities": ["automated program repair", "smallscale language models", "financial burden", "manual repairs"], "background": "1. The need to improve the performance of Automated Program Repair (APR) for small-scale language models, which struggle with making effective single-step modifications and lack feedback from compilers and test cases. 2. To reduce the financial burden of manual repairs and enhance program reliability without requiring large-scale language models."}
{"hash_id": 8754236558786560431, "entities": ["input window occupation", "decoding process", "tool documentation compression"], "background": "1. The need to reduce the occupation of input window and slow down of decoding process caused by lengthy tool documentation in tool-using language models. 2. The potential for compression in tool documentation due to its fixed nature once deployed and the ability to summarize most parts while retaining key information."}
{"hash_id": 3665935642261307794, "entities": ["dataset for medical decisions", "clinical narratives", "clinical decisionmaking"], "background": "1. The lack of a dataset specifically designed for the extraction and classification of medical decisions within clinical narratives hinders the development of automated systems that could improve clinical practice and health policy. 2. The need to understand the complexities of clinical decision-making processes to inform evidence-based guidelines and identify potential risks to patients."}
{"hash_id": 3702101090436811575, "entities": ["textimage retrieval models", "generalizability", "benchmark dataset"], "background": "1. The need to challenge and improve the generalizability of state-of-the-art text-image retrieval models on more complex and diverse data. 2. The requirement for a benchmark dataset that better represents real-world scenarios and can serve as a more rigorous evaluation for text-image retrieval approaches."}
{"hash_id": 5183059524392536092, "entities": ["highquality multilingual imagetext data", "modular visionllm methods", "image encoding languageagnostic"], "background": "1. The need to overcome the high computational cost and limited availability of high-quality multilingual image-text data for pretraining vision-language models. 2. The recognition that modular Vision-LLM methods can be more efficient and that image encoding is language-agnostic, which suggests that an image encoder aligned with English data could potentially be aligned with a multilingual LLM."}
{"hash_id": 548183307811864999, "entities": ["label cooccurrence", "sample distribution balancing", "head tail class performance"], "background": "1. The need to capture semantic feature interactions between classes to address label co-occurrence, which is often ignored by existing methods that focus on sample distribution balancing. 2. The requirement for synchronous improvements in performance on both head and tail classes, as some methods may enhance tail classes at the expense of head classes."}
{"hash_id": 2913292549005881756, "entities": ["object hallucination", "visionlanguage models", "unfaithful responses"], "background": "1. The lack of a general measurement for evaluating object hallucination in vision-language models, which hinders our understanding and ability to mitigate this issue. 2. The need to address the persistent issue of object hallucination in VL models, which leads to the generation of unfaithful responses and highlights underlying problems such as over-reliance on unimodal priors and statistical bias."}
{"hash_id": 6171435373737628549, "entities": ["understanding internal processes", "multimodal understanding", "model interpretability", "processing negation"], "background": "1. The need to understand the internal processes of vision & language models, specifically how they handle linguistic constructs like negation, which are crucial for accurate multimodal understanding. 2. The desire to improve the interpretability of multimodal models by identifying and localizing the model components responsible for processing negation, thereby revealing the strengths and weaknesses of current models."}
{"hash_id": 7976842950950186684, "entities": ["multimodal input", "continuous learning", "catastrophic forgetting", "modality representation", "learning effect"], "background": "1. The need to address the\u5dee\u5f02\u6027\u7684 multimodal \u8f93\u5165\u5728\u8fde\u7eed\u5b66\u4e60\u8fc7\u7a0b\u4e2d\u7684\u8868\u73b0\uff0c\u4ee5\u53ca\u5b83\u4eec\u5bf9\u5b66\u4e60\u52a8\u6001\u7684\u5f71\u54cd\uff0c\u4ee5\u9632\u6b62\u5728\u65b0\u4efb\u52a1\u5b66\u4e60\u65f6\u65e7\u77e5\u8bc6\u7684\u707e\u96be\u6027\u9057\u5fd8\u3002 2. \u73b0\u6709\u65b9\u6cd5\u5728\u89c6\u89c9\u4e0e\u8bed\u8a00\u4efb\u52a1\u7684\u8fde\u7eed\u5b66\u4e60\u4e2d\u5e76\u672a\u5145\u5206\u8003\u8651\u4e0d\u540c\u6a21\u6001\u5728\u8868\u793a\u7a7a\u95f4\u4e2d\u7684\u5dee\u5f02\u6027\uff0c\u4ee5\u53ca\u8fd9\u4e9b\u5dee\u5f02\u6027\u5bf9\u5b66\u4e60\u6548\u679c\u7684\u5f71\u54cd\u3002"}
{"hash_id": 6572269272486541304, "entities": ["error correction", "speech recognition", "machine translation", "image utilization", "translation quality"], "background": "1. The need to correct errors in automatic speech recognition and disambiguate text in machine translation. 2. The potential to improve translation quality by utilizing relevant images from lecture videos that complement the audio information."}
{"hash_id": 720669540326610083, "entities": ["video cot datasets", "reasoning ability", "manual annotation inefficiency"], "background": "1. The current lack of video CoT datasets and the need to improve the reasoning ability of MLLMs in the video domain. 2. The high cost and inefficiency of fully manual annotation of CoTs for video datasets."}
{"hash_id": 7052387441835711169, "entities": ["finegrained conceptual understanding", "contrastive learning", "robust benchmark", "visualtextual alignment"], "background": "1. The need to enhance fine-grained conceptual understanding in vision-language models, which is currently limited by the use of random and too dissimilar negative samples in contrastive learning. 2. The requirement for a more robust benchmark to evaluate the model's ability to validate its conceptual understanding, particularly in terms of fine-grained visual and textual alignment."}
{"hash_id": 377039311616924795, "entities": ["image comprehension", "avsr models", "technical terminology transcription"], "background": "1. The need to evaluate AVSR models' image comprehension capabilities in broader contexts beyond facial features, as existing datasets mainly focus on lip-reading. 2. The challenge of transcribing technical terminologies commonly found in scientific paper explanations without the aid of reference texts."}
{"hash_id": 3378296316801904324, "entities": ["inferential question generation", "human thinking", "causal temporal relationships", "computational demands", "pretraining models"], "background": "1. The lack of existing studies exploring inferential question generation aligned with human thinking, particularly in capturing causal and temporal relationships in visual content. 2. The high computational demands and resource intensiveness of pre-training models from scratch, which hinder the efficiency and scalability of Visual Question Generation."}
{"hash_id": 1774726685507683599, "entities": ["zeroshot crosslingual transfer", "lowresource languages", "languageagnostic semantic knowledge"], "background": "1. The need to improve zero-shot cross-lingual transfer performance, especially for low-resource languages, where existing vision-language models struggle to match their English performance. 2. The recognition that higher layers of multilingual models encode more language-agnostic semantic knowledge, which is crucial for transferring knowledge across languages."}
{"hash_id": 8368724385808200433, "entities": ["userfriendly design process", "nonprofessional users", "visual elements arrangement", "design automation", "expertise requirement"], "background": "1. The need for a more user-friendly and efficient design process for nonprofessional users who lack the skills and resources to create visually appealing layouts. 2. The potential to automate and simplify the arrangement of visual elements in various design tasks, which is currently time-consuming and requires expertise."}
{"hash_id": 861206970290767099, "entities": ["cultural specificities", "vqa datasets", "southeast asia nuances"], "background": "1. The need to capture cultural specificities in VQA datasets to improve the robustness and generalizability of VQA systems in diverse cultural and linguistic settings. 2. Addressing the gap in existing VQA datasets that often overlook the nuances and contextual knowledge unique to different cultures, particularly in Southeast Asia."}
{"hash_id": 4067807145482475343, "entities": ["structured data", "wikimedia commons", "image repository", "content management", "search algorithms"], "background": "1. Enhancing the usability and utility of Wikimedia Commons by unlocking the full potential of its image repository through structured data. 2. Facilitating automation and innovation in content management and analysis, such as search algorithms and machine learning models, by providing structured data annotations for images."}
{"hash_id": 8666877187705577403, "entities": ["arabic rumor verification", "social media disruption", "verification datasets"], "background": "1. The spread of rumors and fake news on social media, particularly in Arabic, causes significant social disruption and requires effective verification methods. 2. Existing studies on Arabic rumor verification incorporating evidence, especially from authorities, are scarce, and there is a lack of datasets to support research in this area."}
{"hash_id": 79826901436306092, "entities": ["dialectal arabic", "nlp applications", "speech recognition"], "background": "1. The increasing use of Dialectal Arabic in written form on social media, which lacks standard orthography, poses significant challenges for NLP applications due to the high degree of noise and data sparsity. 2. The need to improve the performance of NLP systems when the dialect itself is the desired output, such as in automatic speech recognition, and to facilitate evaluation and optimization of these systems."}
{"hash_id": 5563553566266265994, "entities": ["automatic readability assessment", "arabic nlp applications", "modeling approaches"], "background": "1. The need for automatic readability assessment in Arabic to support NLP applications for education, content analysis, and accessibility, which is currently hindered by Arabic's morphological richness and limited readability resources. 2. To bridge the gap in research attention between English readability assessment and other languages like Arabic, and to provide a systematic exploration of modeling approaches for Arabic readability."}
{"hash_id": 7114595416575438228, "entities": ["lowresource language models", "cultural biases", "costeffective solutions"], "background": "1. The need to improve language models trained on low-resource languages, which often rely on translated data that can introduce cultural biases and reduce data quality. 2. The desire to find cost-effective solutions that do not require large amounts of high-quality translated content, which is expensive and time-consuming to produce."}
{"hash_id": 6347953078710858336, "entities": ["arabic texttoimage retrieval", "unique language challenges", "crosslingual accuracy"], "background": "1. The lack of dedicated text-to-image retrieval models for the Arabic language, despite a significant number of Arabic speakers worldwide and the unique challenges posed by Arabic's morphology, grammar, and dialects. 2. The need to capture the nuances and complexities of the Arabic language to improve the accuracy of image-text retrieval in a cross-lingual context."}
{"hash_id": 7575155375812177225, "entities": ["machine translations", "domainspecific scenarios", "technical translations", "arabic to english", "french performance"], "background": "1. The need to improve the accuracy and reliability of machine translations, especially in domain-specific scenarios like legal translation, where terminology and context are crucial. 2. The recognition that while LLMs have made significant strides in general language understanding, their performance in technical and domain-specific translations, particularly from Arabic to English and French, is lacking and requires targeted improvement."}
{"hash_id": 492968174502904420, "entities": ["zeroshot multispeaker tts", "arabic", "multidialect settings"], "background": "1. The significant potential for improvements in zero-shot multi-speaker TTS for Arabic, a language with over 450 million native speakers, to enhance the quality and naturalness of synthesized speech in diverse applications. 2. The lack of public research for Arabic ZS-TTS, particularly in multidialect settings, which hinders the development of TTS technology for Arabic speakers."}
{"hash_id": 766810983202695297, "entities": ["lowresource languages", "tunisian arabic dialect", "selfsupervised learning", "ssl techniques"], "background": "1. The need to improve ASR and SLU performance in low-resource languages, particularly Tunisian Arabic dialect, where annotated data is scarce. 2. The potential to leverage self-supervised learning (SSL) techniques that have shown success in other languages and domains to enhance performance in the specific context of Tunisian Arabic."}
{"hash_id": 518648598160183839, "entities": ["arabic short story data", "story generation", "cultural preservation"], "background": "1. The scarcity of Arabic short story data and minimal focus from the research community on Arabic story generation hinders the development of such technologies in the Arabic language. 2. Storytelling is an essential skill with wide-ranging benefits in education, entertainment, and cultural preservation, and automatic story generation can enhance these areas by creating diverse and tailored narratives."}
{"hash_id": 6729185489334135612, "entities": ["arabicspecific plms", "dialectal variations", "limited training data"], "background": "1. The need to improve the performance of Arabic-specific PLMs on dialectal variations of Arabic, which are underrepresented in existing models. 2. The requirement to develop a model that can achieve high performance with limited training data and computational resources."}
{"hash_id": 1001277938281680272, "entities": ["lack of syntactically annotated corpora", "dialectal arabic parsers", "parsing performance drop"], "background": "1. The lack of syntactically annotated corpora for dialectal Arabic, which hinders the development of accurate parsers for these dialects. 2. The significant drop in parsing performance due to domain differences between the available Modern Standard Arabic (MSA) treebanks and the dialectal Arabic used in informal communication."}
{"hash_id": 980388106357179671, "entities": ["bias in language models", "nonenglish languages", "debateinduced prompting"], "background": "1. The lack of comprehensive research on bias in large language models for non-English languages, particularly Arabic and Russian, which are widely spoken and have unique cultural and political contexts. 2. The need to explore new methodologies, such as debate-induced prompting, to uncover and understand deeply rooted biases in language models that may not be evident through traditional evaluation methods."}
{"hash_id": 743013981399485948, "entities": ["arabic legal texts", "large language models", "legal language complexity"], "background": "1. The need to assess the proficiency of large language models in understanding and processing Arabic legal texts, which is crucial due to the complexity and richness of legal language, especially in Arabic. 2. The desire to find the current state of LLMs in the Arabic Legal domain and guide model development for improved performance in this specialized field."}
{"hash_id": 4631360171045574575, "entities": ["arabic nlp tasks", "annotated datasets", "synthetic data", "language models"], "background": "1. The scarcity of large, annotated datasets for Arabic NLP tasks hinders the development of strong language models, which is crucial for improving performance in low-resource settings. 2. The potential of synthetic data to serve as a cost-effective and accessible alternative to extensive gold-standard datasets, especially when generated from diverse sources, has not been fully explored in Arabic language modeling."}
{"hash_id": 441393270683755893, "entities": ["opensource large language models", "arabic language", "comprehensive benchmark"], "background": "1. To bridge the gap in knowledge about the performance of open-source large language models like LLaMA-3-70B in generating Arabic language, which has rich morphology and a wide range of dialects. 2. To motivate the development of stronger open-sourced Arabic language models by providing a comprehensive benchmark and performance comparison with closed-source models and smaller, dedicated Arabic models."}
{"hash_id": 5285546592587410439, "entities": ["preservation digital presence", "coptic language", "large language models", "low representation languages"], "background": "1. The need to preserve and increase the digital presence of the Coptic language, which is a vital part of cultural heritage with millions of speakers but limited digital resources. 2. The gap in performance of Large Language Models (LLMs) for languages with low representation, which necessitates the development of new methods to enable these models to handle Coptic text."}
{"hash_id": 5807087545858905110, "entities": ["arabic eventargument extraction", "annotated corpora", "natural language understanding systems"], "background": "1. The lack of comprehensive, annotated corpora for event-argument extraction in Arabic, which is crucial for various applications like disaster monitoring and information retrieval. 2. The need to improve the performance of event-argument extraction methods to enhance the utility of natural language understanding systems in under-resourced languages."}
{"hash_id": 8226306618072911118, "entities": ["multimodal language model", "arabic dialects", "dialectal variations", "visual data integration"], "background": "1. The need to bridge the gap in multimodal language model advancements, which are predominantly limited to English, and extend them to Arabic to enhance user interaction and preserve the linguistic diversity of Arabic dialects. 2. The requirement to address the unique challenges posed by Arabic's linguistically diverse environment, particularly in processing dialectal variations and integrating them with visual data."}
{"hash_id": 260688005596973325, "entities": ["arabic nlu research", "linguistic complexity", "wsd", "lmd", "nlp applications"], "background": "1. The lack of focus on Arabic NLU research, despite its importance in NLP, due to the scarcity of datasets for essential tasks like WSD and LMD. 2. The need to address the linguistic complexity and ambiguity in Arabic, which impacts the accuracy and relevance of results in various NLP applications."}
{"hash_id": 4282331123567857699, "entities": ["arabic morphology", "word sense disambiguation", "location mention detection", "advanced methods"], "background": "1. The complexity of Arabic morphology, which includes a rich interplay of roots, stems, and affixes, makes word sense disambiguation and location mention detection particularly challenging, necessitating the need for advanced methods. 2. The lack of training data for the Word Sense Disambiguation (WSD) task and the specific requirements of the Location Mention Detection (LMD) task in terms of precision and efficiency drive the proposal of these methods."}
{"hash_id": 475198001069750301, "entities": ["arabic nlp tools", "financial markets", "banking sector"], "background": "1. The rapid growth of financial markets in the Arab world demands sophisticated Arabic NLP tools to enhance decision-making and customer service in the banking sector. 2. The majority of financial NLP research is conducted in English, neglecting the linguistic diversity and specific needs of Arabic-speaking regions."}
{"hash_id": 8073076307959819803, "entities": ["specialized financial arabic nlp models", "linguistic nuances", "modern standard arabic", "arabic dialects"], "background": "1. The robust growth in Middle Eastern stock markets has increased the demand for specialized financial Arabic NLP models, yet there is a lack of validated, robust models that can handle the linguistic nuances of diverse Arabic dialects. 2. The complexity of linguistic features in Modern Standard Arabic (MSA) and over 27 Arabic dialects poses challenges for existing NLP tools, necessitating the development of advanced methods to support these languages effectively."}
{"hash_id": 6281608565044780411, "entities": ["advanced nlp models", "multidialectal arabic", "intent detection", "financial sector"], "background": "1. The lack of advanced NLP models capable of handling multi-dialectal variations in Arabic, particularly in the financial sector where intent detection is crucial. 2. The need to bridge the gap in significant studies addressing intent detection in the financial sector for languages with limited resources like Arabic."}
{"hash_id": 6226845688197265653, "entities": ["arabic language resources", "intent detection systems", "online banking", "user command classification"], "background": "1. The scarcity of Arabic language and dialect resources hinders the development of robust intent detection systems, which is crucial for enhancing customer service in the banking domain. 2. The increasing use of online banking in Arabic-speaking countries necessitates accurate classification of user commands across multiple dialects to improve service efficiency."}
{"hash_id": 1025157706181110423, "entities": ["arabic datasets", "crossdialectal intent detection", "conversational systems"], "background": "1. The limited availability of Arabic datasets, especially in the banking sector, which hinders the development of effective cross-dialectal intent detection systems. 2. The need to improve the generalization of models to diverse Arabic dialects to enhance the performance of intent detection in task-oriented conversational systems."}
{"hash_id": 2100461346677180455, "entities": ["advanced nlp tools", "financial sector", "middle eastern stock markets", "customer service automation", "arabic dialects"], "background": "1. The increasing importance of advanced NLP tools tailored for the financial sector in the Arab world due to the substantial growth of Middle Eastern stock markets and the need for systems that can handle unique linguistic and cultural nuances. 2. The necessity to enhance customer service and automate query handling in financial institutions by interpreting complex and varied banking data across different Arabic dialects."}
{"hash_id": 2213772675922177801, "entities": ["propagandistic content", "arabic memes", "detection methods", "disinformation detection"], "background": "1. The increasing spread of propagandistic content, especially in Arabic memes, which combine text and images, necessitates the development of robust detection methods to combat disinformation on social media. 2. Previous research has primarily focused on English-language content, leaving a gap in the detection of propaganda in Arabic multimodal content, despite its growing prevalence."}
{"hash_id": 4979533278283174282, "entities": ["arabic social media", "multimodal memes", "propagandistic content", "detection tools", "communication medium"], "background": "1. The increasing use of propaganda in Arabic social media, specifically through multimodal memes, which requires an automated system to detect and classify such content. 2. The lack of research and tools focused on detecting propagandistic content in Arabic multimodal formats, particularly memes, which represent a significant communication medium in social media."}
{"hash_id": 8955983342178404812, "entities": ["propaganda detection", "annotated datasets", "arabic language complexity", "partofspeech tagging", "syntactic analysis"], "background": "1. The lack of annotated datasets for propaganda detection in Arabic, which hinders the training and evaluation of machine learning models for this task. 2. The complexity of Arabic language, with its rich morphology and diverse dialects, which adds layers of difficulty to tasks like part-of-speech tagging and syntactic analysis, essential for identifying propaganda techniques."}
{"hash_id": 9135800268699277829, "entities": ["propagandistic memes", "classification systems", "multimodal propaganda detection"], "background": "1. The rise in propagandistic memes that combine text and images, which can subtly sway audience opinions and propagate misinformation, necessitates the development of effective classification systems to detect such content. 2. Previous research has primarily focused on textual content or persuasion techniques, with a lack of attention to multimodal propaganda detection, especially within Arabic memes."}
{"hash_id": 7544128312209962169, "entities": ["memebased propaganda", "social media influence", "detection difficulty"], "background": "1. The increasing use of memes as a means to spread propaganda, which can influence opinions and behaviors, especially on social media platforms. 2. The difficulty in detecting propaganda in memes due to their persuasive nature and the extensive knowledge required for machine learning models to understand the context."}
{"hash_id": 3181819146680476207, "entities": ["propagandistic content detection", "democratic processes safeguarding", "informed decisionmaking promotion"], "background": "1. Safeguarding democratic processes and fostering a media environment characterized by integrity and transparency through the detection of propagandistic content. 2. Improving the ability to counteract the influence of propaganda and promoting informed decision-making by both individuals and organizations."}
{"hash_id": 9017122989492263569, "entities": ["propaganda manipulation", "political climates", "detection systems", "imbalanced datasets"], "background": "1. The need to combat the manipulation of public opinion through propaganda, especially in today's polarized political and ideological climates. 2. The desire to improve the performance of propaganda detection systems on imbalanced datasets, which can lead to biased predictions and ignore less frequent but important propaganda techniques."}
{"hash_id": 5626786075457262826, "entities": ["uncovering biases", "cooperative environment", "ai bias detection", "annotated dataset"], "background": "1. The need to uncover and address underlying biases and propaganda in news articles, especially concerning sensitive and politically charged issues like the Palestinian-Israeli conflict. 2. The requirement for a cooperative environment to create a thorough and annotated dataset that can be used to train AI models for bias detection in multiple languages and with multiple annotation labels."}
{"hash_id": 6301040877544197538, "entities": ["bias and propaganda detection", "annotation guidelines", "shared corpus", "nlp research"], "background": "1. The need to refine and improve annotation guidelines for bias and propaganda detection in news media to enhance the accuracy and reliability of datasets. 2. The goal to advance research in the field by creating a comprehensive shared corpus that fosters collaboration among NLP researchers."}
{"hash_id": 1141054255994088324, "entities": ["bias annotation framework", "gazaisrael war", "semiautomatic methods"], "background": "1. The need to develop a detailed and publicly available framework for annotating bias in news articles, particularly on sensitive topics like the Gaza-Israel war, to better understand media narratives. 2. The desire to improve the efficiency of bias annotation by incorporating semi-automatic methods, reducing manual effort and potentially increasing the scale at which bias can be analyzed."}
{"hash_id": 6566197505723441431, "entities": ["media representations", "war on gaza", "identifying bias"], "background": "1. The need to shed light on the layers of bias in media representations of the war on Gaza, which can significantly shape public perception. 2. The challenge of overcoming the subjective nature of identifying bias in news narratives and the lack of objective standards for doing so."}
{"hash_id": 3454206470316139054, "entities": ["social media bias", "public sentiment polarization", "misinformation detection"], "background": "1. The increasing concern over the proliferation of bias and propaganda on social media platforms, which can polarize public sentiment and erode democracies. 2. The need for collaborative research and advanced resources to effectively detect and counteract misinformation in multiple languages."}
{"hash_id": 1257475632041977787, "entities": ["singlelabel classification", "dialect identification", "overlapping dialects", "modern standard arabic msa"], "background": "1. The limitations of single-label classification in dialect identification, which often results in errors due to overlapping dialects and the inability to assign multiple dialects to a given text. 2. The need to account for the varying levels of dialectness in Arabic text, especially in relation to Modern Standard Arabic (MSA), to improve the accuracy of dialect processing tasks."}
{"hash_id": 7814825033832082869, "entities": ["arabic dialect classification", "multilabel training data", "traditional identification methods"], "background": "1. The lack of multi-label training data for Arabic dialect classification, which is crucial for accurate multi-label predictions. 2. The need to improve upon traditional dialect identification methods to handle the complexity of multi-label dialect identification tasks."}
{"hash_id": 7679263140870074471, "entities": ["dialectal arabic", "computational processing", "crossdialect translation"], "background": "1. The increasing use of Dialectal Arabic in digital communications highlights the need for improved computational processing to understand and bridge the gap with the formal structure of Modern Standard Arabic. 2. Previous research has been limited in scope, often focusing on single dialects and using limited datasets, which hinders the development of effective cross-dialect translation and dialectness estimation."}
{"hash_id": 6143136886598720849, "entities": ["dialect identification", "arabic dialects", "traditional machine learning techniques"], "background": "1. The need to improve the precision and recall of dialect identification in the diverse and understudied field of Arabic dialects, particularly when resources are limited. 2. The desire to leverage traditional machine learning techniques, given their simplicity and potential for competitive performance in the absence of groundbreaking new methods."}
{"hash_id": 7943331308863772221, "entities": ["lowresource languages", "translation performance", "computational costs"], "background": "1. The need to improve translation performance for low-resource languages, such as Arabic dialects, where limited training data poses a challenge for large language models. 2. The desire to reduce computational costs and resource requirements associated with fine-tuning large language models like LLaMA-3."}
{"hash_id": 8160684298316806089, "entities": ["arabic stance detection", "social media platforms", "decisionmaking processes"], "background": "1. The lack of research on stance detection in Arabic tweets, despite their growing importance in shaping public opinion on social media platforms. 2. The need for advanced technologies that can accurately detect and understand the viewpoints expressed in social media texts, which is crucial for decision-making processes in various sectors."}
{"hash_id": 6268537496632251543, "entities": ["arabic sentiment analysis", "social media", "language processing challenges", "class imbalance"], "background": "1. The rapid expansion of social media usage and the need for automated systems to analyze opinions and sentiments in Arabic textual data. 2. The unique challenges of Arabic language processing, informal writing styles on social media, and class imbalance in datasets."}
{"hash_id": 4001914598285655323, "entities": ["arabic stance detection", "nlp research gap", "arabic text processing complexity"], "background": "1. The lack of research on stance detection in Arabic language, despite being the fifth most spoken language, which creates a significant gap in NLP research. 2. The challenges posed by Arabic's unique scripting, morphology, and dialects, which increase the complexity of processing texts, and the limited availability of Arabic datasets for stance detection."}
{"hash_id": 5035772507268174440, "entities": ["arabic ner", "finegrained dataset", "nlp applications", "entity recognition"], "background": "1. To enrich Arabic NER research by providing a fine-grained dataset that allows for more detailed and specific entity recognition, which is crucial for various NLP applications. 2. To encourage the development and evaluation of diverse methods and techniques for Arabic NER, given the scarcity of resources and the need for improved performance in this domain."}
{"hash_id": 5513957400718108302, "entities": ["improve arabic ner", "nested entities", "wojoodfine corpus", "finegrained entity recognition"], "background": "1. The need to improve the performance of Arabic NER, especially for nested entities, which are more complex and less addressed in current models. 2. The desire to leverage the Wojood-Fine corpus, which provides a rich resource for fine-grained Arabic NER, to enhance the accuracy and granularity of entity recognition."}
{"hash_id": 9085265786567286350, "entities": ["arabic corpora", "ner", "finegrained annotations", "capitalization cues", "morphological inflections"], "background": "1. The limited availability of annotated Arabic corpora for NER, especially in fine-grained annotations, necessitates the development of methods that can enhance the performance of existing models without requiring extensive additional training data. 2. The unique challenges of Arabic NER, such as the absence of capitalization cues and rich morphological inflections, call for innovative approaches that can effectively handle the complexity of the language."}
{"hash_id": 7046996156936020970, "entities": ["crossdomain benchmark", "argument mining", "nlp algorithms"], "background": "1. The lack of a consistent cross-domain benchmark hinders the comparison and understanding of advances in argument mining across different contexts. 2. Previous work has not considered state-of-the-art NLP algorithms and multiple language modelling approaches, limiting generalizability."}
{"hash_id": 8910309528631540613, "entities": ["argumentative relation classification", "nonlexical aspects", "automated dialogue agents"], "background": "1. Existing models for Argumentative Relation Classification often ignore non-lexical aspects of dialogue and have limited understanding of the surrounding context, which hinders their accuracy and adaptability to new domains. 2. The need to improve the generalization capabilities of models in unseen conversation scenarios, particularly for practical applications like Automated Dialogue Agents."}
{"hash_id": 3082301131258015966, "entities": ["argument mining models", "efficiency accuracy", "computational overhead"], "background": "1. The need to improve the efficiency and accuracy of argument mining models by leveraging the commonalities between different argumentative techniques. 2. The desire to reduce the computational overhead and complexity of training separate models for each argument mining sub-task."}
{"hash_id": 2636275416892572439, "entities": ["standardized platform", "multimodal argument mining", "paralinguistic cues"], "background": "1. The need for a standardized platform to facilitate the reproducibility and comparison of models and datasets in Multimodal Argument Mining. 2. The recognition of the importance of paralinguistic cues, particularly auditory information, in argumentative discourse analysis, which has been largely overlooked in previous research."}
{"hash_id": 5758094917054591882, "entities": ["argumentation analysis", "dialogue contexts", "inference anchoring theory"], "background": "1. The need to bridge the gap between argumentation analysis and dialogue contexts, as real-world argumentation often occurs in dynamic exchanges between multiple participants. 2. The requirement for a more holistic approach that considers both argumentative structures and dialogical interactions, which is facilitated by the Inference Anchoring Theory (IAT) framework."}
{"hash_id": 3843902201869020052, "entities": ["argument graphs", "dialam", "iat framework"], "background": "1. The lack of computational frameworks for constructing argument graphs from dialogues, despite the well-studied field of argument mining from monologues. 2. The introduction of DialAM-2024 as a shared task that requires joint modeling of argumentation and dialogue information in the domain-independent IAT framework, highlighting the need for new methods to tackle this nuanced task."}
{"hash_id": 5662094497985390812, "entities": ["retrieval of political arguments", "sociocultural properties", "sociodemographic features prediction", "inclusive retrieval process"], "background": "1. The need to effectively retrieve relevant political arguments that also consider socio-cultural properties in different scenarios, which is crucial for informed decision-making and understanding diverse perspectives. 2. The challenge of predicting socio-demographic features from argument texts, especially in the implicit scenario where such information is not directly available, to ensure the retrieval process is inclusive and reflective of different demographics."}
{"hash_id": 3517972371539600965, "entities": ["sourcefree domain adaptation", "selftraining limitations", "reliable prototypes", "privacy concerns"], "background": "1. The need to improve self-training in source-free domain adaptation due to the limitations of existing methods that often contain substantial errors in pseudo-labeling, which can limit model performance in the target domain. 2. The requirement for a method that can generate reliable prototypes without accessing labeled source data, which is particularly relevant in clinical domains where data sharing is restricted due to privacy concerns."}
{"hash_id": 3890963840886374793, "entities": ["specialized datasets scarcity", "gastroenterology nlp models", "patient privacy protection"], "background": "1. The need to overcome the scarcity of specialized datasets in the healthcare domain, particularly in gastroenterology, which hinders the development of robust NLP models. 2. The requirement to protect patient privacy while still enabling the use of real patient data for training AI models in healthcare."}
{"hash_id": 4705624018232472873, "entities": ["ade classification models", "highstakes domains", "linguistic capabilities", "robust detection"], "background": "1. The need for a more thorough evaluation of ADE classification models in high-stakes domains like medicine, where real-world data can exhibit different patterns and noise than the training data. 2. The requirement to understand what specific linguistic capabilities models have learned beyond their surface-level performance metrics, to ensure robust and reliable detection of ADEs."}
{"hash_id": 2795091639481865128, "entities": ["inaccurate llm outputs", "clinicians", "instruction phrasing", "demographic subgroups"], "background": "1. The potential consequences of inaccurate outputs from LLMs in healthcare are significant, and clinicians are unlikely to be skilled prompt engineers, which highlights the need for more robust models. 2. The sensitivity of LLMs to instruction phrasing can lead to variations in performance and fairness, especially when dealing with demographic subgroups, which is a critical concern in clinical settings."}
{"hash_id": 5265549536871718259, "entities": ["temporal relation extraction", "zeroshot setting", "biomedical domain", "temporal consistency", "clinical notes"], "background": "1. The need to improve the performance of Large Language Models (LLMs) in temporal relation extraction tasks, particularly in the zero-shot setting and within the biomedical domain. 2. The requirement to enhance the temporal consistency of LLM predictions, which is crucial for accurate temporal reasoning in clinical notes."}
{"hash_id": 2597814268888759576, "entities": ["radiologist clinician workload", "hospital workflow inefficiency", "natural language generation", "radiology report automation"], "background": "1. To reduce the repetitive workloads of radiologists and clinicians, which can lead to burnout and inefficiencies in hospital workflows. 2. To improve the quality and accuracy of clinical documentation by leveraging advancements in natural language generation to automate the generation of radiology reports and discharge summaries."}
{"hash_id": 9205516742771972233, "entities": ["automated radiology reports", "diagnostic processes", "workload reduction", "overfitting prevention", "diverse datasets exploration"], "background": "1. To enhance diagnostic processes and reduce the workload on radiologists by automating the generation of radiology reports, which can be prone to fatigue and error when done manually. 2. To improve the performance of automated systems on diverse datasets by preventing overfitting to common phrases and ensuring the exploration of a broader vocabulary during training."}
{"hash_id": 4816233341271945270, "entities": ["scientific publications", "biomedical advances", "summarization methods", "laypeople accessibility"], "background": "1. The rapid growth of scientific publications makes it difficult for laypeople to understand the latest biomedical advances due to complex terminology and scientific language. 2. There is a scarcity of research on summarization targeting laypeople, and existing summarization methods often prioritize technical precision over accessibility."}
{"hash_id": 5743476053910976390, "entities": ["evaluation llms medical explanations", "diverse medical specialties", "diversify opensource llms"], "background": "1. The need for a more comprehensive evaluation of LLMs' ability to generate nuanced medical explanations, as current benchmarks often focus on classification accuracy and lack diverse medical specialties. 2. The requirement to diversify the pool of open-source medical LLMs, which are predominantly based on a single model (Llama2), to enhance performance and knowledge coverage."}
{"hash_id": 3726690913353240687, "entities": ["prompt quality standardization", "prompt engineering optimization", "clinical note sections"], "background": "1. The need to standardize prompt quality to achieve reliable uniformity in LLM performance across different clinical note sections and mentors. 2. The requirement to optimize prompt engineering to help clinicians efficiently use LLMs for clinical practice, given the high stakes in the clinical domain."}
{"hash_id": 8792658139760876314, "entities": ["domainspecific models", "biomedical text complexities", "uncertainty estimates", "highstakes medical applications"], "background": "1. The need for domain-specific models that can better handle the complexities and nuances of biomedical text, given the critical nature of decisions in healthcare. 2. The requirement for models to provide reliable uncertainty estimates to enhance trustworthiness in predictions, especially in high-stakes medical applications."}
{"hash_id": 3540609906688123298, "entities": ["annotated datasets", "chest xrays", "radiology reports"], "background": "1. The high cost and time consumption of creating annotated datasets for chest X-rays, which require extensive supervision to achieve state-of-the-art performance. 2. The global shortage of radiologists and the need for efficient methods that can leverage the existing semi-structured radiology reports as a form of distant supervision."}
{"hash_id": 3319106876042694544, "entities": ["evaluation metrics", "biomedical questionanswering", "human judgements", "traditional metrics", "impracticality human evaluation"], "background": "1. The need for more effective evaluation metrics for biomedical question-answering systems that can correlate better with human judgements, as traditional metrics like ROUGE and BLEU have shown low correlation. 2. The high cost and impracticality of using human experts for evaluation in the biomedical domain."}
{"hash_id": 8294609720216426428, "entities": ["icd code prediction", "patient hospital stay", "clinical notes", "discharge timing"], "background": "1. The ability to predict ICD codes earlier in a patient's hospital stay can lead to more timely interventions, improved patient care, and optimized resource allocation. 2. Utilizing the rich information in clinical notes throughout the patient's stay, rather than just at discharge, can enhance the accuracy and\u5b9e\u7528\u6027 of ICD code predictions, potentially reducing the manual burden on clinicians."}
{"hash_id": 3188231666349770801, "entities": ["large language models", "biomedical machine reading comprehension", "retrieval capabilities"], "background": "1. The need to thoroughly evaluate large language models like GPT on contextual biomedical machine reading comprehension, a domain that presents unique challenges not addressed by general-domain MRC tasks. 2. The requirement to improve the retrieval capabilities of LLMs, which is crucial for enhancing their performance in tasks that demand access to specific domain knowledge."}
{"hash_id": 7261859876290639648, "entities": ["scarcity of training data", "privacyconscious solutions", "small language models", "domainspecific tasks"], "background": "1. The scarcity of training data in clinical document domains and the need for privacy-conscious solutions that do not rely on cloud-based infrastructure. 2. The lack of consensus on the best approach to use small language models for domain-specific tasks, especially in low-resource settings."}
{"hash_id": 131282047440148266, "entities": ["improving medical language models", "patientfacing applications", "realworld benchmarks", "diverse medical questions"], "background": "1. The need to improve the medical accuracy and reliability of large language models in patient-facing applications to avoid potentially harmful advice. 2. The absence of real-world benchmarks that reflect diverse and challenging medical questions asked by patients, which are essential for testing and improving the performance of these models."}
{"hash_id": 877449120925289308, "entities": ["clinical entity extraction", "relation extraction", "generative pretrained transformer models"], "background": "1. The need to improve the precision of clinical entity and relation extraction tasks, especially in healthcare where accuracy is crucial. 2. The recognition that generative pre-trained transformer models like GPT may not capture all medical concepts due to limited or outdated domain-specific data."}
{"hash_id": 8601304244956171390, "entities": ["data labelling cost", "domainspecific tasks", "class imbalance", "clinical datasets"], "background": "1. The need to reduce the cost and expertise required for data labelling in domain-specific tasks like clinical report generation from chest X-ray images. 2. The challenge of class imbalance in clinical datasets, where common (healthy) samples often outnumber rare (unhealthy) samples, leading to poor performance in identifying critical cases."}
{"hash_id": 8648046353412745710, "entities": ["improving language models", "biomedical domain", "pretraining datasets", "quality metrics"], "background": "1. The need to improve the performance of language models in the biomedical domain, which has a specialized vocabulary and requires domain-specific knowledge not found in general texts. 2. The absence of studies examining the selection of pre-training datasets in the biomedical field using quality metrics specific to scientific papers."}
{"hash_id": 3285235006678889438, "entities": ["large language models", "biomedical concept recognition", "hallucination issue", "versatile training approach", "annotated data scarcity"], "background": "1. The need to overcome the limitations of Large Language Models (LLMs) in biomedical concept recognition, such as outdated knowledge and the issue of hallucination, which affect their accuracy and reliability in knowledge-intensive scenarios. 2. The requirement for a more versatile and less domain-specific training approach due to the scarcity of annotated data in the biomedical field, which hinders the effectiveness of state-of-the-art CR systems."}
{"hash_id": 6592536522646982638, "entities": ["antibiotic dosing recommendations", "antimicrobial resistance", "prescription attributes extraction"], "background": "1. The need to optimize antibiotic dosing recommendations to combat antimicrobial resistance, which is a significant public health concern. 2. The challenge of extracting essential prescription attributes from free-text clinical records without the need for extensive labeled data."}
{"hash_id": 7501433868128685795, "entities": ["clinical nlp tasks", "numerical information", "drug dosage calculations"], "background": "1. The extensive use of numbers in clinical texts and the limited understanding of their impact on clinical NLP tasks, which are crucial for precise decision-making. 2. The need to improve the reliability of large language models in handling numerical information, particularly in quantitative tasks such as drug dosage calculations and statistical analyses."}
{"hash_id": 907084332766889397, "entities": ["biomedical literature", "citation relationships", "inline citation contexts", "academic databases"], "background": "1. The increasing volume of biomedical literature makes it difficult for experts to assess publications quickly and understand the citation relationships between them. 2. Existing academic databases lack inline citation contexts, which are crucial for evaluating the value of publications and studying the evolution of research findings."}
{"hash_id": 3527984806737168841, "entities": ["domainadapted llama", "text patterns", "hyperparameters", "document sampling strategies"], "background": "1. The need to understand and quantify the specific text patterns and hyperparameters that contribute to the variability in performance of domain-adapted Llama 2 in biomedical tasks. 2. The desire to improve document sampling strategies and hyperparameter search methods for domain adaptation to enhance the performance of large language models in specific domains."}
{"hash_id": 2007765844527604661, "entities": ["semiautomated annotation", "biomedical ontologies", "structured knowledge extraction"], "background": "1. The need for more efficient and consistent (semi-)automated annotation approaches to improve the overall quality and usefulness of ontologies in the biomedical domain. 2. The challenge of extracting structured knowledge when entities and their relationships are not explicitly mentioned in the text, which is common in real-world biomedical articles."}
{"hash_id": 2311956248104668039, "entities": ["training data reduction", "entity disambiguation techniques", "biomedical entity linking"], "background": "1. The need to reduce training data and resource consumption in biomedical entity linking to make it more accessible and efficient. 2. The requirement for improved entity disambiguation techniques to handle ambiguous cases where multiple entities have similar scores for a common mention."}
{"hash_id": 551346896655715235, "entities": ["clinical texts", "medical decisionsupport systems", "multimodal models integration"], "background": "1. To leverage the valuable information within clinical texts that complements tabular data in electronic health records, which could potentially enhance the performance of medical decision-support systems. 2. To investigate and explain the limited performance improvement observed when including text data in multimodal models, thereby identifying gaps and improving the integration of text in machine learning for healthcare."}
{"hash_id": 3082415603093904920, "entities": ["protection adolescents", "alcohol advertisements", "automated detection system"], "background": "1. The need to protect adolescents from the negative influence of alcohol advertisements on digital platforms, which are a predominant source of exposure and can lead to substance use. 2. The requirement for an automated and accurate detection system that can keep up with the evolving and under-regulated online advertising landscape."}
{"hash_id": 7279579682134481234, "entities": ["clinical nlp research", "generative llms", "epilepsy", "personalized medicine", "antiseizure medications"], "background": "1. The need to improve outcomes for clinical NLP research in epilepsy by leveraging the power of generative LLMs to extract precise information from noisy, unstructured free-text EHRs. 2. The opportunity to enhance personalized medicine by predicting which anti-seizure medications would be most effective for individual patients based on their seizure frequency data extracted from EHRs."}
{"hash_id": 3957142000506956450, "entities": ["nlp techniques", "clinical documents", "documentlevel qa"], "background": "1. Healthcare professionals spend a significant amount of time manually extracting information from large clinical documents, which could be optimized with the use of NLP techniques. 2. Document-level QA from large clinical documents is often impractical due to the high computational resources required for training and inference."}
{"hash_id": 2586713634283777698, "entities": ["automated report generation", "multimodal generative systems", "radiology report generation"], "background": "1. To improve the efficiency of radiologists by automating the report generation process, allowing them to focus more on patient care. 2. To leverage the recent advancements in multimodal generative systems to enhance the performance of radiology report generation by integrating text and images in a single model."}
{"hash_id": 8371634194485689604, "entities": ["radiology report generation", "diagnostic decisionmaking", "large language models", "multimodal capabilities"], "background": "1. The need to enhance the efficiency and accuracy of radiology report generation, which is crucial for diagnostic and therapeutic decision-making. 2. The potential to leverage large language models to acquire multimodal capabilities and improve the understanding and description of chest X-ray images."}
{"hash_id": 7668473922018515797, "entities": ["discharge summary generation", "documentation burden", "large language models", "medical document automation"], "background": "1. To improve the efficiency and accuracy of discharge summary generation, reducing the documentation burden on physicians and enhancing the continuity of patient care. 2. To leverage the capabilities of large language models to automate the creation of complex medical documents, such as discharge summaries, which are crucial for patient outcomes and follow-up care."}
{"hash_id": 5199190280224483032, "entities": ["be understood by the general public"], "background": "1. The need to effectively summarize lengthy biomedical articles into lay summaries that can be understood by the general public, given the increasing volume of research in this field. 2. The exploration of different approaches to optimize performance under varying data availability and computational constraints."}
{"hash_id": 5823219263357985558, "entities": ["bridging research communication gap", "lay summaries", "dataset imbalance", "readability enhancement"], "background": "1. The need to bridge the gap between complex biomedical research information and non-expert audiences, such as the general public and journalists, by providing accessible lay summaries. 2. The requirement to improve the performance and quality of generated lay summaries, especially given the imbalance in dataset distribution and the challenge of maintaining technical accuracy while enhancing readability."}
{"hash_id": 6028791385994018359, "entities": ["bridging scientific knowledge gap", "scientific literacy", "lay summaries", "biomedical literature"], "background": "1. The need to bridge the gap between complex scientific knowledge and the understanding of non-experts, thereby increasing scientific literacy and enabling informed decision-making. 2. The requirement to develop efficient methods that reduce the manual effort and domain expertise needed to create lay summaries of biomedical literature."}
{"hash_id": 8813243216321824923, "entities": ["biomedical research accessibility", "lay summaries", "large language models limitations"], "background": "1. The need to make complex biomedical research accessible to a broader audience, including non-specialists, by simplifying scientific articles into lay summaries. 2. The desire to improve upon the limitations of large language models (LLMs) in domain-specific abstractive summarization, such as quadratic complexity, model hallucination, and domain shift."}
{"hash_id": 4599694457942752745, "entities": ["biomedical literature summarization", "nonexperts accessibility", "nlp abstractive summarization"], "background": "1. The increasing volume and complexity of biomedical literature necessitate efficient summarization techniques to make information accessible to non-experts, such as healthcare professionals and researchers. 2. The emergence of Natural Language Processing (NLP) technologies like abstractive summarization models presents an opportunity to improve the simplification and comprehensibility of research articles."}
{"hash_id": 3383548161235718871, "entities": ["biomedical research articles", "lay summaries", "finetuning approaches"], "background": "1. The need to make complex biomedical research articles more accessible to a general audience by converting them into lay summaries. 2. The desire to improve upon existing fine-tuning approaches for summarization tasks in the biomedical domain, which often struggle with technical jargon and require extensive background knowledge."}
{"hash_id": 1918402042174544263, "entities": ["bridging science communication gap", "lay summaries", "biomedical research"], "background": "1. The need to bridge the gap between complex scientific insights and public understanding, especially for children, to promote accurate dissemination of scientific knowledge and prevent misinformation. 2. The challenge of converting dense technical language in biomedical research papers into accurate and understandable lay summaries for a diverse audience."}
{"hash_id": 7259360260852004674, "entities": ["accessible scientific communication", "lay summaries", "manual generation inefficiencies"], "background": "1. The need for a more accessible way to share complex scientific information with the general public to foster informed decision-making and public support for scientific endeavors. 2. The requirement to reduce the costs and inefficiencies associated with manual generation of lay summaries by expert writers."}
{"hash_id": 5454639580179930638, "entities": ["biomedical literature", "lay summarization", "large language models"], "background": "1. The complexity of biomedical literature often leads to misinterpretations by the general public, and there is a need to bridge the gap between scientific research and public understanding. 2. Existing methods for lay summarization have not fully leveraged the potential of large language models to produce summaries that are both readable and maintain factual accuracy and relevance."}
{"hash_id": 6772175010696448394, "entities": ["access scientific literature", "interdisciplinary knowledge sharing", "efficient sequencetosequence models", "lay summarization"], "background": "1. The need to enhance access to scientific literature and promote interdisciplinary knowledge sharing and public understanding, especially in the biomedical field where clear medical information is crucial for informed health decisions. 2. The requirement to overcome the computational expenses and resource demands of large language models (LLMs) by using more efficient sequence-to-sequence models for lay summarization."}
{"hash_id": 607096665037530570, "entities": ["scientific research accessibility", "biomedical lay summaries", "large language models", "health knowledge dissemination"], "background": "1. The need to make scientific research, particularly in biomedical fields, accessible to a wider audience by converting complex articles into comprehensible lay summaries. 2. The potential of large language models to refactor complex technical information into simpler narratives, which could significantly improve the dissemination of health-related knowledge to non-expert audiences."}
{"hash_id": 5172130612933581867, "entities": ["alignment strategies", "cultural values", "language models", "commonsense morality"], "background": "1. The need to ensure that language models align with the cultural values and preferences of non-English speaking users, as current alignment strategies are predominantly English-centric. 2. The importance of understanding how cultural differences affect the learning and generalization capabilities of language models in terms of commonsense morality."}
{"hash_id": 1049459456633372286, "entities": ["bias mitigation", "fairness", "multilingual models"], "background": "1. The increasing adoption of LLMs in critical real-world applications necessitates the mitigation of bias to ensure fairness and prevent representational harm. 2. Prior research indicates that multilingual models can reduce language-specific ethnic bias, but a comprehensive understanding of their impact on bias mitigation is needed."}
{"hash_id": 6392682904441286219, "entities": ["sociocultural factors", "hate speech detection", "antilgbtq content", "keywordsearch approaches"], "background": "1. The need to account for sociocultural factors that influence the effectiveness of hate speech detection systems, particularly in their ability to monitor anti-LGBTQ+ content across different linguistic and cultural contexts. 2. The recognition that current keyword-search approaches overfit on slurs, potentially missing broader instances of anti-LGBTQ+ content and failing to address the full scope of the problem."}
{"hash_id": 6094380222165794049, "entities": ["climate communication", "oral societies", "nlp nuances"], "background": "1. The need to bridge the communication gap between climate experts and oral societies, which are often overlooked in climate communication efforts. 2. The recognition that existing NLP approaches, such as machine translation, fail to account for the linguistic and cultural nuances of these societies, hindering effective intercultural dialogue."}
{"hash_id": 1047067411257617554, "entities": ["automated sdg target detection", "climateaware nlp", "large language models"], "background": "1. The manual process of inspecting environmental reports for SDG target detection is labor-intensive and time-consuming, necessitating the need for automated methods. 2. The emergence of climate-aware NLP and the potential of Large Language Models (LLMs) to improve efficiency and accuracy in detecting SDG targets in textual data."}
{"hash_id": 6324865334901567990, "entities": ["visionlanguage models", "bistable images", "language priors", "ambiguous visual stimuli"], "background": "1. To investigate and quantify the biases and robustness of vision-language models when interpreting bistable images, which can provide insights into their reliability and alignment with human perception. 2. To understand the influence of language priors and prompts on the models' interpretations of ambiguous visual stimuli, which is crucial for improving the effectiveness of these models in real-world applications."}
{"hash_id": 248459541622827690, "entities": ["cognitive plausibility", "transformer models", "human language comprehension", "surprisal estimates", "psychometric data"], "background": "1. To make transformer models more cognitively plausible by reflecting human cognitive limitations and processing strategies, particularly the incremental and resource-constrained nature of human language comprehension. 2. To improve the alignment between surprisal estimates from transformer models and human psychometric data, specifically in relation to the sensitivity of human parsing to local linguistic information."}
{"hash_id": 4878443590305902675, "entities": ["emergence linguistic properties", "artificial agents", "communication protocols", "human languages", "emergent communication", "image content", "pretrained visionlanguage models", "machine understanding"], "background": "1. To better understand the emergence of linguistic properties in artificial agents and bridge the gap between their communication protocols and human languages. 2. To improve the grounding of emergent communication in actual image content, which is crucial for fine-tuning pre-trained vision-language models and enhancing machine understanding of natural human language."}
{"hash_id": 7017012382286547282, "entities": ["hierarchical structure", "cognitive models", "superior temporal gyrus", "hierarchical sentence structure", "language comprehension"], "background": "1. To investigate the importance of hierarchical structure in language models as cognitive models of human language processing. 2. To identify the specific brain regions, such as the superior temporal gyrus, that may be sensitive to hierarchical sentence structure during language comprehension."}
{"hash_id": 2812674174995778395, "entities": ["prediction versus integration", "computational model", "eyetracking data"], "background": "1. To resolve the ongoing debate in psycholinguistics regarding the relative importance of prediction versus integration in human language processing. 2. To provide a computational model that can better explain the variability in eye-tracking data during language comprehension, particularly across different languages."}
{"hash_id": 4480089818678006730, "entities": ["human attention mechanisms", "machine attention mechanisms", "natural language processing", "cognitive nuances", "anaphora resolution"], "background": "1. To investigate the extent to which human attention mechanisms can be correlated with and improve the performance of machine attention mechanisms in natural language processing tasks. 2. To enhance the understanding of cognitive nuances in human perception and apply these insights to enhance the ability of neural networks to resolve anaphora effectively."}
{"hash_id": 2179835047666428504, "entities": ["large language models", "lexical aspect", "aspectual feature recognition"], "background": "1. The need to understand how well large language models can capture the complex linguistic features of lexical aspect, which are crucial for natural language understanding. 2. The potential to improve the performance of language models in aspectual feature recognition by incorporating verb information in different prompt formats."}
{"hash_id": 7516069281473873940, "entities": ["large language models", "morphological knowledge", "generalization abilities", "language complexity"], "background": "1. To investigate whether large language models (LLMs) can generalize their morphological knowledge beyond their training data to new and unfamiliar words, particularly across different languages. 2. To determine the impact of a language's morphological complexity on the generalization abilities of LLMs and to understand if this complexity affects their performance more than the amount of training data."}
{"hash_id": 7690596990007872115, "entities": ["visionandlanguage models", "humanlike processing", "vlms suitability"], "background": "1. To determine whether vision-and-language models process multimodal information in a human-like way, which could improve interactions between humans and machines. 2. To identify what is missing in current VLMs to make them more suitable for simulating human language emergence and evolution."}
{"hash_id": 1165496582990142901, "entities": ["improving vlms performance", "abstract concepts prediction", "challenging traditional view"], "background": "1. The need to improve the performance of VLMs, like SigLIP, in predicting accurate labels for images, especially for abstract concepts, where semantic relationships are complex and diverse. 2. The desire to challenge and potentially revise the traditional view that concrete concepts have less diverse visual representations and associations compared to abstract concepts."}
{"hash_id": 3057345750631419260, "entities": ["neural networks", "llms", "grammar rules", "morphological generalization"], "background": "1. To investigate the extent to which neural networks, specifically LLMs, learn and apply grammar rules akin to human language acquisition, particularly in the context of complex morphological structures. 2. To provide empirical evidence on the capabilities of LLMs in performing explicit classification tasks that require morphological generalization, which is crucial for understanding their grammatical knowledge."}
{"hash_id": 9000037278880831978, "entities": ["language models", "human cognitive processes", "attention weights", "task specificity"], "background": "1. The need to better understand the internal processes of language models and align them with human cognitive processes, particularly in terms of text importance during reading. 2. The recognition that previous methods of interpreting language models using attention weights may not accurately reflect token significance and that task specificity is crucial for improving correlations with human reading metrics."}
{"hash_id": 16470396171261466, "entities": ["improve generalization ability", "unify conceptualizations", "comprehensive benchmarks"], "background": "1. The need to improve the generalization ability of language models in emotion analysis by unifying different conceptualizations of emotions. 2. The desire to create more comprehensive benchmarks that reflect the full complexity of human emotional understanding."}
{"hash_id": 6563300750846932846, "entities": ["chinese llms", "disciplinary knowledge", "evaluation beyond accuracy", "memorization robustness"], "background": "1. The need to reassess the progress of Chinese LLMs in disciplinary knowledge, as current benchmarks may not fully capture their capabilities or potential issues like data contamination and leakage. 2. The requirement for a more nuanced evaluation of Chinese LLMs beyond accuracy, to understand their memorization and robustness to variations in question presentation."}
{"hash_id": 8257112657164277964, "entities": ["data contamination", "fair evaluation", "knowledge base"], "background": "1. The need to understand the extent of data contamination in available datasets and models to ensure fair and unbiased evaluation of large-scale language models. 2. The requirement for an organized and compiled knowledge base of real cases of data contamination to aid researchers in avoiding the use of contaminated resources."}
{"hash_id": 383933473976824299, "entities": ["endangered languages documentation", "transcription tools", "automatic speech recognition", "linguistic variation"], "background": "1. Accelerating the documentation of endangered languages like \u010cakavian, which lack transcription tools, to preserve their unique linguistic characteristics. 2. Improving the efficiency and accuracy of automatic speech recognition (ASR) for low-resource languages in the presence of significant linguistic variation and code-switching."}
{"hash_id": 3358909852615576717, "entities": ["filipino pos tagging", "nlp pipelines", "crosslingual transfer learning"], "background": "1. The lack of annotated data for Filipino POS tagging hinders the development of robust NLP pipelines for the language, despite the presence of textual resources. 2. Cross-lingual transfer learning, particularly in a zero-shot setting, offers a potential solution to the data scarcity issue by leveraging knowledge from resource-rich languages."}
{"hash_id": 7263280259589705959, "entities": ["debiasing methods", "representations of bias", "parameterefficient techniques", "largescale language models"], "background": "1. Existing debiasing methods fail to capture different representations of bias, leading to incomplete bias removal and unsatisfactory debiasing performance. 2. There is a need for parameter-efficient debiasing techniques to mitigate bias in large-scale language models without requiring extensive computational resources."}
{"hash_id": 7482930692086560204, "entities": ["implicit gender bias", "nlp annotation tasks", "pretrained language models"], "background": "1. The need to define and measure implicit gender bias in a way that is understandable to a wide audience, including those not familiar with NLP annotation tasks. 2. The desire to compare and understand the consistency or differences in gender bias between pre-trained language models and human annotators."}
{"hash_id": 6059018710227313783, "entities": ["gender bias reduction", "ai transparency", "interpretability"], "background": "1. The need to reduce gender bias and discrimination caused by perpetuating gender stereotypes in text. 2. The requirement for improved transparency and interpretability in AI models that predict gender stereotypes to understand the basis of their decisions."}
{"hash_id": 5509283665937550108, "entities": ["gender biases ai summaries", "educational technologies", "reflection summarization"], "background": "1. The need to uncover and mitigate potential gender biases in AI-generated summaries of student reflections, which can affect how teaching staff address the concerns of their students. 2. The importance of ensuring fairness in educational technologies, particularly in reflection summarization, which is crucial for identifying student misconceptions and adapting instruction."}
{"hash_id": 6214261206333945727, "entities": ["gender bias", "llms", "nlp tasks", "harmful consequences"], "background": "1. The need to investigate and mitigate gender bias in translations generated by LLMs, which have become popular for NLP tasks, including machine translation, but have received less attention regarding gender bias compared to NMT models. 2. The potential harmful consequences of gender bias in translations, which can lead to misrepresentation, underrepresentation, and allocational harms for certain social groups."}
{"hash_id": 2182056277142943941, "entities": ["authorship profiling", "societal harm", "privacy breaches", "demographic signals"], "background": "1. To understand the circumstances necessitating the deployment of Authorship Profiling (AP) models and the potential societal harm they can cause, particularly in terms of privacy breaches and the reinforcement of stereotypes. 2. To investigate the core assumption that demographically related signals are comparable across datasets, which is fundamental to improving the accuracy and reducing biases in AP models."}
{"hash_id": 5549445355812817857, "entities": ["gender bias mitigation", "nlp systems", "templatebased approaches", "computational overhead"], "background": "1. The need to measure and mitigate gender bias in NLP systems without relying on the limitations of template-based approaches, which may not accurately represent real-world data and lack generalisability across tasks. 2. The goal to reduce unintended gender bias in NLP models while maintaining overall classification performance and minimizing the computational overhead of data augmentation."}
{"hash_id": 782647298122538820, "entities": ["relation extraction biases", "natural language understanding", "gender bias", "place of birth bias"], "background": "1. The need to identify and mitigate biases in Relation Extraction systems, which are critical for many Natural Language Understanding tasks, to ensure fairness and accuracy, especially for underrepresented groups. 2. The absence of standardized methods and taxonomies for evaluating and addressing biases in Relation Extraction, particularly concerning gender and place of birth."}
{"hash_id": 1472993352479664191, "entities": ["gender bias analysis", "educational text", "nlp methodologies", "smallscale studies"], "background": "1. The need to automatically and quantitatively analyze gender bias in educational text to identify and mitigate reinforcing gender stereotypes in children's learning materials. 2. The gap in using NLP methods to extend gender bias taxonomies and apply them to educational resources, which have been predominantly analyzed using qualitative methodologies suitable for small-scale studies."}
{"hash_id": 764539970829106055, "entities": [], "background": "1. To reduce and prevent the dissemination of gender biases present in training data, which can lead to errors and the amplification of harmful stereotypes in machine translation outputs. 2. To provide a more inclusive and user-centric approach to machine translation by allowing users to select gendered translation alternatives in cases of ambiguity."}
{"hash_id": 5059064633210948393, "entities": ["gender bias", "dependency parsing", "syntactic differences"], "background": "1. To critically assess and address the potential methodological flaws in the original study by Garimella et al. (2019) that suggested gender bias in dependency parsing. 2. To provide a more rigorous foundation for the study of gender biases in foundational NLP tasks by replicating experiments and analyzing specific syntactic differences."}
{"hash_id": 2054358946397198792, "entities": ["gender bias", "llms", "linguistic structures"], "background": "1. The need to reduce gender bias and stereotypes perpetuated by LLMs, which can influence societal concepts of gender and equitable representation. 2. The recognition that linguistic structures and norms can both reflect and reinforce gender biases, necessitating intervention at the level of language modeling."}
{"hash_id": 1486840646230755281, "entities": ["sociodemographic bias", "language models", "bias mitigation"], "background": "1. The potential for harm caused by sociodemographic bias in language models when used in real-world settings, which can lead to negative societal impacts. 2. The need to guide future research towards more effective and reliable solutions for bias mitigation in order to achieve equitable language models."}
{"hash_id": 5352295071374222070, "entities": ["barriers to valid research", "methodological validity", "ethical concerns", "inclusive nlp systems", "sociodemographic inference"], "background": "1. To highlight and address the significant barriers to valid and respectful research caused by the varying engagement with methodological validity and ethical concerns in associating names with sociodemographic characteristics. 2. To contribute to the development of more inclusive NLP systems by acknowledging and mitigating biases and harms associated with the use of personal names in sociodemographic inference."}
{"hash_id": 5380332360696111119, "entities": ["fair representation", "multimodal models", "language inclusivity", "gender bias"], "background": "1. The need to ensure fair representation in large-scale multimodal models that are becoming increasingly integrated into global, multilingual contexts. 2. The recognition that existing models, despite being designed with language inclusivity in mind, have not been evaluated for biases beyond language, such as gender bias, and that these biases may vary across different regions and cultures."}
{"hash_id": 725153356662005859, "entities": ["historical biases", "lgbtqia community", "nonenglish llms", "norwegian models", "bias mitigation research"], "background": "1. The paper is motivated by the need to prevent historical biases, including those against the LGBTQIA+ community, from being perpetuated by large language models, which could otherwise undermine progress towards equality and social justice. 2. The research is driven by the lack of existing studies addressing LGBTQIA+ bias in non-English LLMs, particularly in Norwegian models, despite the Anglocentric bias in current bias mitigation research and the significant impact such biases can have on marginalized communities."}
{"hash_id": 8443990477340665671, "entities": ["gendered pronoun translation", "relationshipbased sentences", "combat bias", "harmful stereotypes"], "background": "1. The need to identify and correct the consistently incorrect translation of gendered pronouns in relationship-based sentences to combat bias against same-gender relationships in machine translation. 2. The recognition that incorrect translations can reinforce harmful stereotypes and disenfranchise individuals in same-gender relationships, who may already face social stigma."}
{"hash_id": 8452288274813960923, "entities": ["partial subjectivity", "annotator demographics", "sexism detection", "diversity perspectives"], "background": "1. The need to address the partial subjectivity and unreliability of datasets for sexism detection due to the influence of annotators' demographic backgrounds on their labeling decisions. 2. The goal to improve the performance of sexism detection models by reflecting the diversity of annotator perspectives and accounting for the ambiguity and disagreements in judgments."}
{"hash_id": 5811669300603321124, "entities": ["gender bias", "automatic translation", "machine translation systems"], "background": "1. The need to raise awareness and address the challenges of gender bias in automatic translation, which can affect the quality and fairness of translation outputs. 2. The pursuit of long-term solutions and advancements in gender representation within machine translation systems, particularly for multilingual and low-resource languages."}
{"hash_id": 1008678288945063279, "entities": ["traditional metrics", "llms creative tasks", "contextspecific evaluation", "human evaluation scaling", "costeffective evaluation"], "background": "1. The ineffectiveness of traditional metrics like BLEU and ROUGE for evaluating the outputs of LLMs in creative tasks or where reference outputs are unavailable, leading to the need for more accurate and context-specific evaluation methods. 2. The high cost and impracticality of scaling human evaluation, prompting the exploration of a more cost-effective solution that combines the capabilities of LLMs with human input for evaluation purposes."}
{"hash_id": 3501051177429858440, "entities": ["toxicity and bias", "language models", "parameterefficient approaches"], "background": "1. The need to address the issue of toxicity and bias inherited from training corpora in large language models, which can lead to unsafe text generation even when using seemingly innocuous prompts. 2. The requirement for more parameter-efficient approaches due to the impracticality of full-model finetuning on the largest language models, which contain over 100 billion parameters and are resource-intensive to tune."}
{"hash_id": 7250677728266457781, "entities": ["cultural awareness in vlms", "diverse user backgrounds", "automatic evaluation metrics", "model hallucination"], "background": "1. To ensure that vision-language models (VLMs) are culturally aware and provide accurate descriptions that resonate with diverse user backgrounds, particularly for visually impaired individuals. 2. To bridge the gap between automatic evaluation metrics and human judgment, addressing the issue of model hallucination and improving the reliability of VLMs as visual assistants."}
{"hash_id": 7136517544594785693, "entities": ["social signals", "llms", "sociolinguistic capabilities"], "background": "1. The increasing integration of LLMs into social contexts demands a better understanding of their ability to process and respond to social signals, which are crucial for effective human interaction. 2. There is a need to systematically investigate and evaluate the socio-linguistic capabilities and limitations of LLMs to improve their performance in diverse and complex social scenarios."}
{"hash_id": 5034787097735695106, "entities": ["lowresource indic language", "translation quality", "data scarcity"], "background": "1. The need to enhance translation quality for low-resource Indic language scenarios, where existing translation systems often perform poorly. 2. The challenge of data scarcity in these scenarios, which hinders the development of effective speech-to-text translation models."}
{"hash_id": 879025639710090897, "entities": ["speechtext matching", "crossmodal retrieval", "limited training resources"], "background": "1. The need to match speech and text in languages where paired speech and text data are not readily available during the pre-training of large language models. 2. The goal to enhance the performance of retrieval systems in a cross-modal and cross-lingual context, particularly in the face of limited training resources for a large number of languages."}
{"hash_id": 2845161317026568212, "entities": ["emotional content translation", "large language models", "translation models", "emotioninfluenced translation"], "background": "1. The motivation to propose these methods stems from the observation that while Large Language Models (LLMs) have achieved significant success in Machine Translation (MT), there is still room for improvement in translation quality, particularly in capturing the nuances of emotional content in text. 2. The second motivation is based on the recognition that previous research has demonstrated the benefit of adding extra information to translation models to influence the translation output, and emotion has not been fully exploited as a dimension to condition the translation process."}
{"hash_id": 5318321313661779334, "entities": ["cascade system", "endtoend st systems", "parallel training data"], "background": "1. The traditional cascade system is preferred due to its generally better performance over end-to-end ST systems, especially because it can leverage large ASR and MT datasets to achieve high-accuracy ASR and MT systems. 2. The end-to-end ST architecture has a disadvantage in the lack of parallel training data, which can negatively impact its performance compared to the traditional cascade ST system."}
{"hash_id": 3757699956449988760, "entities": ["data insufficiency", "endtoend training", "speech translation tasks", "error accumulation", "cascade approach"], "background": "1. The primary motivation is the challenge of data insufficiency in end-to-end training for speech translation tasks, which leads to suboptimal performance. 2. The second motivation is the issue of error accumulation in the cascade approach due to the multi-step process of ASR, MT, and potentially TTS, which this research aims to mitigate through improved training strategies and model ensembling."}
{"hash_id": 1095790989247824655, "entities": ["audiovisual content accessibility", "viewers with diverse needs", "subtitling constraints", "reading speed constraints"], "background": "1. The need to ensure accessibility and comprehension of audiovisual content for viewers with diverse needs, including those with hearing impairments or language barriers. 2. The challenge of adhering to recommended reading speed constraints in subtitling, which can be especially difficult for audiences with slower reading speeds or limited language proficiency."}
{"hash_id": 2816236891465226618, "entities": ["genderrelated translation errors", "speech translation systems", "inclusive technology"], "background": "1. The need to address gender-related errors in automatic translation that not only impact technical accuracy but also risk misrepresenting or underrepresenting gender minorities, potentially reinforcing gendered stereotypes. 2. The importance of ensuring that speech translation systems provide equal service quality and properly recognize and represent women, thus contributing to a more inclusive and equitable technological landscape."}
{"hash_id": 8217869305054653603, "entities": ["simultaneous speechtotext translation", "translation quality", "output latency", "pretrained models"], "background": "1. The need to address the unique challenges of Simultaneous Speech-to-Text Translation (SimulST), where the model must balance translation quality with the real-time requirement of output latency, which is crucial for applications like online meetings and live broadcasts. 2. The interest in leveraging the capabilities of large, pre-trained models without the need for ad-hoc training or adaptation for SimulST, potentially simplifying the deployment and improving performance in a variety of language pairs."}
{"hash_id": 2557872106323005096, "entities": ["automatic subtitling", "language barriers", "readability constraints"], "background": "1. The increasing volume of audiovisual content demands efficient and accessible methods for automatic subtitling to overcome language barriers and enhance the accessibility of this material for a diverse audience. 2. The need to ensure that subtitles are not only accurate translations but also adhere to specific readability and display constraints to minimize the cognitive effort of the viewers."}
{"hash_id": 683260153839551436, "entities": ["lowresource languages", "speech translation systems", "crosslingual similarities"], "background": "1. The significant gap in research and negative impacts on speech communities that use low-resource languages, which are not commonly targeted by NLP tasks. 2. The need to improve the performance of speech translation systems for languages with limited data by exploiting cross-lingual similarities and innovative data augmentation methods."}
{"hash_id": 2167360451041457718, "entities": ["speech translation solutions", "lowresource languages", "innovative approaches", "limited training data"], "background": "1. The need to develop speech translation solutions for low-resource languages, such as Maltese, which lack extensive support for speech technology. 2. The challenge of achieving accurate translation in scenarios where training data is limited, necessitating innovative approaches to improve performance with fewer resources."}
{"hash_id": 3119638495435524761, "entities": ["memory footprint reduction", "ondevice deployment", "lowresource settings"], "background": "1. The need to reduce the large memory footprint of Speech-to-text Translation (ST) models initialized with Self-Supervised Speech (SSS) models, which hinders on-device deployment due to memory constraints. 2. The requirement for a method that does not rely on transcripts, making it applicable in low-resource settings where such data is scarce."}
{"hash_id": 8132174464610202347, "entities": ["neural endtoend systems", "lowresource languages", "pretrained language models", "speech translation", "suboptimal performance"], "background": "1. The lack of parallel data required to train neural end-to-end systems for speech translation, particularly for low-resource languages like Quechua-Spanish, which has historically hindered the development of effective speech translation models. 2. The challenge of creating usable models for low-resource languages within pre-trained language models (PLMs), as these models often do not support such languages, leading to suboptimal performance in speech translation tasks."}
{"hash_id": 7387053535538058814, "entities": ["speech translation systems", "lowresource languages", "speechbased translation"], "background": "1. The need to develop effective speech translation (ST) systems for low-resource languages, which often lack the institutional support and funding for NLP and speech tool development, yet have populations that could greatly benefit from ST tools. 2. The recognition that some low-resource language speakers have low literacy rates or limited writing traditions, necessitating speech-based translation systems rather than text-based ones."}
{"hash_id": 5184107257480663432, "entities": ["simultaneous speech translation", "large language models", "offline speechtotext systems"], "background": "1. The need for real-time translation of spoken language, especially for simultaneous speech translation (SST), which is critical for applications such as live conferencing and broadcasting. 2. The potential of recent advancements in large language models (LLMs) to serve as a robust backbone for improving the performance and efficiency of offline speech-to-text (ST) systems."}
{"hash_id": 8061483979243413928, "entities": ["dialectal arabic speech", "training data scarcity", "translation tasks", "robust approach"], "background": "1. The scarcity of available training data for dialectal Arabic speech to English translation tasks necessitates a method that can effectively utilize limited resources. 2. The complexity of dialectal Arabic variations poses challenges for speech recognition and translation systems, requiring a robust approach to capture nuances in the spoken language."}
{"hash_id": 8795455912095445123, "entities": ["brittleness of shortform systems", "traintest mismatch", "robust longform speech translation", "unsegmented recordings"], "background": "1. The motivation to propose these methods stems from the brittleness of existing short-form systems in real-world scenarios, where performance can fluctuate significantly due to train/test mismatch and the inability to replicate the segmentation characteristics of the training data during blind testing. 2. The second motivation is the need for a robust long-form speech translation system that can handle unsegmented recordings directly, without the reliance on voice-activity detection (VAD) which can introduce complexity and reduce performance in multi-domain scenarios."}
{"hash_id": 3466585126056108469, "entities": ["error propagation", "added latency", "cascadebased translation", "computational costs"], "background": "1. The need to minimize error propagation and added latency that are inherent in traditional cascade-based simultaneous speech translation systems, which combine automatic speech recognition (ASR) and machine translation modules. 2. The goal to reduce the computational costs and the effort required for system development that are associated with conventional end-to-end SimulST models trained specifically for simultaneous settings."}
{"hash_id": 1913333340946098706, "entities": ["large language models", "prediction accuracy", "asr mt", "cascaded speech translation"], "background": "1. The potential of Large Language Models (LLMs) to improve prediction accuracy in ASR and MT, especially in terms of vocabulary and understanding complex terminology. 2. The need to enhance the performance of cascaded speech translation systems, particularly in scenarios with overlapping speakers and background noise."}
{"hash_id": 2750443922206458884, "entities": ["dialectal arabic", "standardized orthography", "annotated data scarcity", "speech translation systems"], "background": "1. The lack of standardized orthography in dialectal Arabic, which complicates the creation of consistent training data for Automatic Speech Recognition (ASR) and Neural Machine Translation (NMT) systems. 2. The scarcity of annotated data for dialectal Arabic variants such as Levantine Arabic and Tunisian Arabic, which hinders the development of high-quality speech translation (ST) systems."}
{"hash_id": 5466163718535891165, "entities": ["translation accuracy", "smallscale llms", "continual pretraining", "spoken language translation"], "background": "1. The need to investigate methods capable of achieving translation accuracy equivalent to existing translation models with relatively small-scale LLMs, as observed in the lower accuracy of small-scale LLMs in 8-shot scenarios compared to supervised encoder-decoder models. 2. The effectiveness of continual pre-training with parallel data in the context of LLMs for translation, which has been reported but not fully explored, especially with respect to the order of source and target sentences in the data and the robustness in translating spoken language."}
{"hash_id": 2848856260327971598, "entities": ["data scarcity", "dialectal translation", "speech translation system"], "background": "1. The need to address data scarcity problems in translating dialectal and low-resource languages into English. 2. The lack of research on which speech translation system (cascaded vs. end-to-end) performs better in dialectal and low-resource scenarios."}
{"hash_id": 256144951920320885, "entities": ["speech translation", "language coverage", "multitask learning", "multilingual multimodal translation"], "background": "1. To expand language coverage and make speech translation (ST) technologies more inclusive, allowing a wider range of users to interact with the technology in their preferred language and format. 2. To address the multi-task learning challenge inherent in multilingual and multimodal translation, which is a central issue in machine learning."}
{"hash_id": 1592597293618184925, "entities": ["simultaneous interpretation corpora", "evaluation methods", "chunkwise monotonic translation", "language pair structural differences"], "background": "1. The existing simultaneous interpretation corpora and evaluation methods may underestimate the performance of SI models due to the use of offline translation corpora and the fact that human interpreters do not translate every piece of information from the source language. 2. There is a need to better understand the characteristics of chunk-wise monotonic translation in SI, especially for language pairs with significant structural differences like English and Japanese, to improve the evaluation and performance of SI models."}
{"hash_id": 1277796227705587760, "entities": ["lowresource language translation", "maltese linguistic structure", "crosslingual model capabilities"], "background": "1. The motivation to propose these methods stems from the challenge of translating a low-resource language like Maltese, which has a complex linguistic structure and limited available data, into a widely spoken language like English. 2. The Maltese language's unique hybrid nature, sharing vocabulary with Arabic, Italian, and English, provides an opportunity to test and potentially enhance the crosslingual capabilities of state-of-the-art models."}
{"hash_id": 4367800124424447842, "entities": [], "background": "1. The increasing demand for subtitles across various media platforms necessitates the need for efficient and high-quality subtitling solutions that can handle the complexities of different languages and speech patterns. 2. The limited display space for subtitles and the need to adapt subtitles to the playback speed of the video and the reading speed of the audience highlight the importance of subtitle compression to enhance subtitle quality without losing essential information."}
{"hash_id": 5858735256052044394, "entities": ["rare words translation", "named entities", "speech translation complexity", "asr issues", "nmt translation challenges"], "background": "1. The challenge of accurately translating rare words, particularly named entities (NEs), which are crucial for conveying the meaning of a sentence and can significantly impact user understanding and experience when mistranslated. 2. The increased complexity of translating NEs in speech translation (ST) due to the dual modality issues of automatic speech recognition (ASR) and the lack of alternative translations for NEs in neural machine translation (NMT)."}
{"hash_id": 4195983732849820314, "entities": ["multiple latency levels", "machine translation", "tradeoff balance"], "background": "1. The need to support multiple latency levels without training separate models for each level, which can be resource-intensive and inefficient. 2. The goal to balance the trade-off between translation quality and latency in simultaneous machine translation, especially in low-latency scenarios such as conferences or lectures."}
{"hash_id": 8333209296511941910, "entities": ["opendomain questionanswering", "largescale facts", "nlp technology", "twostage retrieval", "ssg model", "computational efficiency"], "background": "1. The need for open-domain question-answering models to accurately reason with large-scale facts in databases to potentially substitute or augment existing database management systems with NLP technology. 2. The inefficiency of current two-stage retrieval and reasoning architectures, particularly the SSG model, in scaling inference speed as the size of the database increases, leading to a significant bottleneck in computational efficiency."}
{"hash_id": 5878709950420347436, "entities": ["stateoftheart te models", "large labeled datasets", "large language models", "zeroshot setting"], "background": "1. The need to overcome the dependency of state-of-the-art TE models on large labeled datasets, which limits their generality and applicability in real-world scenarios due to the expense and effort of producing tailored datasets. 2. The observation that Large Language Models (LLMs) have the potential to perform TE with fewer labeled examples or even in a Zero-Shot setting, thus reducing the amount of data required and lifting the restriction of adhering to a predefined set of relations."}
{"hash_id": 4853077368081690503, "entities": ["data scarcity", "information extraction", "privacy regulations"], "background": "1. The need to overcome data scarcity issues that hinder the generalization and performance of Information Extraction models. 2. The requirement to comply with privacy regulations and concerns while still enabling the training of effective IE models without exposing sensitive information."}
{"hash_id": 5888110146843457250, "entities": ["lowresource languages", "multilingual llms", "graph knowledge integration"], "background": "1. The existing multilingual LLMs often struggle with low-resource languages due to the scarcity of training data, which limits their effectiveness in these contexts. 2. Previous research has primarily focused on integrating graph knowledge into LLMs for high-resource languages, such as English, leaving a gap in addressing the challenges faced by low-resource languages."}
{"hash_id": 4962512761784524467, "entities": ["knowledge graph construction", "humanexpert involvement", "domainspecific data", "unique vocabularies", "evolving ontologies"], "background": "1. The need to reduce the significant human-expert involvement required for large-scale knowledge graph construction, which is both time-consuming and expensive. 2. The challenge of building knowledge graphs from domain-specific data due to unique vocabularies, evolving ontologies, and the lack of sufficient annotated data for training robust models."}
{"hash_id": 330987419615446506, "entities": ["textattributed graphs", "graph neural network models", "node classification tasks"], "background": "1. The motivation to propose these methods stems from the complexity and resource-intensiveness of current state-of-the-art (SoTA) techniques, which make it challenging to effectively integrate the rich textual attributes of nodes in Text-Attributed Graphs (TAGs) into Graph Neural Network (GNN) models. 2. The need to simplify the implementation and training processes of models that handle TAGs, while maintaining or improving the performance of node classification tasks, is also a critical motivation behind the proposed methods."}
{"hash_id": 5116876568086371573, "entities": ["factchecking models", "adversarial datasets", "annotated training data"], "background": "1. The need to improve the robustness of fact-checking models against adversarial and out-of-domain datasets, which current supervised models struggle with due to overreliance on specific training data. 2. The desire to reduce the dependency on large amounts of annotated training data, which is expensive and time-consuming to create, and can encode biases and dataset-specific idiosyncrasies into the models."}
{"hash_id": 6503269984686517902, "entities": ["limited labeled data", "kgqa", "fewshot learning", "multihop queries"], "background": "1. The limited availability of labeled data for KGQA makes it challenging to train LLMs effectively, and existing methods that perform well require extensive training on specific knowledge graphs, which is complex and costly. 2. LLMs have shown promise in few-shot learning, but there has been limited exploration of how to design few-shot examples that guide LLMs to generate accurate multi-hop queries for KGQA."}
{"hash_id": 2451548788628334754, "entities": ["handcoded pooling functions", "documentlevel relation extraction", "datadriven learning approach"], "background": "1. The need to overcome the limitations of hand-coded pooling functions which are model-dependent, task-specific, and require significant resources to determine. 2. The desire to increase the flexibility and customizability of information aggregation in document-level relation extraction while still maintaining a data-driven learning approach."}
{"hash_id": 8512251180416248459, "entities": ["domainspecific qa", "realworld benchmarks", "ragqa performance"], "background": "1. The lack of suitable benchmarks that effectively simulate real-world scenarios for evaluating domain-specific product question-answering systems. 2. The need to improve the performance of existing models on domain-specific QA tasks, as generic RAG-QA approaches often struggle to deliver satisfactory results."}
{"hash_id": 1864865450008536615, "entities": ["enhance llm reliability", "critical applications", "factual accuracy", "trust in llms"], "background": "1. The need to enhance the reliability and accuracy of LLMs in critical applications, such as clinical or legal scenarios, where factual accuracy is crucial. 2. The desire to improve the overall trust in LLMs by reducing their tendency to generate factually incorrect content, which can mislead users and compromise the models' integrity."}
{"hash_id": 2601394435239092090, "entities": ["augment fans critique ability", "social networking analysis", "implicit reasoning", "external information integration"], "background": "1. The need to augment fans' ability to critique and explore information related to celebrities by aggregating and interpreting the vast amount of opinions and impressions available on social networking services like X (formerly Twitter). 2. The recognition that the reasoning behind these impressions is often implicit and not fully detailed in the posts themselves, requiring the integration of external information sources to gain a comprehensive understanding."}
{"hash_id": 4758927541593725788, "entities": ["retrievalaugmented generation", "redundant knowledge", "refining strategies", "computational expense", "efficient solution"], "background": "1. The existing Retrieval-Augmented Generation (RAG) systems are vulnerable to redundant knowledge within fixed-size passage-level retrieved documents, which can negatively affect the performance of the system by distracting the LLMs with irrelevant information. 2. Current refining strategies for retrieved documents generally require additional training steps, which can be computationally expensive and not scalable, thus necessitating a more efficient solution that does not rely on additional training."}
{"hash_id": 1949442878766880060, "entities": ["ralms", "unanswerable queries", "conflicting information", "finetuning robustness"], "background": "1. The existing Retrieval-Augmented Language Models (RALMs) face challenges in handling unanswerable queries where the correct answer is not present in the retrieved contexts, and in dealing with conflicting information from different sources. 2. Current approaches often require extensive fine-tuning to improve robustness, which can be resource-intensive and may not be practical or efficient."}
{"hash_id": 610572033852774059, "entities": ["evaluation of llms", "phonological skills", "realworld applications", "speech data", "joint language understanding"], "background": "1. The need to evaluate LLMs on their phonological skills due to the growing number of real-world applications that require joint understanding of written and spoken language. 2. The observation that LLMs, despite being trained on large-scale text data, may not inherently acquire robust phonological skills without access to speech data, potentially leading to suboptimal performance in tasks that involve phonology."}
{"hash_id": 15618163399743835, "entities": ["mcqa leaderboard rankings", "llms knowledge", "contrast sets construction"], "background": "1. The need to ensure that MCQA leaderboard rankings reliably reflect the knowledge of LLMs, rather than their ability to exploit choices-only shortcuts, which could otherwise undermine the validity of the rankings. 2. The recognition that contrast sets, typically built through manual annotation or potentially biased model-generated data, can be difficult and costly to create, necessitating a more efficient and unbiased method for their construction."}
{"hash_id": 4860710461335131181, "entities": ["factual inconsistencies", "large language models", "inconsistency detection"], "background": "1. The existing traditional models are struggling to identify factual inconsistencies, especially those that are very close to facts, leading to potential hallucinations in the summaries generated by Large Language Models (LLMs). 2. Previous attempts to use LLMs for inconsistency detection have underperformed due to their limited ability to follow instructions and the absence of an effective detection methodology tailored to the specific requirements of different benchmarks."}
{"hash_id": 2108820033105048446, "entities": ["enhancing llm taskhandling", "multimodal problemsolving", "artificial general intelligence"], "background": "1. The growing demand for enhancing the task-handling capabilities of Large Language Models (LLMs) to address the multimodal and dynamic nature of real-world information, as people increasingly rely on LLMs for daily challenges. 2. The realization that improving LLMs with multimodal problem-solving skills could be a significant step towards the realization of Artificial General Intelligence (AGI)."}
{"hash_id": 8205363200982422041, "entities": ["inefficiency of pretraining", "adaptability in language models", "decoupling knowledge model"], "background": "1. The inefficiency of traditional pre-training and fine-tuning methods due to the increasing size of language models, which raises hardware and energy issues. 2. The need for improved adaptability, straightforward knowledge editing, and better explainability in language models, which can be achieved by decoupling knowledge and the language model."}
{"hash_id": 9069032750955183336, "entities": ["integration methods", "majority vote", "reliable strategies"], "background": "1. The traditional concatenation approach often results in generating \"unknown\" outputs even when the correct document is among the top-k retrieved passages, indicating a need for more effective integration methods. 2. Current methods, such as majority vote after individual passage input, introduce new challenges where the correct answer does not align with the majority vote, especially as the number of passages increases, necessitating the development of more reliable strategies."}
{"hash_id": 6009973141091094958, "entities": ["limitation of pretrained models", "computational costs", "noncommercial accessibility"], "background": "1. The need to address the limitation of pre-trained large language models in incorporating updated knowledge due to the static nature of their pre-training data. 2. The challenge of high computational costs and resource constraints associated with fine-tuning or continuously pre-training large language models, which\u9650\u5236\u4e86 their accessibility for noncommercial entities and individual researchers."}
{"hash_id": 752397370532191663, "entities": ["domainspecific knowledge", "knowledge workers", "llms enhancement", "ingesting knowledge methods", "disadvantages"], "background": "1. The need to enhance LLMs with domain-specific knowledge to improve their utility for knowledge workers who require accurate and detailed information for professional tasks. 2. The recognition that current methods for ingesting knowledge into LLMs, such as unsupervised pretraining, supervised fine-tuning, and alignment techniques, have significant disadvantages, including incomplete knowledge acquisition, catastrophic forgetting, and high training costs."}
{"hash_id": 2597431953007369705, "entities": ["evaluation methods", "generationbased prediction", "llms capabilities", "nlp benchmarks", "human preferences"], "background": "1. The prevalent probability-based evaluation methods inadequately align with the generation-based prediction approach used by LLMs in real-world scenarios, leading to potential inaccuracies in assessing the models' true capabilities. 2. There is a discrepancy between the rankings of LLMs based on multiple-choice NLP benchmarks using probability-based methods and human preferences for free-text generation outputs, questioning the reliability of such evaluation outcomes."}
{"hash_id": 3794209699699258566, "entities": ["reduce humanannotated training data", "documentlevel relation extraction", "no relation instances"], "background": "1. The need to reduce the reliance on time-consuming and labor-intensive human-annotated training data for document-level relation extraction. 2. The challenge of effectively handling the large number of \"no relation\" instances and improving the performance of pretrained language models in document-level relation extraction."}
{"hash_id": 3432789610998325177, "entities": ["domainspecific llms", "patent response process", "patent language intricacy"], "background": "1. The existing generalized Large Language Models (LLMs) like GPT-4 and LLaMa2 are not domain-specific and struggle with the complexity and intricacy of the patent response process, which requires detailed communication and extensive exchanges of technical and legal knowledge. 2. The distinctive nature of patent language, the uniqueness of each invention, and the intricacy of formulating responses necessitate improvements in patent response systems, as previous LLMs have not significantly enhanced the Office Action and response process."}
{"hash_id": 1690541970901660335, "entities": ["llms vulnerability", "unsafe prompts", "classifierbased approaches"], "background": "1. The current vulnerability of Large Language Models (LLMs) to unsafe prompts, which can lead to responses that engage with illegal or sensitive topics, poses a threat to the safe and ethical use of these models. 2. Existing classifier-based approaches to identifying unsafe prompts have significant drawbacks, such as the need for substantial resources for fine-tuning and inaccuracies in identifying nuanced unsafe prompts."}
{"hash_id": 1861743615049666861, "entities": ["bias and hallucination", "large language models", "ordinal preferences", "human reasoning"], "background": "1. The need to understand and address the bias and hallucination issues that persist in large language models (LLMs), which affect their ability to provide consistent preferential rankings. 2. The importance of ordinal preferences in human reasoning and communication, and the value of aligning LLMs with human inclinations to improve decision-making and the communication of values."}
{"hash_id": 6325339803694381524, "entities": ["retrievalaugmented generation", "multilingual queries", "nonenglish datastores"], "background": "1. The need to extend the benefits of retrieval-augmented generation (RAG) to non-English speakers by testing and possibly adapting existing RAG methodologies for multilingual queries and datastores. 2. The importance of ensuring access to local or culture-specific information for all users of RAG models by retrieving from non-English knowledge datastores, which are often the only source of such information."}
{"hash_id": 4640673590945668074, "entities": ["incorrect information spreading", "large language model hallucinations", "computational resource challenge"], "background": "1. The risk of incorrect information spreading within medical text generation due to large language model hallucinations, which can have dangerous consequences for patients. 2. The computational resource challenge posed by the large size of traditional Language Models, which is not practical for widespread, accessible use."}
{"hash_id": 8266674287726771804, "entities": ["pretrained language models", "structural information", "message passing neural networks", "molecular data"], "background": "1. The recognition that while pretrained language models (LMs) are capable of processing molecular text, there is a gap in research exploring the integration of these models with structural information from molecules, which is crucial for understanding molecular properties and behaviors. 2. The observation that message passing neural networks (MPNNs) have proven to be effective in encoding the structural information of molecules, and thus, there is potential for these models to enhance the capabilities of LMs when dealing with molecular data."}
{"hash_id": 8685631760402272863, "entities": ["accelerate scientific discovery", "complex global challenges", "languagemolecule translation models", "existing approaches", "sparse noisy data"], "background": "1. The need to accelerate scientific discovery in chemistry, which is crucial for addressing complex global challenges such as climate change, healthcare, and pandemics. 2. The recognition that existing approaches to training language-molecule translation models are insufficient due to their reliance on sparse or noisy synthetic data and the requirement for exponentially more data than typically used in NLP tasks."}
{"hash_id": 5029666979321457723, "entities": ["data heterogeneity", "knowledge complexity", "crossmodal relationships", "pharmaceutical sciences", "single model integration"], "background": "1. The need to overcome the challenges posed by data heterogeneity, knowledge complexity, unique objectives, and a spectrum of constraint conditions within the interdisciplinary field of pharmaceutical sciences, which limit the effectiveness of existing large language models. 2. The goal to integrate multiple modalities\u2014nucleic acids, proteins, molecular structures, and natural language\u2014into a single model to capture the rich, cross-modal relationships and functions in pharmaceutical sciences, which are not well addressed by current models that typically handle only two modalities."}
{"hash_id": 5772572310548002436, "entities": ["molecular design", "drug discovery", "materials science", "generating captions"], "background": "1. The challenge of predicting the properties of designed molecules remains difficult, and there is a need for improved methods to tackle this problem in molecular design, which is crucial in various fields such as drug discovery and materials science. 2. The variability in the ways of describing molecular properties in captions makes the task of generating desired captions highly challenging, necessitating a method that can accommodate this diversity in descriptions."}
{"hash_id": 74748254221618390, "entities": ["foundational models", "small molecule applications", "chemical knowledge domains", "resourceintensive pretraining", "domainspecific finetuning"], "background": "1. The need to build foundational models applicable to small molecule applications in various chemical knowledge domains such as biochemistry, electrochemistry, organoleptics, and agricultural chemistry. 2. The reduction of resource-intensive requirements for domain-specific pretraining and fine-tuning, which are currently necessary for models to understand and generate chemistry-related text effectively."}
{"hash_id": 1599541198620584022, "entities": ["statistical approaches", "guiltbyassociation", "rare diseases", "highthroughput sequencing", "genomics data analysis"], "background": "1. The existing statistical and guilt-by-association approaches have limitations in the context of rare diseases due to the scarcity of data and the inability to account for the gene-specific impact of variants. 2. The advent of high-throughput sequencing technologies has led to a vast amount of genomics data that requires advanced computational methods for analysis and interpretation to establish links between genotype and phenotype, especially in the challenging area of rare diseases."}
{"hash_id": 1563173351166390098, "entities": ["drug discovery research", "sample bias problem", "deep generative models"], "background": "1. The need to generate diverse molecules with desired properties to enhance the effectiveness of drug discovery research. 2. The challenge posed by the sample bias problem in existing deep generative models, which leads to the production of structurally similar molecules rather than diverse ones."}
{"hash_id": 6550379017316863921, "entities": ["molecule validity", "smiles representation", "autoregressive models", "error propagation"], "background": "1. The challenge of ensuring molecule validity when generating de novo molecules from textual descriptions, particularly due to issues with the SMILES representation. 2. The limitations of autoregressive models in generating molecular sequences, especially in terms of error propagation and inaccuracies in long sequences."}
{"hash_id": 3646166274299852259, "entities": ["nlp technology evolution", "nonstandard language diversity", "inclusive methods development"], "background": "1. The need to improve NLP technologies to account for the constant changes in languages and dialects over time, ensuring that models remain effective as language evolves, especially in the context of computer-mediated communication and social media platforms. 2. The recognition that communities speaking non-standard language varieties and dialects are often overlooked by existing NLP technologies, which highlights the importance of developing inclusive methods that can address the diversity of language use."}
{"hash_id": 1358497298366888048, "entities": ["word meaning evolution", "historical linguistics", "polysemy homonymy challenges"], "background": "1. The need to understand the evolution of word meanings in historical contexts, which is crucial for linguistics and natural language processing, and can provide insights into cultural and societal shifts. 2. The limitations of traditional manual methods and static word embeddings in capturing the dynamic nature of word meanings, especially in the face of polysemy and homonymy."}
{"hash_id": 7444783447576360816, "entities": ["novel word senses", "costly update process", "computational modeling", "semantic change", "lexicography synchronization"], "background": "1. The motivation to propose these methods stems from the need to efficiently update dictionaries with novel word senses as language and meaning evolve over time, which is currently a costly and time-consuming process for lexicographers. 2. The second motivation is to bridge the gap between computational modeling of semantic change and lexicography, facilitating the synchronization of diachronic word uses with synchronic dictionary entries to improve the accuracy and comprehensiveness of dictionaries."}
{"hash_id": 398283479994514410, "entities": ["linguistic change", "meaning shift", "computational linguistics", "diachronic word sense changes"], "background": "1. The traditional challenge in linguistic inquiry of documenting linguistic change and meaning shift requires fine-grained judgments and extensive historical material surveys, which is labor-intensive and time-consuming. 2. There is a need to develop more ambitious research goals and projects in the field of computational linguistics, specifically automating the annotation and explanation of diachronic word sense changes to advance the field and build bridges between NLP and historical linguistics."}
{"hash_id": 7107974648685664088, "entities": ["annotation effort", "lexical semantic change", "word usage graphs"], "background": "1. The need to reduce the substantial annotation effort required to create reliable benchmarks for Lexical Semantic Change (LSC) over multiple time periods, as traditional word sense annotation is too resource-intensive. 2. The goal to improve the quality of word senses derived from Word Usage Graphs (WUGs) by augmenting human relatedness judgments, which can be limited due to the high number of edges needed to form a complete graph."}
{"hash_id": 2115429371310590457, "entities": ["computational modeling", "semantic change", "diachronic word sense induction"], "background": "1. The current computational modeling of semantic change is overly simplified, focusing only on two time periods and not accounting for the complexity of semantic shifts over a longer duration. 2. The absence of comprehensive benchmarks and diachronic lexicographic resources, along with the noise in unsupervised clustering methods, necessitates a more sophisticated approach to diachronic word sense induction."}
{"hash_id": 2439352676102585472, "entities": ["diachronic semantic change", "computationally efficient detection", "novel senses identification"], "background": "1. The motivation to propose these methods stems from the need to automate the detection and explanation of diachronic semantic change, which is critical for understanding how language evolves over time and for maintaining the relevance of lexical resources. 2. The challenge of accurately identifying novel senses in a computationally efficient manner without relying on expensive manual annotation or extensive linguistic resources drives the development of this approach."}
{"hash_id": 8252327221667542119, "entities": ["contextual meaning", "lexical complexity", "lexical semantic change", "subjective annotation", "objective measures"], "background": "1. The shared focus on contextual meaning in both lexical complexity prediction and lexical semantic change detection suggests a potential overlap that could be exploited to improve the accuracy and reliability of these subjective annotation processes. 2. The subjectivity inherent in the manual annotation of both lexical complexity and lexical semantic change data introduces variability and potential error, which motivates the search for objective measures that could reduce this uncertainty and improve consistency."}
{"hash_id": 4338952616456965431, "entities": ["political discourse manipulation", "online media", "dogwhistles", "computational analysis"], "background": "1. The need to combat the manipulation of meaning and deceptive communication strategies in political discourse, particularly in the fast-paced environment of online media. 2. The ethical problems posed by dogwhistles that convey racist or derogatory attitudes, which are challenges for democratic society and require computational methods for automated analysis due to the implicit nature of these meanings."}
{"hash_id": 210010218869399888, "entities": ["lexical semantic change", "axolotl shared task", "automatic annotation", "target word usages"], "background": "1. The automation of Lexical Semantic Change (LSC) studies is essential to understand how word meanings evolve over time, and the AXOLOTL-24 shared task provides a platform for developing and evaluating methods in this area. 2. The need for automatic annotation of individual usages of target words rather than target words as a whole, which is a more granular and challenging task than previous shared tasks in LSC, necessitates the development of new methods to address this specific requirement."}
{"hash_id": 862791233136215576, "entities": ["prompt engineering", "lexical semantic change detection", "example retrieval algorithms"], "background": "1. The current lack of evaluation of prompt engineering techniques, such as few-shot learning, for improving Lexical Semantic Change Detection (LSCD) performance. 2. The need for a method that uses prompt engineering to build an LSCD model, particularly one that employs example retrieval algorithms to find the most similar language change context pairs compared to input pairs."}
{"hash_id": 2665491504858841700, "entities": ["lowresource language translation", "crosslingual understanding", "datadriven approach limitations", "suboptimal alignments"], "background": "1. The need to expand machine translation capabilities to previously unseen and low-resource languages, where traditional data-driven approaches are limited by the scarcity of training data. 2. The desire to improve the cross-lingual understanding and generation abilities of large language models, which currently struggle with suboptimal cross-lingual alignments due to the lack of data in low-resource languages."}
{"hash_id": 8666793385930987089, "entities": ["linguistic systems", "translation systems", "kpop terminology", "benchmark dataset"], "background": "1. The need to bridge the gap in translation systems that fail to capture the unique linguistic systems and terminologies developed by social groups, as exemplified by the Kpop fandom. 2. The desire to advance research in terminology-based machine translation by providing a benchmark dataset that can be used to test and improve translation models' ability to handle specialized language systems."}
{"hash_id": 4591796510135104213, "entities": ["asl syntactical structures", "sign language translation", "gloss intermediary step"], "background": "1. The complexity of ASL, which includes its own syntactical structures and intricate morphological and phonological properties, necessitates a specialized approach for translation that cannot be directly derived from text-based machine translation systems. 2. The lack of a written form for ASL and the multi-channel nature of sign languages present a unique challenge for translation that requires the use of glosses as an intermediary step, which itself has not yet been perfected."}
{"hash_id": 8954733107249624707, "entities": ["multilingual models", "lowresource languages", "catastrophic forgetting", "nmt performance"], "background": "1. The existing pre-trained multilingual models like mBART-50, which are successful in various NMT tasks, do not support numerous low-resource languages, especially those spoken in regions like the Indian subcontinent, leading to a gap in translation quality for these languages. 2. The process of expanding language support for models like mBART-50 is complex and can result in performance decline due to catastrophic forgetting when new languages are added, necessitating a more efficient and effective approach to enhance NMT for low-resource languages."}
{"hash_id": 6356720354779952751, "entities": ["bilingual resources scarcity", "cantoneseenglish translation", "pivot language research"], "background": "1. The scarcity of bilingual resources for Cantonese-English translation, which hampers the development of quality translation systems for Cantonese. 2. The need to bridge the gap in research regarding the use of Mandarin as a pivot language for improving translation performance between Cantonese and English."}
{"hash_id": 5497182359552910127, "entities": ["tokenisation impact", "morphologically rich languages", "tokenisation inconsistencies", "translation quality"], "background": "1. The recognition that tokenisation, often considered a solved problem for languages like English, can significantly impact the performance of machine translation systems when applied to morphologically rich languages such as Maltese. 2. The observation that existing datasets, such as OPUS-100, contain tokenisation inconsistencies that can lead to suboptimal translation quality and inflated evaluation metrics like BLEU scores."}
{"hash_id": 7118295093005597424, "entities": ["improve machine translation", "lowresource ladin", "val badia variant", "backtranslation models", "translation quality"], "background": "1. The need to improve machine translation for the low-resource Ladin language, specifically the Val Badia variant, which has minimal parallel data available. 2. The desire to understand the impact of different back-translation models on translation quality in a low-resource scenario."}
{"hash_id": 9041130770887857838, "entities": ["indigenous languages translation", "lowresourced languages", "new machine translation methods"], "background": "1. The limited availability of parallel translation examples for indigenous languages necessitates the development of new methods that can effectively utilize the few available resources. 2. The need to harness the powerful capabilities of large language models to address the challenges faced in translating low-resourced languages, where traditional machine translation methods often fail due to the lack of data."}
{"hash_id": 5939075078894031369, "entities": ["machine translation systems", "lowresource languages", "crosssystem comparisons"], "background": "1. The need for a comprehensive evaluation of machine translation systems, particularly for low-resource languages, to determine the best-performing systems and identify their shortcomings. 2. The lack of clarity in the current landscape of translation systems due to the abundance of new systems and the computational demands of NMT, which hinders cross-system comparisons."}
{"hash_id": 5818856421209311806, "entities": ["bert models", "hebrew manuscripts", "orthographical deviations", "manual reconstruction"], "background": "1. The existing BERT models for Hebrew struggle to fill gaps in manuscript texts due to the many orthographical deviations found in these manuscripts. 2. Hebrew Studies scholars are faced with the laborious task of manually reconstructing missing words in damaged texts, which is time-consuming and may not always yield accurate results."}
{"hash_id": 5328530596623928659, "entities": ["neural methods", "ancient coptic manuscripts", "textual reconstructions"], "background": "1. The limited application of neural methods in the reconstruction of ancient Coptic manuscripts, despite their potential to augment traditional qualitative methods. 2. The need to assist scholars in ranking the likelihood of textual reconstructions within lacunae, given the frequent damage and missing text in these valuable historical documents."}
{"hash_id": 1126121320969987659, "entities": ["lack of labeled samples", "character recognition", "oracle bone characters", "effective recognition method"], "background": "1. The lack of labeled samples for training deep models, which is a common challenge in the field of character recognition for ancient scripts like Oracle bone inscriptions. 2. The need for a more effective method to recognize Oracle bone characters due to the challenges posed by the poor condition of the oracle bones, including damage, contamination, and noise, which were not satisfactorily addressed by previous methods."}
{"hash_id": 4969896990148192856, "entities": ["morphological tagging", "inflectional languages", "transformer models", "nlp methods"], "background": "1. The need for specialized techniques to handle the morphological tagging of inflectional languages with small corpora, which are not well-served by traditional NLP methods designed for analytic languages like Modern English. 2. The uncertainty surrounding the best approach to utilize transformer models for morphological tagging in these languages, given their promising potential but untested adaptations."}
{"hash_id": 8975278840638456530, "entities": ["factual accuracy", "longtail knowledge domains", "rag models", "niche domains"], "background": "1. To enhance the factual accuracy and reduce hallucinations in LLMs when dealing with long-tail knowledge domains, such as ancient Indian philosophy, which are not well-represented in pre-training data. 2. To leverage the potential of RAG models to provide verified, authentic sources for questions in niche domains, thereby improving the reliability of responses for end-users."}
{"hash_id": 3095278071077275615, "entities": ["computational stylometric methods", "premodern languages", "feature calculation", "homonyms", "semantic ambiguity"], "background": "1. The need to improve the accuracy of computational stylometric methods in premodern languages like Latin, which are challenged by a lack of computational resources for tasks such as part-of-speech tagging and semantic disambiguation. 2. The recognition that hand-curated feature sets may be insufficient due to their insensitivity to homonyms, semantic ambiguity, and other complexities of natural language, necessitating a more nuanced approach to feature calculation."}
{"hash_id": 4731532820932404690, "entities": ["opendomain dialogue evaluation", "outdated datasets", "modern large language models", "fluency and relevance", "coherence and commonsense"], "background": "1. The current evaluation benchmarks for open-domain dialogue evaluation rely on outdated datasets and chatbots that do not reflect the capabilities of modern Large Language Models, leading to evaluations that are not accurate or meaningful for contemporary chatbot performance. 2. The traditional focus on Fluency and Relevance as primary quality aspects is no longer sufficient for assessing the performance of modern chatbots, which are now capable of producing fluent and relevant responses but still struggle with more complex aspects such as Coherence and Commonsense."}
{"hash_id": 5214445412381163296, "entities": ["humanlabelled data", "supervised learning", "scalable solutions", "new intents", "task definitions"], "background": "1. The need to reduce the time-consuming and labor-intensive process of acquiring human-labelled data for a predefined set of intents in supervised learning methods. 2. The requirement for scalable solutions that can handle the addition of new intents or changes in task definitions without the need for retraining with new labelled data."}
{"hash_id": 4831411515236439943, "entities": ["taskoriented dialogue systems", "user familiarity", "system robustness", "conversational tasks"], "background": "1. The recognition that existing task-oriented dialogue (TOD) systems are unstable and lack robustness due to the naive assumption about user familiarity during data collection. 2. The need to understand how the familiarity of users with TOD systems' capabilities influences the successful completion of conversational tasks in realistic scenarios."}
{"hash_id": 5712234753089222666, "entities": ["conversational interfaces", "search paradigm", "information retrieval", "holistic inspection", "theoretical requirements"], "background": "1. The increasing prevalence of conversational interfaces and their adoption of a search paradigm that challenges traditional information retrieval approaches, necessitating a better understanding of the engineering process. 2. The need for a more holistic inspection that connects theoretical requirements with realizable functional components due to the complexity of proposed systems and the gap in practical implementations."}
{"hash_id": 4244643246430925122, "entities": ["static hard negative sampling", "dynamic hard negative sampling", "models changing state", "suboptimal performance", "high computational costs"], "background": "1. The need to overcome the limitations of static hard negative sampling, which does not adapt to the model's changing state during training, resulting in sub-optimal performance. 2. The challenge of the high computational costs associated with dynamic hard negative sampling that restricts its applicability to certain model architectures, particularly those with slower inference speeds."}
{"hash_id": 8340942255122151438, "entities": ["improve reasoning accuracy", "large language models", "external verification"], "background": "1. The need to improve the reasoning accuracy of Large Language Models (LLMs) without necessitating additional model training. 2. The potential to enhance the reliability of LLM-generated solutions by verifying them through an external method."}
{"hash_id": 6981704592282970185, "entities": ["answer calibration", "multistep reasoning", "large language models"], "background": "1. The existing literature lacks systematic analysis on different answer calibration approaches for multi-step reasoning, indicating a gap in understanding and optimizing this aspect of reasoning processes in Large Language Models (LLMs). 2. Answer calibration is a crucial part of the multi-step reasoning process that can be integrated into path generation models, yet there is a need for a unified evaluation framework to assess the effectiveness of various calibration strategies."}
{"hash_id": 711818479981892687, "entities": ["lightweight llms", "code generation", "reinforcement learning", "rlhf scalability"], "background": "1. The need to improve the code generation abilities of lightweight LLMs, which are limited in size and therefore in their ability to handle complex tasks such as generating code with appropriate API calls. 2. The high cost and scalability issues associated with traditional Reinforcement Learning with Human Feedback (RLHF) approaches due to the requirement for high-quality human feedback."}
{"hash_id": 6766980535092060171, "entities": ["ngrambased metrics", "human evaluations", "large language models"], "background": "1. The existing N-gram-based metrics like BLEU and ROUGE often do not align well with human evaluations, particularly in content assessment, which motivates the need for a new evaluation approach. 2. The reliance of current methods on reference summaries as a \"gold standard\" is limiting, especially when dealing with varied and abstract summaries, which drives the search for a more effective evaluation framework using Large Language Models (LLMs)."}
{"hash_id": 5314139342570448002, "entities": ["limitation of llms", "complex reasoning tasks", "symbolic formulations refinement", "semantic errors"], "background": "1. To overcome the limitations of LLMs in generating accurate intermediate formal specifications for complex reasoning tasks. 2. To improve the refinement process of symbolic formulations, which often introduce semantic errors and require multiple non-linear steps to resolve."}
{"hash_id": 8590299988322820442, "entities": ["numerical computations", "large language models", "toolaugmented llms", "llmcot counterparts", "output text formats"], "background": "1. The existing Large Language Models (LLMs) tend to make mistakes or hallucinations during intermediate numerical computations in complex mathematical reasoning tasks. 2. Tool-augmented LLMs, while improving accuracy, still have limitations and do not consistently outperform their traditional LLM-COT counterparts, partly due to the stringent requirements on output text formats which may influence the reasoning process."}
{"hash_id": 1168041541844910669, "entities": ["information retrieval", "large language models", "secure model", "distributed data"], "background": "1. The need to maintain privacy when performing information retrieval for large language models, as current methods leak query and database content information. 2. The requirement for a secure model that allows LLMs to access and utilize private, distributed data without centralization, especially for retrieval and inference phases."}
{"hash_id": 8770749506738354676, "entities": ["privacypreserving techniques", "stereotypical bias", "differential privacy"], "background": "1. The need to understand and mitigate the persistence of stereotypical bias in language models, especially when privacy-preserving techniques are applied. 2. The requirement to balance privacy protection with the reduction of bias in machine learning models, given the scaling issues associated with Differential Privacy (DP) in LMs."}
{"hash_id": 6556785339787906174, "entities": ["privacy leaks", "language models", "privacypreserving techniques"], "background": "1. The need to prevent privacy leaks when using language models as remote services, as current methods may expose sensitive information to untrusted providers or eavesdroppers. 2. The requirement for privacy-preserving techniques that do not rely on access to the remote model's parameters or require significant computational resources on the user's end."}
{"hash_id": 3043963142821987670, "entities": ["semantic coherence", "privatized text outputs", "metric differential privacy", "document length variability", "privacy protections"], "background": "1. The need to enhance the semantic coherence of privatized text outputs, as current word-level Metric Differential Privacy approaches often result in grammatically incorrect or semantically incoherent texts. 2. The goal of introducing variability in the length of privatized documents to improve privacy protections, since word-level perturbations produce documents with the same length as the original, potentially compromising privacy."}
{"hash_id": 2376764782990231677, "entities": ["pii extraction rates", "large language models", "privacy leakage risks", "blackbox llm access"], "background": "1. The varying performance in PII extraction rates and the lack of consensus on evaluating the risk of PII leakage from large language models, which could lead to underestimating the capabilities of realistic adversaries. 2. The need to improve the extractability of PII in the challenging and realistic setting of black-box LLM access to better assess privacy leakage risks."}
{"hash_id": 629273545313158419, "entities": ["personalized llms", "mobile devices", "user data privacy"], "background": "1. The growing need for personalized LLMs on mobile devices due to the generation of substantial amounts of valuable, non-public data. 2. The requirement to maintain user data privacy by performing all data storage and computation exclusively on the device without any data leaving it."}
{"hash_id": 8950190028942121859, "entities": ["unlock stateoftheart performance", "private contexts", "cascade systems", "private data settings"], "background": "1. The need to unlock state-of-the-art performance in private contexts without exposing sensitive data to remote models. 2. The requirement to design cascade systems that can operate in settings with private data, which standard systems do not adequately address."}
{"hash_id": 6683566815346853693, "entities": [], "background": "1. The need to comply with data privacy regulations like the European GDPR while sharing sensitive datasets. 2. The requirement to preserve the utility and performance of the dataset for research purposes without compromising individual privacy."}
{"hash_id": 1280725479713622476, "entities": ["privacy policies", "regulatory frameworks", "natural language processing", "digital privacy"], "background": "1. The complexity of privacy policies hinders users from comprehending how their data is collected, used, shared, and managed, which undermines the effectiveness of regulatory frameworks like GDPR that rely on user understanding. 2. The increasing importance of digital privacy, underscored by high-profile data scandals, necessitates advancements in natural language processing (NLP) to provide tools that can help individuals, companies, and regulators better understand and manage privacy policies."}
{"hash_id": 7444640067065979029, "entities": ["attack methods on llms", "model weights", "manual prompts", "chatgpt adaptability"], "background": "1. The existing attack methods on LLMs either require access to the model weights or lack the precision to control the specific content of the output, which limits their effectiveness and subtlety in potential malicious applications. 2. Current methods often rely on manually crafted prompts that are not only labor-intensive and domain-specific but also quickly become ineffective against updated models like ChatGPT-4, necessitating a more adaptable and robust approach."}
{"hash_id": 3829520015568892677, "entities": ["semantic relations", "word embeddings", "word sense disambiguation", "new word senses"], "background": "1. The need to improve the representation of semantic relations such as hypernym-hyponym relationships in word embeddings, which are not well captured by traditional point-based word vectors. 2. The requirement for word sense disambiguation systems to handle new senses of words that are not predefined in existing inventories, as senses of words evolve over time and new senses emerge."}
{"hash_id": 2542098431381976637, "entities": ["qa system reliability", "unseen domain questions", "domain adaptation methods"], "background": "1. The need to improve the reliability of QA systems in real scenarios by enhancing their ability to answer questions from unseen domains. 2. The requirement to overcome the limitations of existing domain adaptation methods that either rely on synthetic data generation or pseudolabeling, which can be computationally expensive and prone to noise."}
{"hash_id": 3299495815469689212, "entities": ["text ranking models", "outofdomain scenarios", "supervised data scarcity", "relevanceaware queries", "model adaptation"], "background": "1. The need to improve the effectiveness of text ranking models in out-of-domain scenarios where supervised data is scarce. 2. The recognition that diverse, relevance-aware queries can enhance the relevancy representation of texts in unseen domains, thereby aiding in model adaptation."}
{"hash_id": 3679806046867277474, "entities": ["debiasing methods", "output space", "intrinsic bias", "unbiased examples"], "background": "1. Existing debiasing methods primarily operate in the output space and may not effectively reduce intrinsic bias in the model's internal representations, leading to potential reemergence of bias in different settings. 2. There is a need for a method that intrinsically debiases model representations, ensuring that the model learns from unbiased examples and avoids learning spurious correlations that hinder generalization to out-of-distribution data."}
{"hash_id": 6262153003770148807, "entities": ["adversarial training", "class imbalance", "model reliability", "temporal reasoning tasks"], "background": "1. The need to improve adversarial training for natural language understanding by incorporating task-specific label distribution to address class imbalance, which often leads to lower performance on minority classes. 2. The desire to enhance model reliability and controllability in the presence of randomized adversarial perturbations, especially in challenging temporal reasoning tasks."}
{"hash_id": 2360116130716613113, "entities": ["high parameter models", "gpu memory constraints", "storage space requirements"], "background": "1. The increasing number of parameters in modern language models leads to high GPU memory and training time requirements during fine-tuning, making it infeasible for environments with limited computing resources. 2. Fine-tuning large language models requires significant storage space, which is challenging for devices with limited storage capacities, especially when deploying models across multiple tasks."}
{"hash_id": 3344797959692381096, "entities": ["improve ner re tasks", "malaysian english", "languagespecific pretraining"], "background": "1. The need to improve the performance of NER and RE tasks in Malaysian English, which is a low-resource creole language with distinctive linguistic features not well-captured by existing models. 2. The recognition that pretraining language models on language-specific and geographically-focused corpora can lead to significant improvements in handling the nuances of Malaysian English."}
{"hash_id": 8517414793709091467, "entities": ["unsupervised domain adaptation", "generative language models", "continued pretraining", "classification performance"], "background": "1. The need to bridge the gap in applying unsupervised domain adaptation (UDA) methods to generative language models, which have shown significant advancements but are fragile under data distribution shifts. 2. The exploration of Continued Pre-Training (CPT) as a viable method for improving classification performance on the unlabeled target domain, given its success in labeled domain adaptation and its potential to extend the applicability of modern language models."}
{"hash_id": 5013776394161386489, "entities": ["morphological validity", "transformerbased language models", "vocabulary redundancy"], "background": "1. Improving the morphological validity of tokenisations to enhance the performance of transformer-based language models, especially for languages with complex morphology. 2. Reducing vocabulary redundancy and potentially improving model efficiency by eliminating the need for special space symbols that indicate word boundaries."}
{"hash_id": 5225415829200390132, "entities": ["suitability assessment", "kge models", "pretraining settings", "downstream tasks performance"], "background": "1. The need to assess the suitability of KGE models as general-purpose representations of knowledge graphs beyond link prediction. 2. The desire to understand how different pre-training settings affect the performance of KGEs on various downstream tasks."}
{"hash_id": 9069595035607075157, "entities": ["beyond single clusterings", "diverse perspectives", "textbased descriptions", "visionlanguage models"], "background": "1. The need to explore beyond single clusterings in visual data to capture diverse perspectives and user-specific interests. 2. The potential to enhance interpretability and insights from image datasets by utilizing text-based descriptions generated by advanced vision-language models."}
{"hash_id": 5208015647940594350, "entities": ["deductive reasoning performance", "smaller pretrained language models", "alternative to gnns", "multiplechoice question answering"], "background": "1. The need to improve the deductive reasoning performance of smaller pretrained language models like BERT and RoBERTa, which demonstrate inconsistent performance in logical reasoning tasks. 2. The desire to find an alternative to GNNs that is simpler, faster to converge during training, and can perform as well on multiple-choice question answering datasets."}
{"hash_id": 355190469034589800, "entities": ["transformerbased models", "sentence embeddings", "linguistic information", "explainability", "robustness"], "background": "1. The need to understand the internal mechanisms of transformer-based models and how they encode various types of linguistic information in sentence embeddings. 2. The desire to improve the explainability and robustness of neural models by identifying and localizing specific linguistic information within sentence embeddings."}
{"hash_id": 689304793869555630, "entities": ["fewshot learning", "nlp", "limited data adaptability"], "background": "1. The need to improve few-shot learning in NLP without relying on large language models or extensive fine-tuning, given their computational and resource demands. 2. The desire to develop a method that can learn from very limited data (4, 8, 16 examples) per class, mimicking human adaptability to new tasks with minimal exposure."}
{"hash_id": 8597097107956545375, "entities": ["position embeddings", "transformer models", "sequence order"], "background": "1. The need to understand the internal structure and dimensionality of position embeddings in transformer models, which are crucial for encoding sequence order. 2. The potential to improve the efficiency and effectiveness of transformer models by optimizing the use of position embeddings."}
{"hash_id": 6229517585817323424, "entities": ["computational expense reduction", "extreme multilabel classification", "label encoding retrieval"], "background": "1. The need to reduce the computational expense of training deep neural networks in extreme multi-label classification problems due to the large number of output labels. 2. The requirement to improve the label encoding and retrieval capacity, especially when dealing with a high number of class labels."}
{"hash_id": 3105794010699249764, "entities": ["crosslingual sentence embeddings", "semantic leakage", "disentangling semantics"], "background": "1. The need to improve the effectiveness of disentangling semantics and language-specific information in cross-lingual sentence embeddings for better parallel data mining. 2. The desire to address the issue of semantic leakage, where language-specific information unintentionally contaminates semantic representations, hindering the retrieval of embeddings that accurately represent sentence meaning."}
{"hash_id": 4730008690064857145, "entities": ["language models", "pathstar task", "representation generalization", "teacherforcing nexttoken prediction"], "background": "1. To understand and overcome the specific limitations of language models that cause them to fail on seemingly simple tasks like the path-star task. 2. To explore the role of representation and generalization in the context of teacher-forcing and next-token prediction paradigm."}
{"hash_id": 8588324838400270479, "entities": ["whitening operations", "sentence embeddings", "llms", "evaluation platform", "commodity machines"], "background": "1. The inconsistent performance of whitening operations on sentence embeddings from LLMs, which is both model-dependent and task-dependent, necessitates a deeper understanding of their effectiveness. 2. The lack of a comprehensive and accessible evaluation platform for assessing the quality of embeddings from LLMs, particularly on commodity machines, drives the need for improved methods."}
{"hash_id": 158699101516413954, "entities": ["insights generalization", "llm mechanisms evolution", "decoderonly models"], "background": "1. The need to determine if insights gained from studying LLM mechanisms at a single snapshot in time can generalize to models that undergo continuous training or fine-tuning in real-world settings. 2. The lack of research on the evolution of LLM mechanisms, particularly circuits, over the course of pre-training in decoder-only models, which are commonly deployed."}
{"hash_id": 7022639167570784489, "entities": ["targeted multimodal evidence", "scholarly document processing", "ghost claims", "evidential weight", "scientific synthesis"], "background": "1. The lack of datasets designed to help scholarly document processing models retrieve very targeted multimodal evidence (figures or tables) and methodological details that ground scientific claims, which is essential for accurate sensemaking and decision-making. 2. The need to prevent the spread of \"ghost claims\" that lack empirical evidence but are asserted as if they have evidential weight, which can compromise downstream scientific processes and synthesis."}
{"hash_id": 5502073549806424555, "entities": ["related work generation", "multidocument summarization", "structure", "novelty statements"], "background": "1. The current approach to related work generation treats it as multi-document summarization, ignoring the importance of structure and novelty statements which are crucial for readers to understand the context and novelty of the research. 2. Manually writing a well-structured related work section with clear novelty statements is a time-consuming process that requires deep expertise in the field, and thus there is a need for automated methods to assist authors in this task."}
{"hash_id": 6619298175722152419, "entities": ["literature retrieval", "researcher perspective", "large language models", "topicbased alignment"], "background": "1. The existing approaches for literature retrieval and categorization often do not match the researcher's unique perspective and understanding of the field, leading to discrepancies in literature review generation. 2. The high cost and impracticality of using vanilla paid Large Language Models for tasks such as topic-based alignment of scientific papers to research proposals due to the token lengths of scientific articles."}
{"hash_id": 6515535921131975740, "entities": ["hallucinations mitigation", "llm interpretability", "scientific document reasoning"], "background": "1. The need to mitigate the issue of hallucinations in LLMs, where models generate seemingly plausible but factually incorrect information, particularly in the context of scientific document reasoning. 2. The importance of improving the interpretability and verifiability of LLM predictions by ensuring that models can provide accurate and relevant evidence to support their answers in scientific tasks."}
{"hash_id": 1763873926250639101, "entities": ["abstractive citation generation", "nonfactual hallucinations", "manual annotation cts", "interannotator agreement"], "background": "1. The existing abstractive approaches to citation generation, which rely on cited paper abstracts, are prone to generating non-factual hallucinations due to the lack of content from the full text of the cited papers. 2. The manual annotation of cited text spans (CTS) is extremely time-consuming and labor-intensive, and the resulting datasets are small with low inter-annotator agreement, limiting the development of CTS-based citation generation systems."}
{"hash_id": 1364121250816220746, "entities": ["scientific image representation", "metadata enhancement", "joint imagetext retrieval"], "background": "1. The need to improve the representation and retrieval of scientific images and their captions, which are unique in their precision and context compared to natural images. 2. The untapped potential of metadata in scientific publications to enhance the contextual understanding and retrieval performance of joint image-text representations."}
{"hash_id": 2297963838178171767, "entities": ["unified taxonomy", "scientific document classification", "unsupervised methodology", "scientific text peculiarities"], "background": "1. The need for a unified, coarse-grained, non-overlapping taxonomy to uniquely classify scientific documents, which is essential for strategic tasks such as research portfolio management and systematic review processes. 2. The requirement for an unsupervised methodology that avoids the high costs and biases associated with manual annotation while managing the peculiarities of scientific text, especially in the context of abstracts."}
{"hash_id": 3856242893465573901, "entities": ["entity linking", "scientific tables", "scattered context"], "background": "1. The need to improve the accuracy of entity linking in scientific tables by extracting and utilizing more relevant context from the surrounding text. 2. The recognition that existing methods struggle with scattered and incomplete context, which hinders the effectiveness of entity linking in scientific literature."}
{"hash_id": 2502411772001252446, "entities": ["scientific research rigor", "misinformation concern", "zeroshot learning"], "background": "1. The need to assess the rigor of scientific research and address concerns such as misinformation or misinterpretation of scientific output by verifying scientific claims efficiently. 2. The high costs and difficulty associated with manually curating supervision data for training claim verification models, which has inspired the exploration of zero-shot or few-shot learning approaches to reduce these expenses and improve scalability."}
{"hash_id": 5391400443570664991, "entities": ["efficient search functions", "academic databases", "knowledge graph embedding", "linked data", "textual publication information"], "background": "1. The need for more efficient search and recommendation functions in academic databases to support researchers' activities by allowing them to find papers and discover individuals with similar research interests based on their previous interests. 2. The challenge of applying existing knowledge graph embedding methods due to the lack of linked data in some academic databases, which motivates the exploration of alternative methods that can utilize more readily available textual publication information."}
{"hash_id": 2702822195237172170, "entities": ["scientific document representation", "sentencelevel embeddings", "titleabstractbased models"], "background": "1. Existing scientific document representation learning models are restricted to using only the title and abstract, failing to capture the detailed information present in the full text of scientific papers. 2. Current sentence-level embeddings and title-abstract-based models do not adequately represent the intricate details and the various aspects (like background, methodology, and results) of a scientific paper."}
{"hash_id": 5541468669131373845, "entities": ["llms", "tabular data integration", "scholarly documents", "research outcomes"], "background": "1. The existing Large Language Models (LLMs) have limited capability to integrate and reason over tabular data within scholarly documents, which is a crucial aspect of scientific literature understanding. 2. Tables often encapsulate key findings and provide a condensed view of research outcomes, which, when understood, can significantly enhance the performance of LLMs on scientific literature tasks."}
{"hash_id": 7714492276410313628, "entities": ["quote attribution", "literary works", "natural language processing", "chinese literature", "languagespecific nuances"], "background": "1. The manual process of quote attribution in literary works is time-consuming and labor-intensive, and automating it could significantly enhance efficiency and accuracy in textual analysis. 2. Advancements in natural language processing and machine learning have not been fully exploited for quote attribution in Chinese literature, which presents unique challenges due to language-specific nuances."}
{"hash_id": 8831497574218880151, "entities": ["reproducibility in llms", "instruction finetuning", "responsible development"], "background": "1. The need for reproducibility in fine-tuned large language models (LLMs) due to the lack of transparency in the instruction fine-tuning processes of existing models. 2. The desire to foster responsible development and sharing of LLMs, especially in the context of prominent models being restrictive in technological sharing."}
{"hash_id": 1019839622302204128, "entities": ["lowresource languages", "persian", "entity linking"], "background": "1. The existing state-of-the-art studies primarily focus on English knowledge bases and do not adequately address the challenges posed by low-resource and difficult languages, such as Persian. 2. There is a lack of annotated text for many low-resource languages, necessitating a method that can perform effective entity linking with limited labeled data."}
{"hash_id": 4525144836069320467, "entities": ["computational resources", "train large language models", "transferring knowledge", "pretrained models"], "background": "1. The excessive computational resources and time required to train large language models from scratch. 2. The lack of methods for transferring knowledge from smaller pre-trained models to larger ones to improve efficiency and reduce costs."}
{"hash_id": 4075393940009578677, "entities": ["semantic parsing", "universal decompositional semantics", "large language models"], "background": "1. The need to enhance the efficiency and accuracy of semantic parsing tasks, particularly in the context of Universal Decompositional Semantics (UDS), which is crucial for natural language understanding. 2. The emergence of large language models (LLMs) like ChatGPT suggests potential advancements in semantic parsing, but there is a gap in understanding their specific impact on UDS tasks."}
{"hash_id": 4155126948989170714, "entities": ["automatic conversation understanding", "annotated data", "longdistance interactions"], "background": "1. The extensive amount of online conversations creates a need for automatic conversation understanding to structure discussions and find necessary information. 2. Existing methods often rely on annotated data or assumptions that do not capture the complexity of long-distance interactions and informal conversation styles."}
{"hash_id": 8706130502324076526, "entities": ["cantonese speakers", "nlp ecosystem", "stateoftheart practices", "transformer architectures"], "background": "1. The significant population of Cantonese speakers worldwide deserves a more developed NLP ecosystem to support their language needs in digital contexts. 2. The current gap in resources and alignment with state-of-the-art NLP practices for Cantonese hinders its integration into the transformative \"pretraining and fine-tuning\" era dominated by Transformer architectures."}
{"hash_id": 1945647642942632084, "entities": ["finegrained emotional information", "sentiment analysis", "chinese integration", "semantic understanding"], "background": "1. The need to represent more fine-grained emotional information in sentiment analysis than discrete affective states. 2. The challenge of integrating traditional and simplified Chinese to improve the robustness of review representation and capturing the internal relatedness across multiple dimensions for better semantic understanding."}
{"hash_id": 8440136131394317574, "entities": ["finegrained sentiment analysis", "valencearousal space", "aspectopinion relationships", "imbalanced datasets"], "background": "1. The need for a more fine-grained sentiment analysis approach that moves beyond discrete sentiment polarity to represent affective states as continuous numerical values across multiple dimensions (Valence-Arousal space). 2. The challenge of accurately modeling the relationships between aspect and opinion terms in sentences with multiple aspects and opinions, and addressing the issue of imbalanced datasets which can lead to biased model predictions."}
{"hash_id": 3365592732375520654, "entities": ["finegrained sentiment analysis", "affective states", "fewshot learning", "aspectbased sentiment analysis", "large language models"], "background": "1. The need for a more fine-grained approach to sentiment analysis that can capture both valence and arousal dimensions for each aspect term in restaurant reviews, as traditional methods often focus on discrete sentiment polarity rather than continuous affective states. 2. The challenge of improving the few-shot learning capability of large language models (LLMs) for Aspect-Based Sentiment Analysis (ABSA) tasks, given that LLMs are highly sensitive to the choice of in-context examples and their performance can be limited by the selection and number of these examples."}
{"hash_id": 8010844078664893563, "entities": ["human emotions", "continuous recognition", "llms application", "dimabsa task"], "background": "1. The recognition that human emotions are inherently continuous and require a more nuanced approach than the traditional discrete sentiment labels. 2. The lack of systematic exploration into the application of Large Language Models (LLMs) for the task of Dimensional Aspect-Based Sentiment Analysis (dimABSA), which limits the precision of sentiment analysis on aspects."}
{"hash_id": 2851563410167476693, "entities": ["prompting methods", "finetuning methods", "performanceexplainability balance"], "background": "1. The motivation to address the problem stems from the observation that while prompting methods are more transparent and explainable to humans due to their use of natural language, they often underperform compared to fine-tuning methods in terms of accuracy. 2. Another motivation is the recognition that prompting and fine-tuning are not mutually exclusive, and that there is potential to combine their strengths to achieve a balance between performance and explainability without necessarily compromising one for the other."}
{"hash_id": 7059916088353114952, "entities": ["conversational ai", "personalized response", "longterm memory interactions"], "background": "1. The need to improve personalized and consistent response generation in conversational AI by considering the interaction between different types of long-term memory. 2. The observation that existing work has focused on only one type of long-term memory, ignoring the complex interactions between various types of memory in real-world conversational contexts."}
{"hash_id": 2404175194364296653, "entities": ["emotional information", "sentiment analysis", "sentiment intensity"], "background": "1. The need for more fine-grained emotional information in sentiment analysis to better understand affective states. 2. The limitation of traditional sentiment polarity representation in capturing the nuances of sentiment intensity."}
{"hash_id": 8285354925723298323, "entities": ["aspectbased sentiment analysis", "sentiment dimensions", "continuous realvalued scores", "valencearousal dimensions"], "background": "1. Existing Aspect-Based Sentiment Analysis (ABSA) works typically treat sentiment as coarse-grained polarities, which overlooks the complexity of sentiment dimensions and fails to capture the nuances of user opinions in terms of valence and arousal. 2. The SIGHAN-2024 dimABSA task introduces the novel challenge of representing sentiment states as continuous real-valued scores in valence-arousal dimensions, which requires more sophisticated methods to accurately predict intensity and extract the necessary sentiment elements."}
{"hash_id": 1477942612852010579, "entities": ["unsupervised morphological learning", "interpretability performance", "complex morphologies"], "background": "1. To replicate the natural process by which humans acquire morphological rules without explicit supervision, aligning with the unsupervised learning abilities of humans. 2. To improve the interpretability and performance of unsupervised morphological learning models, particularly for complex morphologies like Turkish, for use in various applications such as spell checking and automatic speech recognition."}
{"hash_id": 1314035183019183388, "entities": ["improve llms performance", "lowresource languages", "indexical shift", "dedicated dataset", "evaluation method"], "background": "1. The need to improve the performance of Large Language Models (LLMs) on linguistic tasks in low-resource languages like Turkish, where unique grammatical challenges such as Indexical Shift are not adequately addressed during their training on data-rich languages like English. 2. The absence of a dedicated dataset and evaluation method for assessing LLMs' ability to understand and resolve indexical pronouns in the context of Indexical Shift, which is a critical aspect of language comprehension."}
{"hash_id": 6968127467236218891, "entities": ["euphemism detection", "turkish nlp applications", "cultural context", "linguistic nuances"], "background": "1. The lack of available datasets for automatic euphemism detection in Turkish hinders the advancement of euphemism research and NLP applications in the language. 2. Euphemisms are a complex form of language that requires a deeper understanding of cultural context and linguistic nuances, which can be better captured by fine-tuned language models."}
{"hash_id": 38609199282233487, "entities": ["inequality between language communities", "llms in lessresourced languages", "evaluating", "enhancing midand lowresource llms"], "background": "1. To bridge the inequality between language communities and the gap between developed and developing countries by evaluating and improving the quality of LLMs in less-resourced languages like Kazakh. 2. To provide insights into the applicability of existing LLMs for Kazakh and to contribute to the methodology of evaluating and enhancing LLMs for mid-and low-resource languages."}
{"hash_id": 3623402112651746058, "entities": ["social media data", "social disorder detection", "advanced pretrained models"], "background": "1. The need to accurately classify and identify social disorders in children and adolescents from social media data to better understand and address their health concerns. 2. The potential to improve the performance of text classification models for social disorder detection by leveraging advanced pre-trained models and data augmentation techniques."}
{"hash_id": 5299743767449475785, "entities": ["llms performance dependency", "lowresource settings", "lowcost solutions", "finetuning enhancement", "realworld applications"], "background": "1. The high dependency of LLMs' performance on the amount of data available for fine-tuning, which makes them less effective in low-resource settings. 2. The need for low-cost solutions to enhance the limited data available for fine-tuning LLMs in order to improve their performance in real-world applications."}
{"hash_id": 917779433652704060, "entities": ["extracting medical entities", "adverse drug events", "social media text", "method without finetuning"], "background": "1. The challenge of extracting medical entities such as adverse drug events (ADEs) from social media due to the unstructured and informal nature of social media text. 2. The need for a method that does not require extensive fine-tuning on downstream tasks, especially when sufficient resources and data for finetuning models like BERT are not available."}
{"hash_id": 1293751480804057295, "entities": ["multilingual named entity recognition", "conventional prompts", "smallscale datasets"], "background": "1. The challenge of named entity recognition (NER) in the context of multilingualism, where traditional models struggle to handle multiple languages and limited data. 2. The poor performance of large language models with conventional prompts in extracting target medical entities from web text, especially when faced with small-scale datasets."}
{"hash_id": 3035960657047671527, "entities": ["entityn social media health data", "public health research", "colloquial language", "medical terms", "advanced methods"], "background": "1. The increasing use of social media as a platform for sharing health-related information presents an opportunity to utilize this data for public health research and to improve health monitoring and response strategies. 2. The challenge of interpreting colloquial language used in social media posts, which often contains medical terms that can be misleading or difficult for standard models to interpret accurately, necessitates the development of advanced methods to improve model performance."}
{"hash_id": 5813308903091491769, "entities": ["misinformation risk", "llms data annotation", "inherent biases", "hallucinations"], "background": "1. The concern that Large Language Models (LLMs) like GPT-4 may generate plausible but false content, leading to the spread of misinformation, false narratives, fake news, and spam, especially in critical domains such as healthcare. 2. The debate within the research community about the potential issues with LLMs in data annotation, including inherent biases and hallucinations, which can affect the reliability and trustworthiness of the annotated data."}
{"hash_id": 8517420097020267782, "entities": ["social media data", "public health implications", "healthrelated information", "targeted interventions", "educational programs"], "background": "1. The need to leverage social media as a rich source of real-time data to understand the public health implications of non-medical substance use and children's health disorders. 2. The requirement for more accurate and nuanced methods to extract and classify health-related information from social media to inform targeted interventions and educational programs."}
{"hash_id": 8960075129567375991, "entities": ["realtime data assessment", "childhood disorders", "data augmentation", "model performance"], "background": "1. The need to explore data sources for assessing the link between pregnancy exposures and childhood disorders in real-time and at scale, which is crucial for early detection and intervention in public health. 2. The recognition that while data augmentation can improve model performance by increasing training data diversity and robustness, there is a critical need to ensure the quality of these augmentations to maintain the original meanings of the tweets."}
{"hash_id": 7005852588125405523, "entities": ["medication monitoring", "nonenglish languages", "adverse drug reaction", "social media content analysis", "patient perspective"], "background": "1. The need for continuous monitoring of medication usage and effects, especially in non-English languages, where there is a lack of representation in existing datasets and tools for adverse drug reaction (ADR) detection. 2. The challenge of extracting entities and relations from the noisy and informal nature of user-generated content in social media, which can provide up-to-date insights from the patient's perspective on ADRs."}
{"hash_id": 5589409561539614419, "entities": ["artificial agents controllability", "human verbal communication", "ai control efficiency", "common world model"], "background": "1. To enhance the controllability of artificial agents by enabling them to be modulated more easily and directly through human verbal communication, rather than solely through observational data. 2. To improve the efficiency of AI control by allowing humans to adjust multiple policies simultaneously through changes to a common world model, as opposed to updating policies individually in a model-free approach."}
{"hash_id": 1422915384207481357, "entities": ["transformer architecture", "academicindustry gap", "machine learning engineers"], "background": "1. To bridge the gap between academic training and industry needs by providing a course that imparts both theoretical knowledge and practical skills in Transformer architecture. 2. To enhance the efficiency and quality of work in industry by ensuring machine learning engineers have a comprehensive understanding of Transformers and their applications."}
{"hash_id": 1315724703777579267, "entities": ["nlp research advancements", "course curricula updates", "inclusive educational resources"], "background": "1. The rapid acceleration of developments in NLP research and the need for frequent updates to course curricula to keep up with these advancements. 2. The desire to create inclusive and adaptable educational resources that can accommodate heterogeneous student groups and different examination regulations across universities."}
{"hash_id": 1999481902339147036, "entities": ["nlp advancements", "large language models", "educational curricula", "multilinguality", "language diversity"], "background": "1. The rapid advancements in NLP and the emergence of Large Language Models necessitate updating educational curricula to ensure that students are proficient with the latest techniques. 2. The diverse multicultural audience at the institution, which includes a high proportion of first-generation immigrants, can benefit from a course that covers multilinguality and language diversity, thereby addressing both educational and cultural needs."}
{"hash_id": 1080787703467266603, "entities": ["empower endangered language speakers", "develop language technologies", "avoid colonialist practices"], "background": "1. The need to empower speakers of endangered languages by enabling them to develop their own language technologies, thereby ensuring that the technologies are aligned with their own needs and that they maintain sovereignty over their language data. 2. The desire to avoid the colonialist practices that have historically been associated with language technology development for endangered languages, including data extraction without consent, over-reliance on outside experts, and the development of tools that do not reflect the priorities of the language community."}
{"hash_id": 6865969772393693773, "entities": ["action understanding systems", "pattern memorization", "temporal context", "action interconnections"], "background": "1. The existing action understanding systems rely heavily on pattern memorization rather than true understanding, which limits their adaptability and fragility in new settings. 2. There is a need for reliable action representations that can capture the temporal context and interconnections between human actions, as current systems do not effectively understand these relationships."}
{"hash_id": 1127119504162734991, "entities": ["graphinformed text representations", "node representations", "selfsupervised pretraining method", "downstream tasks"], "background": "1. The need to overcome the limitations of existing methods that either do not produce graph-informed text representations or fail to learn node representations, thus not fully capitalizing on the benefits of combining text and graph information in text-attributed graphs. 2. The goal of creating a general, self-supervised pretraining method that does not make assumptions about the downstream tasks or require labeled data, which would allow for more efficient training and improved performance across various applications."}
{"hash_id": 2265635286927713627, "entities": ["baseline for parsing", "umr graphs", "amr parsing limitation"], "background": "1. The need to establish a strong baseline for parsing documents into UMR graphs due to the limited size of the available UMR v1.0 corpus. 2. The recognition that while AMR parsing has matured and is used in various downstream applications, it is inherently limited to single sentences and does not fully capture document-level linguistic phenomena."}
{"hash_id": 7152567469844921982, "entities": ["unstructured text document repositories", "retail banking", "lowresource language settings", "spanbased approaches"], "background": "1. The need to efficiently manage and structure the vast amount of unstructured text document repositories within organizations, particularly in specialized domains like retail banking. 2. The challenge of data scarcity and linguistic complexity in low-resource language settings, which traditional span-based approaches struggle to address effectively."}
{"hash_id": 2521470465573713214, "entities": ["automated knowledge graph updates", "endtoend generative models", "labeled corpora training"], "background": "1. The need to automate the process of updating and populating knowledge graphs to keep them relevant and updated without the extensive manual curation that is currently required. 2. The limitations of existing end-to-end generative models, which require large labeled corpora for training and need retraining when the knowledge graph evolves or when working with different ontologies or domains."}
{"hash_id": 6936303257019207518, "entities": ["automated medical codes classification", "healthcare workflows", "interpretable models", "clinical integration"], "background": "1. The need to improve the accuracy and completeness of automated medical codes classification to streamline healthcare workflows, such as research and insurance billing, by incorporating domain knowledge. 2. The requirement for more transparent and interpretable models that allow domain experts to understand the reasoning behind correct and incorrect predictions for better integration into clinical practice."}
{"hash_id": 1827364859354650851, "entities": ["large language models", "hallucinations", "latent space characteristics"], "background": "1. The first motivation is the recognition that large language models (LLMs) are prone to generating hallucinations, which are fabrications that can damage the trustworthiness of the information they provide, affecting decision-making and user confidence. 2. The second motivation is the hypothesis that LLM hallucinations are not unstructured and may share characteristics in the latent space, which suggests that these characteristics could be exploited to detect hallucinations."}
{"hash_id": 5755687253866694079, "entities": ["large language models", "symbolic meaning representations", "semantic graphs", "syntactic simplification"], "background": "1. The growing capability of large language models (LLMs) to solve NLP tasks with instruction-following has led to questioning the utility of symbolic meaning representations like semantic graphs, prompting a need to reassess their role and effectiveness in tasks that require deep semantic understanding. 2. The limitations observed in directly appending semantic graphs like AMR to the input of LLMs, which have been found to be either not beneficial or harmful in many tasks, motivate the exploration of alternative ways to integrate these representations with LLMs for improved performance in syntactic simplification."}
{"hash_id": 5856567067434545796, "entities": ["mcqa tasks", "logical coherence", "structured knowledge graphs"], "background": "1. The challenge of current state-of-the-art models, such as BERT and its variants, in effectively solving MCQA tasks due to their inability to fully consider logical coherence in addition to grammatical construction when predicting answers. 2. The need to improve the efficiency and performance of LLMs in retrieval tasks by integrating structured knowledge from graphs, which can provide logical connections that are not captured by text alone."}
{"hash_id": 5423577888020854695, "entities": ["kbqa systems", "factoid questions", "knowledge graphs integration"], "background": "1. The current challenge in KBQA systems to accurately answer factoid questions that require precise, context-specific information. 2. The need for effective integration of structured, contextual data from knowledge graphs with text-based language models to enhance the contextual grounding of answers."}
{"hash_id": 5174871173549649914, "entities": ["traditional kgqa methods", "large language models", "factual hallucination"], "background": "1. Traditional Knowledge Graph Question Answering (KGQA) methods, such as knowledge retrieval and semantic parsing, struggle with the complex reasoning required to answer natural language questions. 2. Recent methods leveraging large language models (LLMs) for KGQA still face issues with factual hallucination during reasoning processes, leading to inaccuracies in answers."}
{"hash_id": 8864422881691965690, "entities": ["emotion recognition in conversation", "subjective nature", "conversational context", "efficient models", "label granularities"], "background": "1. The existing challenge in Emotion Recognition in Conversation (ERC) due to the subjective nature of human emotions and the difficulty in representing conversational context accurately. 2. The need for efficient models that can generalize beyond the basic emotions and adapt to different label granularities without the computational expense associated with current state-of-the-art models."}
{"hash_id": 4568527121774569202, "entities": ["target identity bias", "toxicity detection", "model generalization"], "background": "1. Models can learn to associate specific identity terms with toxicity due to the biased nature of training corpora, leading to limited generalization to new or less frequent targets. 2. Existing methods of correcting for target identity bias may not provide the necessary specificity to improve model performance and reduce bias without compromising on the detection of toxicity."}
{"hash_id": 1256589179755191013, "entities": ["performance degradation", "temporal shifts", "social media tasks"], "background": "1. The need to understand and mitigate the performance degradation of language models on social media tasks due to temporal shifts, which are common in dynamic streaming data environments. 2. The lack of a standardized evaluation scheme to assess the adaptability of language models to temporal changes specifically on social media platforms like Twitter."}
{"hash_id": 9050396225079813542, "entities": ["compound aspectbased sentiment analysis", "large language models", "llamabased models"], "background": "1. The need to improve the performance of large language models in compound aspect-based sentiment analysis (ABSA) tasks, where they currently lag behind fine-tuned models. 2. The potential of fine-tuned large language models, such as LLaMA-based models, in ABSA tasks has not been fully explored or realized."}
{"hash_id": 7994861774742539229, "entities": ["misinformation proliferation", "user behavior", "languagespecific features"], "background": "1. The need to better understand user behavior to effectively combat the proliferation of misinformation on social media platforms like Twitter. 2. The limited research on the agents that share misinformation, particularly when it comes to analyzing language-specific features that could differentiate between misinformation spreaders and other users."}
{"hash_id": 3576251221811682236, "entities": ["safety ethical alignment", "llms", "contextual meaning", "generative ai biases", "informational integrity"], "background": "1. The need to ensure safety and ethical alignment in LLMs without compromising the contextual meaning of the generated content, as traditional approaches often\u727a\u7272\u4e86\u5185\u5bb9\u7684\u5b8c\u6574\u6027\u4ee5\u8ffd\u6c42\u5b89\u5168\u6027\u3002 2. The rise of ethical and safety challenges due to the misuse and biased outputs of generative AI, necessitating innovative solutions to mitigate biases while maintaining informational integrity."}
{"hash_id": 6970066212684403749, "entities": ["crosslingual sentiment analysis", "large language models", "limited annotated data"], "background": "1. The need to understand and evaluate the performance of large language models in cross-lingual sentiment analysis, particularly in comparison to smaller multilingual language models, given the recent advancements in LLMs. 2. The requirement to identify the most effective model for cross-lingual sentiment analysis in scenarios with limited annotated data, such as zero-shot and few-shot learning."}
{"hash_id": 2521783503669467048, "entities": ["public discourse monitoring", "healthrelated issues", "social media data analysis", "sentiment dynamics"], "background": "1. The need to monitor and understand the public discourse on health-related issues, such as face mask usage, during the COVID-19 pandemic to inform effective communication and mitigate the spread of misinformation. 2. The requirement for a more nuanced and structured approach to analyze large volumes of social media data, which can reveal long-term narratives and sentiment dynamics."}
{"hash_id": 7594091618282808465, "entities": ["emotions in written texts", "nonconversational data", "emotional content", "perceived complexity"], "background": "1. The need to improve the analysis of emotions in nonconversational written texts, which is less developed in NLP compared to conversational data, to better understand the emotional content used by authors to engage readers and contribute to text complexity. 2. The recognition that emotions expressed in different modes (direct vs. indirect) can influence the perceived complexity of a text and that current NLP approaches often overlook this aspect."}
{"hash_id": 7651414866627451483, "entities": ["opinion measurement method", "user stances", "nlp advancements", "practical needs"], "background": "1. The need for an opinion measurement method that can assess user stances on multiple topics simultaneously with high temporal and topical resolution, and minimal cost and time investment. 2. The existence of a gap between the latest NLP advancements and the practical needs of analysts and social science researchers for a method that \"works\" in measuring user opinions."}
{"hash_id": 6776631946388655916, "entities": ["media credibility", "opiniondriven articles", "factchecking", "subjective language"], "background": "1. The increased skepticism towards media credibility due to the prevalence of opinion-driven articles that can sway readers' beliefs. 2. The need for improved fact-checking to counter the rise of subjective language in news reporting, which is often used in fake news and misleading articles."}
{"hash_id": 4016023868741440290, "entities": ["lowresource languages", "codeswitched language data", "sentiment emotion detection"], "background": "1. The need to effectively analyze sentiment and emotion in low-resource languages, which are underrepresented in NLP research but are widely used in social media platforms like Twitter. 2. The importance of understanding and utilizing code-switched language data, which is a common phenomenon in multilingual societies such as Kenya, to improve the accuracy of sentiment and emotion detection systems."}
{"hash_id": 4385150608307146555, "entities": ["fake news", "trust erosion", "polish language detection"], "background": "1. The significant threat of fake news in Poland, which erodes trust in institutions and manipulates public opinion. 2. The lack of comprehensive datasets and tools specifically designed for fake news detection in the Polish language."}
{"hash_id": 1132806871784505678, "entities": ["human context incorporation", "group attributes", "individual traits", "pretrained models", "task performance impact"], "background": "1. The need to improve the effectiveness of pre-trained language models by incorporating the human context that influences language production, which is currently missing from these models. 2. The desire to understand the relative strengths and complementarity of using group attributes versus individual traits in pre-trained language models and how they impact performance on different tasks."}
{"hash_id": 5154307807289307192, "entities": ["targeted sentiment analysis", "news headlines", "large language models", "prompt design"], "background": "1. The need to understand and predict the targeted sentiment in news headlines, which can significantly impact public opinion and information perception, while navigating the complexities of subjective annotation paradigms. 2. The potential to improve the performance and applicability of large language models (LLMs) in targeted sentiment analysis without the need for extensive fine-tuning or annotated datasets, by optimizing prompt design."}
{"hash_id": 6460286522251753114, "entities": ["sophisticated dialogue systems", "mental healthcare applications", "standardized datasets", "affectrelated phenomena"], "background": "1. The need for more sophisticated dialogue systems that can understand and express empathy and personality, which is crucial for applications in mental healthcare and human-AI interactions. 2. The lack of standardized datasets and clear definitions of empathy that can be used to model complex affect-related phenomena in conversational agents."}
{"hash_id": 8604692040193119018, "entities": ["empathy distress prediction", "psychological constructs", "contextdependent expressions"], "background": "1. To improve the accuracy of empathy and distress prediction by LLMs, which currently may have differing nuances for these psychological constructs compared to the task definitions. 2. To account for the context-dependent nature of empathy and distress expressions, which vary based on the communication context and individuals involved."}
{"hash_id": 6487459607743984256, "entities": ["improve language model performance", "predict empathy emotion scores", "encoder framework limitations", "challenging multimodal data"], "background": "1. The need to improve the performance of large language models in predicting empathy and emotion scores accurately, especially in challenging multi-level and multimodal data environments. 2. The requirement to address the limitations of traditional encoder frameworks and the issues of limited samples, imbalanced label distributions, and the model's tendency to learn templated outputs rather than the underlying scoring logic."}
{"hash_id": 1545510542942236539, "entities": ["relational information", "personality prediction", "big five traits"], "background": "1. Traditional machine learning models often fail to capture the relational information between entities, which is crucial for accurate personality prediction. 2. Incorporating both semantic content from text and relational information from knowledge graphs can significantly enhance the prediction accuracy for the Big Five personality traits."}
{"hash_id": 6496387051899048611, "entities": ["emotional responses", "computational methods", "emotional patterns"], "background": "1. To better understand and model the emotional responses of empathy and distress in written expressions, which can contribute to improved psychological assessments and support. 2. To leverage computational methods to analyze emotional patterns in text, aiming to provide a basis for future work in emotional intelligence and machine learning."}
{"hash_id": 6151206922117856429, "entities": ["empathy emotion detection", "conversational contexts", "computational approaches", "nuanced models", "emotional responses"], "background": "1. The need to improve the accuracy of empathy and emotion detection in conversational contexts to better understand human interactions and promote well-being. 2. The desire to advance the field of computational approaches to subjectivity, sentiment, and social media analysis by introducing more nuanced models that can predict emotional responses at the speech turn level."}
{"hash_id": 2740575743213554196, "entities": ["empathy emotion prediction", "bertbased models", "wassa shared task"], "background": "1. To improve the accuracy of empathy and emotion prediction in conversation turns, which is essential for applications like human interaction processing and emotional intelligence. 2. To enhance the performance of existing BERT-based models by incorporating advanced training techniques and loss functions, addressing the limitations of current methods in the WASSA shared task."}
{"hash_id": 8166752640623950529, "entities": ["figurative language influence", "emotional perception", "conversational contexts", "automated empathy detection", "empathic conversations dataset"], "background": "1. The need to improve the understanding of how figurative language influences emotional perception in conversational contexts, which is essential for advancing social tools like chatbots and therapeutic applications. 2. The recognition that current research on automated empathy detection in back-and-forth conversations is scarce, and there is an opportunity to explore this domain with the Empathic Conversations dataset provided by the WASSA shared task."}
{"hash_id": 8978401526505295249, "entities": ["empathetic agents", "opinion mining", "wassa shared task"], "background": "1. The need for more effective and empathetic agents in various applications such as opinion mining, marketing, customer support, and therapeutic practices. 2. The opportunity to benchmark and improve upon existing approaches to empathy and emotion prediction in conversation through the WASSA shared task."}
{"hash_id": 2856393917819781579, "entities": ["crosslingual emotion detection", "large language models", "customer service mental health"], "background": "1. The need to accurately detect emotions in texts across different languages to enhance global digital communication in various domains such as customer service and mental health assessments. 2. The desire to improve upon existing methods by leveraging the power of large language models to achieve superior performance in cross-lingual emotion detection."}
{"hash_id": 8710312554618081420, "entities": ["multilingual emotion detection", "scarce training data", "monolingual and multilingual models", "accuracy and explainability"], "background": "1. The need to improve performance and interpretability in multilingual emotion detection, especially in languages with scarce training data. 2. The desire to leverage the strengths of both monolingual and multilingual models to overcome the limitations of current approaches in terms of accuracy and explainability."}
{"hash_id": 4328931700394812877, "entities": ["crosslingual emotion detection", "specific emotiontriggering words", "multilingual contexts"], "background": "1. The need for improved accuracy in cross-lingual emotion detection to better understand and predict emotions across different languages. 2. The requirement for an effective method to identify specific words that trigger emotions in multilingual contexts, which is essential for enhanced natural language processing."}
{"hash_id": 5465538102860988042, "entities": ["crosslingual emotion detection", "semantic nuances", "model explainability", "interpretability"], "background": "1. The need to improve the performance of cross-lingual emotion detection in tweets, which is challenging due to the semantic and emotional nuances across diverse cultures and languages. 2. The importance of model explainability and interpretability, especially with the rise of large language models and the necessity to understand their predictions."}
{"hash_id": 2800631346620064569, "entities": ["crosslingual emotion detection", "interpretable detection systems", "emotion information transfer"], "background": "1. The need to improve the performance of cross-lingual emotion detection in the face of limited and imbalanced training data, cultural and linguistic differences, and the opacity of pre-trained language models. 2. The goal to develop interpretable and explainable emotion detection systems that can transfer emotion information across languages and identify emotion triggers effectively."}
{"hash_id": 47517029127662469, "entities": ["quantify semantic content", "linguistic realization", "scalable measurement", "influence complexity"], "background": "1. The need to quantify and understand the relative contributions of semantic content and linguistic realization in NLP tasks to improve their effectiveness and accuracy. 2. The requirement for a scalable method to measure the influence of input elements on the complexity of multiple-choice questions and the sentiment classification output distribution."}
{"hash_id": 3977139230905089564, "entities": ["unseen topic generalization", "large language models", "large multimedia models", "stance detection"], "background": "1. The need to infer stance without the requirement of prior examples of stance attribution, enabling the model to generalize to unseen topics and frames of communication. 2. The desire to leverage the reasoning capabilities of Large Language Models (LLMs) and Large Multimedia Models (LMMs) to handle complex judgments involved in stance detection, particularly when dealing with both text and images."}
{"hash_id": 6634694853138883386, "entities": ["dataefficient multimodal generation", "large language models", "alignment processes"], "background": "1. The need for a more data-efficient and high-quality multimodal generation model that overcomes the limitations of current models relying on encoders like CLIP or ImageBind. 2. The desire to fully utilize the semantic understanding and reasoning capabilities of large language models (LLMs) in the context of multimodal generation, without the information loss that can occur in alignment processes."}
{"hash_id": 3698010715760430091, "entities": ["multidomain datasets", "general persuasion model", "user intents", "persuasive strategies"], "background": "1. The lack of large-scale multi-domain datasets hinders the development of a general persuasion model capable of multi-turn following and planning. 2. Existing models struggle with understanding user intents and applying effective persuasive strategies across different domains."}
{"hash_id": 660247478726251300, "entities": ["software verification", "automated proof generation", "large language models"], "background": "1. The need to automate the software verification process to reduce the significant resources and manpower required for writing and verifying formal proofs. 2. The potential of large language models (LLMs) to improve the efficiency and effectiveness of automated proof generation in software verification."}
{"hash_id": 6303934231624438488, "entities": ["improve code generation", "multilingual scenarios", "universal intermediate representation"], "background": "1. The need to improve the accuracy of code generation in multilingual scenarios, as current methods like chain-of-thought prompting have limitations in effectively translating or generating code. 2. The potential to bridge the gap between natural language prompts and executable code by using a universal, programming language-agnostic intermediate representation."}
{"hash_id": 5052186173831805136, "entities": ["enhance large language models", "crucial memories", "retrievalaugmented generation", "database partitions"], "background": "1. The need to enhance the performance of Large Language Models by focusing on crucial memories and reducing noise introduced by a whole-database retrieval approach. 2. The potential to improve various aspects of retrieval-augmented generation, including index structure construction, privacy data protection, and support for distributed processing, through the use of database partitions."}
{"hash_id": 3660220477154396802, "entities": ["computational expense reduction", "resourceefficient finetuning", "limited labeled data"], "background": "1. The need to reduce computational expense and resource requirements when fine-tuning large pre-trained language models for generating natural language explanations. 2. The requirement for a method that can effectively generate predictions and natural language explanations with limited labeled training data."}
{"hash_id": 7756504419861467555, "entities": ["handling ambiguous emotional data", "majorityvote labels", "nuanced emotion representation"], "background": "1. The need to improve the handling of ambiguous emotional data, which is often excluded from training datasets, leading to decreased performance in real-world applications. 2. The desire to capture fine-grained emotional distinctions by moving beyond majority-vote labels to a more nuanced representation of emotion."}
{"hash_id": 8703457958581882783, "entities": ["semantic relationships", "multihop reasoning", "knowledge graphs incompleteness"], "background": "1. The need to capture semantic relationships between passages for improved multi-hop reasoning in open domain question answering. 2. The issue of incompleteness in existing knowledge graphs, which leads to a lack of critical information necessary for answering certain questions."}
{"hash_id": 663149268876469613, "entities": ["continual relation extraction", "noisy labels", "label inconsistencies"], "background": "1. The need to adapt to emerging novel relations while preserving old knowledge in continual relation extraction (CRE) tasks, especially in the presence of noisy labels which are common in real-world datasets. 2. The limitations of existing CRE methods that assume the correctness of labels in streaming data, leading to significant performance drops when faced with label inconsistencies."}
{"hash_id": 5904322787791819504, "entities": ["evaluation of large language models", "knowledge capabilities", "model sensitivity"], "background": "1. The need for a more reliable and comprehensive evaluation of large language models' knowledge capabilities, as current benchmarks may not account for the models' sensitivity to prompts. 2. The desire to select appropriate language models for practical use by gaining deeper insights into their knowledge boundaries."}
{"hash_id": 285812626945168002, "entities": ["listwise reranking", "zeroshot retrieval", "positional bias"], "background": "1. The need to improve the efficiency of listwise reranking in zero-shot retrieval tasks, especially in comparison to the quadratic time complexity of pairwise reranking methods and the inefficiency of large language models. 2. The requirement to overcome positional bias in listwise reranking, which can lead to suboptimal solutions due to a tendency to favor passages presented at the beginning or end of the list."}
{"hash_id": 334124295822012295, "entities": ["improve multilingual pretrained models", "crosslingual transfer tasks", "script barrier"], "background": "1. The need to improve the performance of multilingual pretrained language models on crosslingual transfer tasks, especially for low-resource languages with different scripts. 2. The desire to overcome the script barrier where different scripts are located in different subspaces, leading to suboptimal crosslingual transfer."}
{"hash_id": 6971761987967713053, "entities": ["unsupervised syntactic language models", "branching biases", "multimodality support"], "background": "1. The need to develop unsupervised syntactic language models that can scale to large datasets and learn explicit compositions in language, similar to human language understanding, without relying on annotated parse trees. 2. The desire to overcome the limitations of previous models which have branching biases, data dependencies that impede parallel training, and are not interpretable or flexible enough for multi-modality support."}
{"hash_id": 6128334567883680244, "entities": ["large language models", "ushaped performance", "summarization tasks", "context utilization"], "background": "1. Large language models exhibit a U-shaped performance pattern, favoring the initial and final segments of the input context, which may lead to crucial content being ignored in the middle, especially in summarization tasks. 2. There is a need to improve the efficiency of context utilization in summarization to ensure that salient information is not overlooked and is accurately reflected in the generated summaries."}
{"hash_id": 7166572597111335037, "entities": ["improve credibility", "intext citations", "smaller llms", "effective training approaches"], "background": "1. The need to improve the credibility of responses from Large Language Models by including accurate and relevant in-text citations. 2. The unsatisfactory performance of previous methods, especially on smaller LLMs, which prompts the search for more effective training approaches."}
{"hash_id": 6528669070428966704, "entities": ["semantic entity recognition", "entity boundaries", "document text representation", "visuallyrich documents"], "background": "1. The need to improve the performance of semantic entity recognition tasks in documents by better considering entity boundaries, which are often ignored by existing models. 2. The requirement for a more detailed analysis of the document text representation to enhance the understanding of visually-rich documents, which contain multi-modal and discrete information."}
{"hash_id": 3011418007368325796, "entities": ["llm mathematical understanding", "llm reasoning robustness", "question formulation variations"], "background": "1. The need to assess whether LLMs truly understand and apply mathematical knowledge or simply rely on shortcuts for mathematical reasoning. 2. The requirement to improve the robustness of LLMs' math reasoning capabilities, especially when faced with variations in question formulation."}
{"hash_id": 3483424273684942616, "entities": null, "background": null}
{"hash_id": 6264117109770620525, "entities": ["long sequence handling", "language models", "multitask benchmark"], "background": "1. The need to rigorously evaluate and improve language models' ability to handle long sequences, such as books and codebases, which are beyond the current capabilities of most models. 2. The absence of a comprehensive, multi-task benchmark specifically designed for assessing performance on long context understanding."}
{"hash_id": 1350654110588932243, "entities": ["questioning capability", "large language models", "educational questions", "teaching abilities benchmark"], "background": "1. The need to evaluate and enhance the questioning capability of large language models as potential educators, which is crucial for guiding learners and assessing students' knowledge levels. 2. The lack of a benchmark that assesses the teaching abilities of LLMs from the perspective of a teacher, rather than a student, focusing on their ability to generate high-quality educational questions."}
{"hash_id": 9102268459730444503, "entities": ["multimodal retrieval models", "textvisual information processing", "joint imagetext representation"], "background": "1. The need to improve the capability of multi-modal retrieval models to process both text and visual information, as current models are mostly text-oriented and lack robust image understanding. 2. The desire to enhance the representation of text-only and image-only data, as well as the joint representation of image-text data, which is largely unexplored in existing models."}
{"hash_id": 661712840333667129, "entities": ["training large language models", "human preferences alignment", "prompt optimization method"], "background": "1. The high cost and inaccessibility of training large language models (LLMs) to align them with human preferences. 2. The need for an interpretable and efficient method to optimize prompts for LLMs without modifying the models themselves."}
{"hash_id": 2645245032765112715, "entities": ["linguistic diversity", "llm evaluation", "korean language", "evaluation framework", "data contamination"], "background": "1. The need to expand LLM evaluation beyond the English language to include linguistic diversity, particularly for Korean. 2. The requirement for a robust evaluation framework that mitigates data contamination and provides fair assessments of LLMs."}
{"hash_id": 5974793457587908650, "entities": ["continuous updates", "knowledgebased datasets", "evolving realworld knowledge", "retrievalaugmented language models", "knowledge trained updates"], "background": "1. The need for continuous updates to knowledge-based datasets to maintain accuracy and relevance in the face of evolving real-world knowledge. 2. The limitation of retrieval-augmented language models (RaLMs) in handling knowledge that has not been trained on or recently updated."}
{"hash_id": 833866866110015492, "entities": ["diversity cultural sensitivity", "texttoimage generation", "creative capabilities"], "background": "1. The need to enhance the diversity and cultural sensitivity of text-to-image generation outputs to make them more globally appealing and adaptable. 2. The desire to improve the creative capabilities of text-to-image generation models by incorporating a wider range of prompts and influences from various cultures."}
{"hash_id": 1004298629334531100, "entities": ["metaphor comprehension abilities", "llms", "crossdomain mappings"], "background": "1. The need to understand and evaluate the metaphor comprehension abilities of LLMs, which are fundamental to human communication and cognitive processes. 2. The lack of comprehensive datasets that test LLMs' ability to interpret metaphors as cross-domain mappings, rather than mere lexical substitutions."}
{"hash_id": 7789520741319728493, "entities": ["improve retrieval augmentation", "unified embedding model", "diverse retrieval tasks"], "background": "1. The need to improve the effectiveness of retrieval augmentation in LLMs, as existing methods are not optimized for this purpose and lack versatility across different retrieval scenarios. 2. The requirement for a unified embedding model that can handle diverse retrieval tasks without sacrificing performance, addressing the limitations of both general-purpose and task-specific embedders."}
{"hash_id": 6762465152332807284, "entities": ["limitation of linguistic knowledge", "sensory experience", "multimodal sensory inputs"], "background": "1. To probe the limitations of linguistic knowledge acquired by language models in the absence of sensory experience, which may lead to incomplete understanding. 2. To examine the potential \"blind spots\" in language learning due to the lack of multimodal sensory inputs during training."}
{"hash_id": 3305903553398873742, "entities": ["data scarcity", "medical data integration", "diverse medical data sources"], "background": "1. The need to overcome data scarcity in the medical domain by integrating more medical data sources into the pre-training process. 2. The desire to broaden the spectrum of applicable downstream tasks by training models on diverse medical data sources."}
{"hash_id": 8043070128258688362, "entities": [], "background": "1. The need for an explainable evaluation metric that can provide insights into why a particular score is assigned to an image caption, enhancing the understanding of model performance. 2. The desire to reduce the reliance on expensive and sometimes unavailable reference captions for the evaluation of image captions."}
{"hash_id": 5037482067396659, "entities": ["mental manipulation detection", "natural language processing nlp", "manipulative content identification"], "background": "1. The scarcity of resources and research in the field of Natural Language Processing (NLP) on mental manipulation detection, which is essential for protecting potential victims. 2. The insufficiency of existing datasets and state-of-the-art models in identifying and categorizing manipulative content in conversations due to its subtle and context-dependent nature."}
{"hash_id": 7929063829888738995, "entities": ["personalized code generation", "developer productivity", "multiuser coding styles"], "background": "1. The need to generate personalized code that adapts to the specific coding styles of different developers or projects, which can significantly increase developer productivity and reduce code maintenance costs. 2. The lack of research on multi-user personalized code generation, and the challenges in efficiently learning and evaluating coding styles for multiple users."}
{"hash_id": 2565100685959451045, "entities": ["causal direction", "event causality identification", "iterative updating"], "background": "1. The need to consider causal direction in event causality identification to better exploit causal structures and improve accuracy. 2. The potential to enhance the identification process by iteratively updating events' representations based on the directionality and structure of already identified causalities."}
{"hash_id": 5077573465517869677, "entities": ["enhance language model expressivity", "predictive capabilities", "nonparametric sequence embedding"], "background": "1. The need to enhance the expressivity and predictive capabilities of language models beyond the restrictive finite parametric vocabulary space. 2. The desire to improve the generalization and robustness of language models, particularly in out-of-domain datasets, by integrating a nonparametric sequence embedding space."}
{"hash_id": 3535816166350244114, "entities": ["imbalanced data distributions", "clustering degradation", "nid performance"], "background": "1. Most prior works in NID assume uniform intent class distributions, which do not reflect real-world long-tailed and imbalanced scenarios. 2. Existing NID methods suffer from clustering degradation in unbalanced data distributions, leading to poor performance in practical applications."}
{"hash_id": 4779367649771403177, "entities": ["improve accessibility", "technical texts", "information loss", "text simplification"], "background": "1. The need to improve the accessibility of technical texts, especially in high-stake domains like medicine, without sacrificing crucial information. 2. The desire to address the frequent information loss and vagueness that occur during the text simplification process."}
{"hash_id": 227755969603003920, "entities": ["high computational costs", "retrievalaugmented generation", "questionanswering tasks", "retrieval necessity", "llm responses"], "background": "1. The high computational costs and potential performance degradation when using large language models (LLMs) for retrieval-augmented generation in question-answering tasks. 2. The need to accurately determine when retrieval is necessary to avoid introducing irrelevant information and improve the efficiency and quality of LLM responses."}
{"hash_id": 3950359948856065741, "entities": ["weakly supervised video localization", "temporal relations", "annotation process", "efficient approach"], "background": "1. Existing weakly supervised natural language video localization methods often fail to account for the complex temporal relations around a language query, leading to illogical predictions. 2. The annotation process for fully supervised video localization is time-consuming and expensive, necessitating a more efficient weakly supervised approach."}
{"hash_id": 4428284529168162759, "entities": ["language model quality", "linguistic knowledge interconnection", "interpretability research"], "background": "1. The need to understand the quality of language models' processing and their language abilities beyond simple performance metrics, especially when error analysis is not possible due to high model performance. 2. The desire to interpret how linguistic knowledge interconnects and is conceptualized within language models, which is crucial for both interpretability researchers and cognitive scientists."}
{"hash_id": 5259167136231140002, "entities": ["exploration efficiency", "reinforcement learning", "code generation", "executed code segments", "optimization"], "background": "1. The need to improve the exploration efficiency of reinforcement learning in code generation due to the increasing complexity of human requirements resulting in longer code sequences. 2. The requirement to optimize the model more precisely by focusing on executed code segments, as unexecuted code snippets can lead to imprecise optimization."}
{"hash_id": 6403536970275548908, "entities": ["high refusal rate", "benign queries", "overkill phenomenon", "user experience", "safety prioritization"], "background": "1. The need to mitigate the high refusal rate of large language models to benign queries containing harmless but potentially harmful-looking words, which hinders their effectiveness and user experience. 2. The absence of in-depth analysis and solutions for the 'overkill' phenomenon where models prioritize safety to the point of declining useful responses."}
{"hash_id": 929252335484458451, "entities": ["instructionfollowing constraints", "large language models", "varying difficulty benchmark"], "background": "1. The need to assess Large Language Models' proficiency in following specific constraints within instructions, which is essential for real-world applications but often overlooked in existing benchmarks. 2. The requirement for a benchmark that considers varying difficulty levels of instructions, controlled by the number of constraints, to precisely evaluate the upper limit of instruction-following capability in LLMs."}
{"hash_id": 4469434197633411646, "entities": ["connection between operations", "observable behaviors", "residual stream signals", "global features dependencies"], "background": "1. The need to gain a clearer understanding of the connection between the elementary operations of large foundation models and their observable behaviors. 2. The desire to explore the potential for less visible signals in the model's residual stream to play a role in maintaining global features responsible for long-range dependencies without interfering with next token prediction."}
{"hash_id": 5878697745050919299, "entities": ["large language models", "inference cost", "attention algorithms", "redundant memory access"], "background": "1. The high inference cost of large language models (LLMs) due to long system prompts hinders their ability to serve more users within a fixed hardware budget. 2. Existing attention algorithms have redundant memory access issues that slow down LLM inferences, which are known to be memory-bound."}
{"hash_id": 4905987711530031979, "entities": ["improving reasoning chains", "reducing hallucinations", "leveraging external knowledge"], "background": "1. The need to improve the reliability and accuracy of reasoning chains generated by Large Language Models (LLMs) by reducing hallucinations and providing factual and faithful reasoning. 2. The goal to enhance the adaptability of LLMs to complex reasoning tasks without requiring parameter updates by leveraging external knowledge."}
{"hash_id": 3132532979386645578, "entities": ["bridging heuristiccoherent gap", "executability realworld", "ai planning capabilities"], "background": "1. The need to bridge the gap between the heuristic and coherent plans generated by LLMs and the executability of these plans in real-world open domains. 2. The desire to enhance the planning capabilities of AI systems to handle a wide range of actions and tasks in various domains, moving towards more human-like planning."}
{"hash_id": 8361711583125729361, "entities": ["human gestures", "social robots", "semantic relationship", "gesture variability"], "background": "1. The need to improve the realism and diversity of human gestures generated from speech to enhance the acceptance of social robots and digital humans by human users in various applications. 2. The requirement to better model the semantic relationship between speech and gesture to avoid the generation of smoothed and unrealistic average gestures, and to capture the one-to-many variability in possible gestures."}
{"hash_id": 3257940548965401608, "entities": ["llmbackend kbqa", "hallucinations", "robust reasoning framework", "contextaware correction approach"], "background": "1. Existing LLM-backend KBQA methods struggle with reliability and efficiency, particularly when dealing with hallucinations, which motivated the need for a more robust reasoning framework. 2. Traditional self-correction methods do not effectively leverage environmental feedback, leading to a need for a more context-aware and selective correction approach."}
{"hash_id": 8559957786151580062, "entities": ["argumentation mining", "interrelationships", "shared encoder", "subtasks dependencies"], "background": "1. The need to explicitly model the inter-relationships among the three subtasks in argumentation mining (argumentative relation type classification, argumentative relation identification, and argumentative relation type classification) to improve performance. 2. The limitation of existing methods that rely on a shared encoder to implicitly capture these interactions, which fails to comprehensively model the dependencies between subtasks."}
{"hash_id": 4405729624004757711, "entities": ["uncertainty quantification", "large language model", "semantic importance"], "background": "1. The need to improve the reliability of large language model outputs by accurately quantifying uncertainty, which is crucial for applications like therapy and mental health where dense communication with AI is required. 2. The observation that current uncertainty quantification methods treat all tokens equally, despite the varying levels of semantic importance among them."}
{"hash_id": 6448197896398732052, "entities": ["multilingual benchmark", "lowresource languages", "evaluation methods"], "background": "1. The need for a robust multilingual benchmark to evaluate the quality of representation spaces of multilingual vision-and-language models, particularly for low-resource languages. 2. The observation that current evaluation methods are limited to English or high-resource languages, and the expensive and inefficient processes involved in creating manually annotated multilingual datasets."}
{"hash_id": 2407261702342817064, "entities": ["emergent abilities", "large language models", "empirical foundation", "model performance"], "background": "1. To clarify the nature of emergent abilities in large language models, which are crucial for understanding their potential and risks, and for guiding future model development and usage. 2. To provide a rigorous empirical foundation for explaining the performance of language models on various tasks, thereby improving their efficient use and avoiding overestimation of their capabilities."}
{"hash_id": 2944943120269879575, "entities": ["instruction tuning", "code llms", "multitask scenarios"], "background": "1. The current instruction tuning methods for Code LLMs mainly focus on traditional code generation tasks and lack effectiveness in complex multi-task scenarios, leading to limited generalization ability. 2. Existing methods for instruction data generation in the code domain depend heavily on teacher LLMs, produce duplicate instruction instances, and lack task-specific instructions for multi-task scenarios."}
{"hash_id": 4650427925114338851, "entities": ["multilingual complex reasoning", "llms", "codetrained llms"], "background": "1. The need to enhance the multilingual complex reasoning capabilities of LLMs, as prior works have primarily focused on English or simple reasoning tasks, missing a comprehensive assessment of their abilities across languages. 2. The recognition that LLMs trained on code exhibit improved reasoning skills, which could be leveraged to bridge the performance gap between English and non-English reasoning tasks."}
{"hash_id": 2061753925704693833, "entities": ["unconstrained setting", "chineselanguage dataset", "hallucinations evaluation"], "background": "1. The need for an unconstrained setting to generate hallucinations that better simulates real-world scenarios and allows for deeper analysis of model behavior. 2. The absence of a well-established Chinese-language dataset dedicated to the evaluation of hallucinations in large language models."}
{"hash_id": 3567933987318515192, "entities": ["llmbased agents", "learning generalization", "diverse tasks", "autonomous learning", "behavioral strategies"], "background": "1. The need for LLM-based agents to possess learning and generalization capabilities across diverse tasks, especially in complex and dynamic scenarios with imperfect information. 2. The absence of agents that can autonomously learn from past experiences and evolve their behavioral strategies without manual prompt engineering or parameter tuning."}
{"hash_id": 6659265671669335327, "entities": ["cost of finetuning llms", "hidden population preferences", "generative recommenders"], "background": "1. The high cost and impracticality of continuously fine-tuning Large Language Models (LLMs) in generative recommenders, despite the need to adapt to user preferences. 2. The necessity to actively explore and discover hidden population preferences to improve recommendation quality beyond simply exploiting known high-engagement items."}
{"hash_id": 928874119653968580, "entities": ["jailbreak attacks", "harmful content", "safety disclaimers", "selfregulate responses"], "background": "1. The need to defend large language models against jailbreak attacks, which can lead to the generation of harmful content and are not effectively mitigated by existing defense methods. 2. The observation that even though harmful tokens may dominate in probability, safety disclaimers still appear among the top tokens, suggesting a potential for a model to self-regulate its responses."}
{"hash_id": 106030874437558503, "entities": ["summary quality dimensions", "rouge scores limitations", "reward system alignment"], "background": "1. Existing summarization methods often prioritize a single dimension of summary quality, leading to imbalanced performance across other important dimensions such as consistency, coherence, relevance, and fluency. 2. The limitations of ROUGE scores in evaluating summary quality and the need for a more aligned reward system with human preferences."}
{"hash_id": 5381975175474473521, "entities": ["indepth research ift", "ift data construction", "behavioral norm transfer", "world knowledge injection"], "background": "1. The lack of in-depth research on the mechanisms of IFT, which hinders the development of effective strategies for IFT data construction, model training, and evaluation. 2. The need to discern the separate roles of behavioral norm transfer and world knowledge injection in the IFT process to improve the effectiveness of fine-tuning large language models."}
{"hash_id": 8831997188221615048, "entities": ["instructionfollowing capabilities", "machine translation", "inference process"], "background": "1. The need to fully leverage the acquired instruction-following capabilities of large language models in the context of machine translation, as current approaches fail to yield translations of comparable quality to supervised neural machine translation systems. 2. The recognition that a more complex and reflective inference process could enhance the translation performance of LLMs, similar to how humans improve task performance through self-reflection."}
{"hash_id": 92545988241235278, "entities": ["moe llms", "parameter sizes", "deployment challenges", "expert efficiency"], "background": "1. The immense parameter sizes of MoE LLMs make them challenging to deploy due to the high memory and storage requirements, limiting their practical use. 2. Not all experts within an MoE model contribute equally to performance, which suggests that efficiency could be greatly improved by identifying and reducing the influence of less significant experts."}
{"hash_id": 1206850199988873425, "entities": ["counterfactual reasoning models", "causal relationship", "datasetspecific heuristic", "exploiting dataset artifacts"], "background": "1. The need to improve the ability of counterfactual reasoning models to maintain a clear causal relationship between the counterfactual condition and the generated outcome. 2. The requirement to overcome the limitations of existing methods, such as dataset-specific heuristic approaches and fine-tuning pre-trained language models, which often fail to generalized and can be prone to exploiting dataset artifacts."}
{"hash_id": 2405270657341463584, "entities": ["crosslingual adaptation", "language adapters", "transformerbased models"], "background": "1. The need to improve the efficiency and effectiveness of language adapters in cross-lingual adaptation of transformer-based language models. 2. The lack of understanding of the internal mechanisms and the space in which language adapters operate within the frozen pre-trained model."}
{"hash_id": 761379490473932388, "entities": ["hallucination problem", "llm reliability", "measurement tools", "hallucination assessment"], "background": "1. The need to reduce the 'hallucination' problem in LLMs to enhance their reliability for wide-scale applications. 2. The lack of comprehensive and fine-grained measurement tools to assess and govern the issue of hallucinations in LLMs."}
{"hash_id": 6417355233688347917, "entities": ["data complexity", "transformers", "systematicity", "attention patterns"], "background": "1. Transformers struggle with generalizing to novel compositions of structures and entities when trained on low-complexity data, which is a significant limitation in terms of data efficiency and practical application. 2. The observation that Transformers trained on high-complexity data exhibit systematic attention patterns and generalizable behavior, suggesting that systematicity can be induced in the model with the right techniques."}
{"hash_id": 4871957599333724897, "entities": ["llmbased mobile agents", "multidimensional reasoning", "uionly operations", "complex task scenarios"], "background": "1. The lack of adequate benchmarks to evaluate the performance of LLM-based mobile agents, particularly in terms of their multi-dimensional reasoning and decision-making capabilities. 2. The inefficiency of current UI-only operations and the need for more realistic and complex task scenarios that involve multiple applications and real user queries."}
{"hash_id": 6034130851870218522, "entities": ["procedural constraints", "specialized domains", "domainspecific languages"], "background": "1. The need to accurately represent procedural constraints in highly specialized domains like scientific experiments, which cannot be effectively captured by general-purpose languages. 2. The desire to reduce the substantial manual effort and expertise required to design custom domain-specific languages (DSLs) for each specialized domain."}
{"hash_id": 7512832196306465723, "entities": ["manual extraction", "experimental procedures", "chemical databases", "reactants and products"], "background": "1. The manual extraction of experimental procedures from scientific literature is time-consuming, costly, and prone to errors, especially in the face of rapidly expanding scientific data. 2. Existing chemical databases prioritize storing information on reactants and products over the actual sequences of chemical actions, leading to a gap in actionable knowledge for automation and reproducibility."}
{"hash_id": 2776174185244614947, "entities": ["retaining key information", "large language models", "high compression ratios", "semantic consistency"], "background": "1. The need to retain key information in compressed contexts to maintain the performance of Large Language Models (LLMs) at high compression ratios. 2. The desire to reduce inference cost and improve throughput while preserving the semantic consistency of the compressed context."}
{"hash_id": 4207744303976951410, "entities": ["automatic metric", "human judgment alignment", "taskoriented evaluation", "downstream tasks effectiveness"], "background": "1. The need for an automatic metric that aligns closely with human judgment for evaluating summarizers, as current methods lack theoretical foundations and do not always correlate well with human evaluations. 2. The requirement for a task-oriented evaluation approach that can measure the effectiveness of summaries in enabling agents to perform downstream tasks without reading the full source text."}
{"hash_id": 8788072049835374880, "entities": ["workload facilitation", "keyphrase generation", "multilingual legal dataset"], "background": "1. The need to facilitate the workload of legal experts by automatically generating keyphrases that can summarize lengthy legal documents. 2. The absence of a comprehensive multilingual dataset in the legal domain that can be used to train and evaluate keyphrase generation models."}
{"hash_id": 7734234749075622910, "entities": ["reviewing process", "area chairs", "summarization methods", "consensus opinion"], "background": "1. The increasing number of paper submissions to conferences has strained the reviewing process, imposing a heavy workload on area chairs who must discern the main arguments from multiple reviews. 2. Existing summarization methods focus on generating a consensus opinion, neglecting the unique and divergent opinions that are also crucial for area chairs in making informed decisions."}
{"hash_id": 6184316012719528155, "entities": ["multimodal large language models", "technological divide", "arabic resources", "cultural dialectal nuances"], "background": "1. The need to bridge the technological divide by developing multimodal large language models for languages other than English, particularly Arabic which has a large speaker population. 2. The absence of high-quality multimodal resources and benchmarks for Arabic that can assess the cultural and dialectal nuances of the language."}
{"hash_id": 7080774131643843028, "entities": ["coding assistants", "hallucinations", "data protection", "privacyfocused approach"], "background": "1. To mitigate the issue of \"hallucinations\" and incorrect information generated by coding assistants when dealing with code beyond their training data, ensuring more accurate responses. 2. To address data protection concerns and the potential exposure of sensitive or proprietary code to unintended parties, promoting a more privacy-focused approach."}
{"hash_id": 8373910948140240622, "entities": ["zeroshot time series forecasting", "large language models", "prompting methods"], "background": "1. The need to improve the effectiveness of zero-shot time series forecasting using Large Language Models (LLMs) without the requirement for domain-specific training data. 2. The desire to address the limitations of existing prompting methods, which oversimplify the dynamic nature of time series data and do not integrate state-of-the-art prompt strategies."}
{"hash_id": 5121561091353474875, "entities": ["cognitive differences", "surprisal entropy effects", "human reading processes", "language models", "cognitive profiles"], "background": "1. To account for individual cognitive differences that are often overlooked in surprisal and entropy effects research, which could provide a more accurate understanding of human reading processes. 2. To investigate whether language models' predictions of reading times are biased towards certain cognitive profiles, and to improve these models' predictive power by considering interactions with cognitive measures."}
{"hash_id": 843368447338398814, "entities": ["bridging model capabilities", "realworld coding tasks", "comprehensive benchmark"], "background": "1. The need to bridge the gap between the capabilities of large language models in code synthesis and the practical requirements of real-world coding tasks. 2. The requirement for a more comprehensive and challenging benchmark that reflects the complexity and variety of scenarios encountered in real user coding queries."}
{"hash_id": 3697583429231942540, "entities": ["humanlike feedback", "large language models", "feedback quality enhancement"], "background": "1. The need to improve the ability of large language models to provide feedback that aligns with human-like criteria usage, as current research neglects this aspect of human task execution. 2. The desire to enhance the quality of feedback provided by LLMs, which can be used to guide both human and model performance in various tasks."}
{"hash_id": 9082204955105796002, "entities": ["language abilities llms", "crosslingual semantic alignment", "nonenglish instructiontuning", "pretraining data distribution"], "background": "1. The need to balance the language abilities of LLMs across multiple languages, as they are currently unbalanced towards English due to the distribution of pre-training data. 2. The insufficient performance of LLMs on non-English instruction-tuning tasks, which demands improvement in cross-lingual semantic alignment."}
{"hash_id": 7693780479773854681, "entities": ["multimodal language models", "complex event relations", "eventrelated vqa", "dynamic changes"], "background": "1. The need to enhance the ability of multi-modal large language models to understand and reason about complex event relations across different data modalities, which is crucial for various applications such as visual storytelling and event-related VQA. 2. The recognition that existing models fail to capture the underlying principles governing event evolution, leading to a lack of understanding of event sequences and dynamic changes in real-world scenarios."}
{"hash_id": 6707192854414246729, "entities": ["high cognitive load", "humangenerated summaries", "domainspecific summaries", "large language models", "domain training"], "background": "1. The high cognitive load and inefficiency of human-generated persona-based domain-specific summaries, which do not scale well with the growth of domains and personas. 2. The lack of satisfactory accuracy and high cost of using generic Large Language Models (LLMs) for generating domain-specific summaries without specific domain training."}
{"hash_id": 707123000241998126, "entities": ["reduce hallucinations", "lvlms", "disturbance instructions"], "background": "1. The need to reduce hallucinations in LVLMs, which inaccurately represent visual content and hinder their application in decision-making and open-ended generation. 2. The observation that disturbance instructions exacerbate hallucinations, suggesting that addressing the influence of such instructions could mitigate the issue."}
{"hash_id": 2881104941468462860, "entities": ["reward function design", "reinforcement learning", "multimodal information"], "background": "1. The task of designing reward functions in Reinforcement Learning is challenging and time-consuming, often requiring domain expertise, and current methods using Large Language Models have overlooked the potential of multimodal information, such as visual data. 2. Incorporating visual data into the reward function design process could enhance the model's comprehension and lead to more accurate and effective reward functions."}
{"hash_id": 3332491151016176162, "entities": ["propaganda spread", "news channels", "public opinion", "aibased detection models"], "background": "1. The urgency to combat the spread of propaganda through news channels in the digital age, which can influence public opinion and behavior. 2. The need for reliable training data to improve the performance of AI-based propaganda detection models."}
{"hash_id": 1762771582052643850, "entities": ["stance detection", "arabic social media", "frozen pretrained models"], "background": "1. To reduce the computational and time resources required for stance detection in Arabic social media by avoiding the costly fine-tuning of pre-trained language models. 2. To demonstrate the effectiveness of using frozen pre-trained language models as feature extractors for competitive performance in stance detection tasks."}
{"hash_id": 7740322795977249455, "entities": ["catastrophic forgetting", "continual learning", "parameter efficiency"], "background": "1. The need to overcome catastrophic forgetting in continual learning while reusing knowledge across tasks to improve learning efficiency. 2. The requirement to maintain parameter efficiency as the number of tasks grows, to avoid excessive computational and storage costs."}
